
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>TAPAS ‚Äî transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="transformerxl.html" rel="next" title="Transformer XL"/>
<link href="t5.html" rel="prev" title="T5"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ü§ó Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ü§ó Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TAPAS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage-fine-tuning">Usage: fine-tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage-inference">Usage: inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapas-specific-outputs">Tapas specific outputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapasconfig">TapasConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapastokenizer">TapasTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapasmodel">TapasModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapasformaskedlm">TapasForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapasforsequenceclassification">TapasForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tapasforquestionanswering">TapasForQuestionAnswering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> ¬ª</li>
<li>TAPAS</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/tapas.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="tapas">
<h1>TAPAS<a class="headerlink" href="#tapas" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a recently introduced model so the API hasn‚Äôt been tested extensively. There may be some bugs or slight
breaking changes to fix them in the future.</p>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¬∂</a></h2>
<p>The TAPAS model was proposed in <a class="reference external" href="https://www.aclweb.org/anthology/2020.acl-main.398">TAPAS: Weakly Supervised Table Parsing via Pre-training</a> by Jonathan Herzig, Pawe≈Ç Krzysztof Nowak, Thomas M√ºller,
Francesco Piccinno and Julian Martin Eisenschlos. It‚Äôs a BERT-based model specifically designed (and pre-trained) for
answering questions about tabular data. Compared to BERT, TAPAS uses relative position embeddings and has 7 token types
that encode tabular structure. TAPAS is pre-trained on the masked language modeling (MLM) objective on a large dataset
comprising millions of tables from English Wikipedia and corresponding texts. For question answering, TAPAS has 2 heads
on top: a cell selection head and an aggregation head, for (optionally) performing aggregations (such as counting or
summing) among selected cells. TAPAS has been fine-tuned on several datasets: <a class="reference external" href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a> (Sequential Question Answering by Microsoft), <a class="reference external" href="https://github.com/ppasupat/WikiTableQuestions">WTQ</a> (Wiki Table Questions by Stanford University) and <a class="reference external" href="https://github.com/salesforce/WikiSQL">WikiSQL</a> (by Salesforce). It achieves state-of-the-art on both SQA and WTQ, while
having comparable performance to SOTA on WikiSQL, with a much simpler architecture.</p>
<p>The abstract from the paper is the following:</p>
<p><em>Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the
collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations
instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition,
the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we
present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak
supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation
operator to such selection. TAPAS extends BERT‚Äôs architecture to encode tables as input, initializes from an effective
joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with
three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by
improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL
and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our
setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.</em></p>
<p>In addition, the authors have further pre-trained TAPAS to recognize <strong>table entailment</strong>, by creating a balanced
dataset of millions of automatically created training examples which are learned in an intermediate step prior to
fine-tuning. The authors of TAPAS call this further pre-training intermediate pre-training (since TAPAS is first
pre-trained on MLM, and then on another dataset). They found that intermediate pre-training further improves
performance on SQA, achieving a new state-of-the-art as well as state-of-the-art on <a class="reference external" href="https://github.com/wenhuchen/Table-Fact-Checking">TabFact</a>, a large-scale dataset with 16k Wikipedia tables for table
entailment (a binary classification task). For more details, see their follow-up paper: <a class="reference external" href="https://www.aclweb.org/anthology/2020.findings-emnlp.27/">Understanding tables with
intermediate pre-training</a> by Julian Martin Eisenschlos,
Syrine Krichene and Thomas M√ºller.</p>
<p>The original code can be found <a class="reference external" href="https://github.com/google-research/tapas">here</a>.</p>
<p>Tips:</p>
<ul class="simple">
<li><p>TAPAS is a model that uses relative position embeddings by default (restarting the position embeddings at every cell
of the table). Note that this is something that was added after the publication of the original TAPAS paper.
According to the authors, this usually results in a slightly better performance, and allows you to encode longer
sequences without running out of embeddings. This is reflected in the <code class="docutils literal notranslate"><span class="pre">reset_position_index_per_cell</span></code> parameter of
<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>, which is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> by default. The default versions of the models available
in the <a class="reference external" href="https://huggingface.co/models?search=tapas">model hub</a> all use relative position embeddings. You can still
use the ones with absolute position embeddings by passing in an additional argument <code class="docutils literal notranslate"><span class="pre">revision="no_reset"</span></code> when
calling the <code class="docutils literal notranslate"><span class="pre">.from_pretrained()</span></code> method. Note that it‚Äôs usually advised to pad the inputs on the right rather than
the left.</p></li>
<li><p>TAPAS is based on BERT, so <code class="docutils literal notranslate"><span class="pre">TAPAS-base</span></code> for example corresponds to a <code class="docutils literal notranslate"><span class="pre">BERT-base</span></code> architecture. Of course,
TAPAS-large will result in the best performance (the results reported in the paper are from TAPAS-large). Results of
the various sized models are shown on the <a class="reference external" href="https://github.com/google-research/tapas">original Github repository</a>.</p></li>
<li><p>TAPAS has checkpoints fine-tuned on SQA, which are capable of answering questions related to a table in a
conversational set-up. This means that you can ask follow-up questions such as ‚Äúwhat is his age?‚Äù related to the
previous question. Note that the forward pass of TAPAS is a bit different in case of a conversational set-up: in that
case, you have to feed every table-question pair one by one to the model, such that the <cite>prev_labels</cite> token type ids
can be overwritten by the predicted <cite>labels</cite> of the model to the previous question. See ‚ÄúUsage‚Äù section for more
info.</p></li>
<li><p>TAPAS is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore
efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained
with a causal language modeling (CLM) objective are better in that regard.</p></li>
</ul>
</div>
<div class="section" id="usage-fine-tuning">
<h2>Usage: fine-tuning<a class="headerlink" href="#usage-fine-tuning" title="Permalink to this headline">¬∂</a></h2>
<p>Here we explain how you can fine-tune <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> on your own dataset.</p>
<p><strong>STEP 1: Choose one of the 3 ways in which you can use TAPAS - or experiment</strong></p>
<p>Basically, there are 3 different ways in which one can fine-tune <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a>,
corresponding to the different datasets on which Tapas was fine-tuned:</p>
<ol class="arabic simple">
<li><p>SQA: if you‚Äôre interested in asking follow-up questions related to a table, in a conversational set-up. For example
if you first ask ‚Äúwhat‚Äôs the name of the first actor?‚Äù then you can ask a follow-up question such as ‚Äúhow old is
he?‚Äù. Here, questions do not involve any aggregation (all questions are cell selection questions).</p></li>
<li><p>WTQ: if you‚Äôre not interested in asking questions in a conversational set-up, but rather just asking questions
related to a table, which might involve aggregation, such as counting a number of rows, summing up cell values or
averaging cell values. You can then for example ask ‚Äúwhat‚Äôs the total number of goals Cristiano Ronaldo made in his
career?‚Äù. This case is also called <strong>weak supervision</strong>, since the model itself must learn the appropriate
aggregation operator (SUM/COUNT/AVERAGE/NONE) given only the answer to the question as supervision.</p></li>
<li><p>WikiSQL-supervised: this dataset is based on WikiSQL with the model being given the ground truth aggregation
operator during training. This is also called <strong>strong supervision</strong>. Here, learning the appropriate aggregation
operator is much easier.</p></li>
</ol>
<p>To summarize:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%"/>
<col style="width: 13%"/>
<col style="width: 66%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>Task</strong></p></td>
<td><p><strong>Example dataset</strong></p></td>
<td><p><strong>Description</strong></p></td>
</tr>
<tr class="row-even"><td><p>Conversational</p></td>
<td><p>SQA</p></td>
<td><p>Conversational, only cell selection questions</p></td>
</tr>
<tr class="row-odd"><td><p>Weak supervision for aggregation</p></td>
<td><p>WTQ</p></td>
<td><p>Questions might involve aggregation, and the model must learn this given only the answer as supervision</p></td>
</tr>
<tr class="row-even"><td><p>Strong supervision for aggregation</p></td>
<td><p>WikiSQL-supervised</p></td>
<td><p>Questions might involve aggregation, and the model must learn this given the gold aggregation operator</p></td>
</tr>
</tbody>
</table>
<p>Initializing a model with a pre-trained base and randomly initialized classification heads from the model hub can be
done as follows (be sure to have installed the <a class="reference external" href="https://github.com/rusty1s/pytorch_scatter">torch-scatter dependency</a>
for your environment):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasConfig</span><span class="p">,</span> <span class="n">TapasForQuestionAnswering</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># for example, the base sized model with default SQA configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or, the base sized model with WTQ configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TapasConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-wtq'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or, the base sized model with WikiSQL configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TapasConfig</span><span class="p">(</span><span class="s1">'google-base-finetuned-wikisql-supervised'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>Of course, you don‚Äôt necessarily have to follow one of these three ways in which TAPAS was fine-tuned. You can also
experiment by defining any hyperparameters you want when initializing <a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>, and then
create a <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> based on that configuration. For example, if you have a
dataset that has both conversational questions and questions that might involve aggregation, then you can do it this
way. Here‚Äôs an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasConfig</span><span class="p">,</span> <span class="n">TapasForQuestionAnswering</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you can initialize the classification heads any way you want (see docs of TapasConfig)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TapasConfig</span><span class="p">(</span><span class="n">num_aggregation_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">average_logits_per_cell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">select_one_column</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># initializing the pre-trained base sized model with our custom classification heads</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>What you can also do is start from an already fine-tuned checkpoint. A note here is that the already fine-tuned
checkpoint on WTQ has some issues due to the L2-loss which is somewhat brittle. See <a class="reference external" href="https://github.com/google-research/tapas/issues/91#issuecomment-735719340">here</a> for more info.</p>
<p>For a list of all pre-trained and fine-tuned TAPAS checkpoints available in the HuggingFace model hub, see <a class="reference external" href="https://huggingface.co/models?search=tapas">here</a>.</p>
<p><strong>STEP 2: Prepare your data in the SQA format</strong></p>
<p>Second, no matter what you picked above, you should prepare your dataset in the <a class="reference external" href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA format</a>. This format is a TSV/CSV file with the following
columns:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">id</span></code>: optional, id of the table-question pair, for bookkeeping purposes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">annotator</span></code>: optional, id of the person who annotated the table-question pair, for bookkeeping purposes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">position</span></code>: integer indicating if the question is the first, second, third,‚Ä¶ related to the table. Only required
in case of conversational setup (SQA). You don‚Äôt need this column in case you‚Äôre going for WTQ/WikiSQL-supervised.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">question</span></code>: string</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">table_file</span></code>: string, name of a csv file containing the tabular data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">answer_coordinates</span></code>: list of one or more tuples (each tuple being a cell coordinate, i.e. row, column pair that is
part of the answer)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">answer_text</span></code>: list of one or more strings (each string being a cell value that is part of the answer)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aggregation_label</span></code>: index of the aggregation operator. Only required in case of strong supervision for aggregation
(the WikiSQL-supervised case)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float_answer</span></code>: the float answer to the question, if there is one (np.nan if there isn‚Äôt). Only required in case of
weak supervision for aggregation (such as WTQ and WikiSQL)</p></li>
</ul>
<p>The tables themselves should be present in a folder, each table being a separate csv file. Note that the authors of the
TAPAS algorithm used conversion scripts with some automated logic to convert the other datasets (WTQ, WikiSQL) into the
SQA format. The author explains this <a class="reference external" href="https://github.com/google-research/tapas/issues/50#issuecomment-705465960">here</a>. Interestingly, these conversion scripts
are not perfect (the <code class="docutils literal notranslate"><span class="pre">answer_coordinates</span></code> and <code class="docutils literal notranslate"><span class="pre">float_answer</span></code> fields are populated based on the <code class="docutils literal notranslate"><span class="pre">answer_text</span></code>),
meaning that WTQ and WikiSQL results could actually be improved.</p>
<p><strong>STEP 3: Convert your data into PyTorch tensors using TapasTokenizer</strong></p>
<p>Third, given that you‚Äôve prepared your data in this TSV/CSV format (and corresponding CSV files containing the tabular
data), you can then use <a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> to convert table-question pairs into <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">attention_mask</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">token_type_ids</span></code> and so on. Again, based on which of the three cases you picked above,
<a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> requires different inputs to be fine-tuned:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 28%"/>
<col style="width: 72%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>Task</strong></p></td>
<td><p><strong>Required inputs</strong></p></td>
</tr>
<tr class="row-even"><td><p>Conversational</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">labels</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Weak supervision for aggregation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">labels</span></code>, <code class="docutils literal notranslate"><span class="pre">numeric_values</span></code>,
<code class="docutils literal notranslate"><span class="pre">numeric_values_scale</span></code>, <code class="docutils literal notranslate"><span class="pre">float_answer</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Strong supervision for aggregation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention</span> <span class="pre">mask</span></code>, <code class="docutils literal notranslate"><span class="pre">token</span> <span class="pre">type</span> <span class="pre">ids</span></code>, <code class="docutils literal notranslate"><span class="pre">labels</span></code>, <code class="docutils literal notranslate"><span class="pre">aggregation_labels</span></code></p></td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> creates the <code class="docutils literal notranslate"><span class="pre">labels</span></code>, <code class="docutils literal notranslate"><span class="pre">numeric_values</span></code> and <code class="docutils literal notranslate"><span class="pre">numeric_values_scale</span></code> based on
the <code class="docutils literal notranslate"><span class="pre">answer_coordinates</span></code> and <code class="docutils literal notranslate"><span class="pre">answer_text</span></code> columns of the TSV file. The <code class="docutils literal notranslate"><span class="pre">float_answer</span></code> and <code class="docutils literal notranslate"><span class="pre">aggregation_labels</span></code>
are already in the TSV file of step 2. Here‚Äôs an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">'google/tapas-base'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TapasTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Actors'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Brad Pitt"</span><span class="p">,</span> <span class="s2">"Leonardo Di Caprio"</span><span class="p">,</span> <span class="s2">"George Clooney"</span><span class="p">],</span> <span class="s1">'Number of movies'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"87"</span><span class="p">,</span> <span class="s2">"53"</span><span class="p">,</span> <span class="s2">"69"</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"What is the name of the first actor?"</span><span class="p">,</span> <span class="s2">"How many movies has George Clooney played in?"</span><span class="p">,</span> <span class="s2">"What is the total number of movies?"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">answer_coordinates</span> <span class="o">=</span> <span class="p">[[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)],</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">answer_text</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">"Brad Pitt"</span><span class="p">],</span> <span class="p">[</span><span class="s2">"69"</span><span class="p">],</span> <span class="p">[</span><span class="s2">"209"</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span> <span class="n">answer_coordinates</span><span class="o">=</span><span class="n">answer_coordinates</span><span class="p">,</span> <span class="n">answer_text</span><span class="o">=</span><span class="n">answer_text</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'max_length'</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span>
<span class="go">{'input_ids': tensor([[ ... ]]), 'attention_mask': tensor([[...]]), 'token_type_ids': tensor([[[...]]]),</span>
<span class="go">'numeric_values': tensor([[ ... ]]), 'numeric_values_scale: tensor([[ ... ]]), labels: tensor([[ ... ]])}</span>
</pre></div>
</div>
<p>Note that <a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> expects the data of the table to be <strong>text-only</strong>. You can use
<code class="docutils literal notranslate"><span class="pre">.astype(str)</span></code> on a dataframe to turn it into text-only data. Of course, this only shows how to encode a single
training example. It is advised to create a PyTorch dataset and a corresponding dataloader:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tsv_path</span> <span class="o">=</span> <span class="s2">"your_path_to_the_tsv_file"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table_csv_path</span> <span class="o">=</span> <span class="s2">"your_path_to_a_directory_containing_all_csv_files"</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">TableDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">item</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">table_csv_path</span> <span class="o">+</span> <span class="n">item</span><span class="o">.</span><span class="n">table_file</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="c1"># be sure to make your table data text only</span>
<span class="gp">... </span>        <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">queries</span><span class="o">=</span><span class="n">item</span><span class="o">.</span><span class="n">question</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">answer_coordinates</span><span class="o">=</span><span class="n">item</span><span class="o">.</span><span class="n">answer_coordinates</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">answer_text</span><span class="o">=</span><span class="n">item</span><span class="o">.</span><span class="n">answer_text</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span>
<span class="gp">... </span>        <span class="p">)</span>
<span class="gp">... </span>        <span class="c1"># remove the batch dimension which the tokenizer adds by default</span>
<span class="gp">... </span>        <span class="n">encoding</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">encoding</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="gp">... </span>        <span class="c1"># add the float_answer which is also required (weak supervision for aggregation case)</span>
<span class="gp">... </span>        <span class="n">encoding</span><span class="p">[</span><span class="s2">"float_answer"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">float_answer</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">encoding</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">tsv_path</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TableDataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that here, we encode each table-question pair independently. This is fine as long as your dataset is <strong>not
conversational</strong>. In case your dataset involves conversational questions (such as in SQA), then you should first group
together the <code class="docutils literal notranslate"><span class="pre">queries</span></code>, <code class="docutils literal notranslate"><span class="pre">answer_coordinates</span></code> and <code class="docutils literal notranslate"><span class="pre">answer_text</span></code> per table (in the order of their <code class="docutils literal notranslate"><span class="pre">position</span></code>
index) and batch encode each table with its questions. This will make sure that the <code class="docutils literal notranslate"><span class="pre">prev_labels</span></code> token types (see
docs of <a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>) are set correctly. See <a class="reference external" href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb">this notebook</a>
for more info.</p>
<p><strong>STEP 4: Train (fine-tune) TapasForQuestionAnswering</strong></p>
<p>You can then fine-tune <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> using native PyTorch as follows (shown here for
the weak supervision for aggregation case):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasConfig</span><span class="p">,</span> <span class="n">TapasForQuestionAnswering</span><span class="p">,</span> <span class="n">AdamW</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># this is the default WTQ configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TapasConfig</span><span class="p">(</span>
<span class="gp">... </span>           <span class="n">num_aggregation_labels</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">use_answer_as_supervision</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">answer_loss_cutoff</span> <span class="o">=</span> <span class="mf">0.664694</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">cell_selection_preference</span> <span class="o">=</span> <span class="mf">0.207951</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">huber_loss_delta</span> <span class="o">=</span> <span class="mf">0.121194</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">init_cell_selection_weights_to_zero</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">select_one_column</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">allow_empty_column_selection</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>           <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.0352513</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/tapas-base"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times</span>
<span class="gp">... </span>   <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
<span class="gp">... </span>        <span class="c1"># get the inputs;</span>
<span class="gp">... </span>        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"token_type_ids"</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">numeric_values</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"numeric_values"</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">numeric_values_scale</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"numeric_values_scale"</span><span class="p">]</span>
<span class="gp">... </span>        <span class="n">float_answer</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"float_answer"</span><span class="p">]</span>

<span class="gp">... </span>        <span class="c1"># zero the parameter gradients</span>
<span class="gp">... </span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="gp">... </span>        <span class="c1"># forward + backward + optimize</span>
<span class="gp">... </span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">numeric_values</span><span class="o">=</span><span class="n">numeric_values</span><span class="p">,</span> <span class="n">numeric_values_scale</span><span class="o">=</span><span class="n">numeric_values_scale</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">float_answer</span><span class="o">=</span><span class="n">float_answer</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="gp">... </span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="usage-inference">
<h2>Usage: inference<a class="headerlink" href="#usage-inference" title="Permalink to this headline">¬∂</a></h2>
<p>Here we explain how you can use <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> for inference (i.e. making predictions
on new data). For inference, only <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code> (which you can obtain using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>) have to be provided to the model to obtain the logits. Next, you can use the
handy <code class="docutils literal notranslate"><span class="pre">convert_logits_to_predictions</span></code> method of <a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> to convert these into predicted
coordinates and optional aggregation indices.</p>
<p>However, note that inference is <strong>different</strong> depending on whether or not the setup is conversational. In a
non-conversational set-up, inference can be done in parallel on all table-question pairs of a batch. Here‚Äôs an example
of that:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasTokenizer</span><span class="p">,</span> <span class="n">TapasForQuestionAnswering</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">'google/tapas-base-finetuned-wtq'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TapasTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Actors'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Brad Pitt"</span><span class="p">,</span> <span class="s2">"Leonardo Di Caprio"</span><span class="p">,</span> <span class="s2">"George Clooney"</span><span class="p">],</span> <span class="s1">'Number of movies'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"87"</span><span class="p">,</span> <span class="s2">"53"</span><span class="p">,</span> <span class="s2">"69"</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"What is the name of the first actor?"</span><span class="p">,</span> <span class="s2">"How many movies has George Clooney played in?"</span><span class="p">,</span> <span class="s2">"What is the total number of movies?"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'max_length'</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predicted_answer_coordinates</span><span class="p">,</span> <span class="n">predicted_aggregation_indices</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_logits_to_predictions</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">inputs</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
<span class="gp">... </span>        <span class="n">outputs</span><span class="o">.</span><span class="n">logits_aggregation</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># let's print out the results:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">id2aggregation</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"NONE"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"SUM"</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">"AVERAGE"</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="s2">"COUNT"</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">aggregation_predictions_string</span> <span class="o">=</span> <span class="p">[</span><span class="n">id2aggregation</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">predicted_aggregation_indices</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">answers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">coordinates</span> <span class="ow">in</span> <span class="n">predicted_answer_coordinates</span><span class="p">:</span>
<span class="gp">... </span>  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">coordinates</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="gp">... </span>    <span class="c1"># only a single cell:</span>
<span class="gp">... </span>    <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">iat</span><span class="p">[</span><span class="n">coordinates</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="gp">... </span>  <span class="k">else</span><span class="p">:</span>
<span class="gp">... </span>    <span class="c1"># multiple cells</span>
<span class="gp">... </span>    <span class="n">cell_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">coordinate</span> <span class="ow">in</span> <span class="n">coordinates</span><span class="p">:</span>
<span class="gp">... </span>       <span class="n">cell_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">iat</span><span class="p">[</span><span class="n">coordinate</span><span class="p">])</span>
<span class="gp">... </span>    <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cell_values</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">display</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="n">predicted_agg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">answers</span><span class="p">,</span> <span class="n">aggregation_predictions_string</span><span class="p">):</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">if</span> <span class="n">predicted_agg</span> <span class="o">==</span> <span class="s2">"NONE"</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Predicted answer: "</span> <span class="o">+</span> <span class="n">answer</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">else</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Predicted answer: "</span> <span class="o">+</span> <span class="n">predicted_agg</span> <span class="o">+</span> <span class="s2">" &gt; "</span> <span class="o">+</span> <span class="n">answer</span><span class="p">)</span>
<span class="go">What is the name of the first actor?</span>
<span class="go">Predicted answer: Brad Pitt</span>
<span class="go">How many movies has George Clooney played in?</span>
<span class="go">Predicted answer: COUNT &gt; 69</span>
<span class="go">What is the total number of movies?</span>
<span class="go">Predicted answer: SUM &gt; 87, 53, 69</span>
</pre></div>
</div>
<p>In case of a conversational set-up, then each table-question pair must be provided <strong>sequentially</strong> to the model, such
that the <code class="docutils literal notranslate"><span class="pre">prev_labels</span></code> token types can be overwritten by the predicted <code class="docutils literal notranslate"><span class="pre">labels</span></code> of the previous table-question
pair. Again, more info can be found in <a class="reference external" href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb">this notebook</a>.</p>
</div>
<div class="section" id="tapas-specific-outputs">
<h2>Tapas specific outputs<a class="headerlink" href="#tapas-specific-outputs" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput"><a name="//apple_ref/cpp/Class/transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.models.tapas.modeling_tapas.</code><code class="sig-name descname">TableQuestionAnsweringOutput</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">logits</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">logits_aggregation</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TableQuestionAnsweringOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Output type of <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> (and possibly <code class="xref py py-obj docutils literal notranslate"><span class="pre">answer</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">aggregation_labels</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">numeric_values</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">numeric_values_scale</span></code> are provided)) ‚Äì Total loss as the sum of the hierarchical cell selection log-likelihood loss and (optionally) the
semi-supervised regression loss and (optionally) supervised loss for aggregations.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì Prediction scores of the cell selection head, for every token.</p></li>
<li><p><strong>logits_aggregation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>, <cite>optional</cite>, of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_aggregation_labels)</span></code>) ‚Äì Prediction scores of the aggregation head, for every aggregation operator.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</p></li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="tapasconfig">
<h2>TapasConfig<a class="headerlink" href="#tapasconfig" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.TapasConfig"><a name="//apple_ref/cpp/Class/transformers.TapasConfig"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TapasConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_size</span><span class="o">=</span><span class="default_value">30522</span></em>, <em class="sig-param"><span class="n">hidden_size</span><span class="o">=</span><span class="default_value">768</span></em>, <em class="sig-param"><span class="n">num_hidden_layers</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">num_attention_heads</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">intermediate_size</span><span class="o">=</span><span class="default_value">3072</span></em>, <em class="sig-param"><span class="n">hidden_act</span><span class="o">=</span><span class="default_value">'gelu'</span></em>, <em class="sig-param"><span class="n">hidden_dropout_prob</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">attention_probs_dropout_prob</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">max_position_embeddings</span><span class="o">=</span><span class="default_value">1024</span></em>, <em class="sig-param"><span class="n">type_vocab_sizes</span><span class="o">=</span><span class="default_value">[3, 256, 256, 2, 256, 256, 10]</span></em>, <em class="sig-param"><span class="n">initializer_range</span><span class="o">=</span><span class="default_value">0.02</span></em>, <em class="sig-param"><span class="n">layer_norm_eps</span><span class="o">=</span><span class="default_value">1e-12</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">gradient_checkpointing</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">positive_label_weight</span><span class="o">=</span><span class="default_value">10.0</span></em>, <em class="sig-param"><span class="n">num_aggregation_labels</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">aggregation_loss_weight</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">use_answer_as_supervision</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">answer_loss_importance</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">use_normalized_answer_loss</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">huber_loss_delta</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">temperature</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">aggregation_temperature</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">use_gumbel_for_cells</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_gumbel_for_aggregation</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">average_approximation_function</span><span class="o">=</span><span class="default_value">'ratio'</span></em>, <em class="sig-param"><span class="n">cell_selection_preference</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">answer_loss_cutoff</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_num_rows</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">max_num_columns</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">average_logits_per_cell</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">select_one_column</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">allow_empty_column_selection</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">init_cell_selection_weights_to_zero</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">reset_position_index_per_cell</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">disable_per_token_loss</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">aggregation_labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">no_aggregation_label_index</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/configuration_tapas.html#TapasConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasConfig" title="Permalink to this definition">¬∂</a></dt>
<dd><p>This is the configuration class to store the configuration of a <a class="reference internal" href="#transformers.TapasModel" title="transformers.TapasModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasModel</span></code></a>. It is used to
instantiate a TAPAS model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the TAPAS <cite>tapas-base-finetuned-sqa</cite>
architecture. Configuration objects inherit from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedConfig</span></code> and can be used to control
the model outputs. Read the documentation from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> for more information.</p>
<p>Hyperparameters additional to BERT are taken from run_task_main.py and hparam_utils.py of the original
implementation. Original implementation available at <a class="reference external" href="https://github.com/google-research/tapas/tree/master">https://github.com/google-research/tapas/tree/master</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 30522) ‚Äì Vocabulary size of the TAPAS model. Defines the number of different tokens that can be represented by the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> passed when calling <a class="reference internal" href="#transformers.TapasModel" title="transformers.TapasModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasModel</span></code></a>.</p></li>
<li><p><strong>hidden_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 768) ‚Äì Dimensionality of the encoder layers and the pooler layer.</p></li>
<li><p><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) ‚Äì Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>num_attention_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) ‚Äì Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>intermediate_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 3072) ‚Äì Dimensionality of the ‚Äúintermediate‚Äù (often named feed-forward) layer in the Transformer encoder.</p></li>
<li><p><strong>hidden_act</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gelu"</span></code>) ‚Äì The non-linear activation function (function or string) in the encoder and pooler. If string,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"gelu"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"relu"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"swish"</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gelu_new"</span></code> are supported.</p></li>
<li><p><strong>hidden_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) ‚Äì The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p></li>
<li><p><strong>attention_probs_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) ‚Äì The dropout ratio for the attention probabilities.</p></li>
<li><p><strong>max_position_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1024) ‚Äì The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).</p></li>
<li><p><strong>type_vocab_sizes</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">256,</span> <span class="pre">256,</span> <span class="pre">2,</span> <span class="pre">256,</span> <span class="pre">256,</span> <span class="pre">10]</span></code>) ‚Äì The vocabulary sizes of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">token_type_ids</span></code> passed when calling <a class="reference internal" href="#transformers.TapasModel" title="transformers.TapasModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasModel</span></code></a>.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.02) ‚Äì The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p></li>
<li><p><strong>layer_norm_eps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-12) ‚Äì The epsilon used by the layer normalization layers.</p></li>
<li><p><strong>gradient_checkpointing</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to use gradient checkpointing to save memory at the expense of a slower backward pass.</p></li>
<li><p><strong>positive_label_weight</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 10.0) ‚Äì Weight for positive labels.</p></li>
<li><p><strong>num_aggregation_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The number of aggregation operators to predict.</p></li>
<li><p><strong>aggregation_loss_weight</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) ‚Äì Importance weight for the aggregation loss.</p></li>
<li><p><strong>use_answer_as_supervision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether to use the answer as the only supervision for aggregation examples.</p></li>
<li><p><strong>answer_loss_importance</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) ‚Äì Importance weight for the regression loss.</p></li>
<li><p><strong>use_normalized_answer_loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to normalize the answer loss by the maximum of the predicted and expected value.</p></li>
<li><p><strong>huber_loss_delta</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) ‚Äì Delta parameter used to calculate the regression loss.</p></li>
<li><p><strong>temperature</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) ‚Äì Value used to control (OR change) the skewness of cell logits probabilities.</p></li>
<li><p><strong>aggregation_temperature</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) ‚Äì Scales aggregation logits to control the skewness of probabilities.</p></li>
<li><p><strong>use_gumbel_for_cells</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to apply Gumbel-Softmax to cell selection.</p></li>
<li><p><strong>use_gumbel_for_aggregation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to apply Gumbel-Softmax to aggregation selection.</p></li>
<li><p><strong>average_approximation_function</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"ratio"</span></code>) ‚Äì Method to calculate the expected average of cells in the weak supervision case. One of <code class="xref py py-obj docutils literal notranslate"><span class="pre">"ratio"</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"first_order"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"second_order"</span></code>.</p></li>
<li><p><strong>cell_selection_preference</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) ‚Äì Preference for cell selection in ambiguous cases. Only applicable in case of weak supervision for
aggregation (WTQ, WikiSQL). If the total mass of the aggregation probabilities (excluding the ‚ÄúNONE‚Äù
operator) is higher than this hyperparameter, then aggregation is predicted for an example.</p></li>
<li><p><strong>answer_loss_cutoff</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) ‚Äì Ignore examples with answer loss larger than cutoff.</p></li>
<li><p><strong>max_num_rows</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 64) ‚Äì Maximum number of rows.</p></li>
<li><p><strong>max_num_columns</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 32) ‚Äì Maximum number of columns.</p></li>
<li><p><strong>average_logits_per_cell</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to average logits per cell.</p></li>
<li><p><strong>select_one_column</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether to constrain the model to only select cells from a single column.</p></li>
<li><p><strong>allow_empty_column_selection</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to allow not to select any column.</p></li>
<li><p><strong>init_cell_selection_weights_to_zero</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to initialize cell selection weights to 0 so that the initial probabilities are 50%.</p></li>
<li><p><strong>reset_position_index_per_cell</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether to restart position indexes at every cell (i.e. use relative position embeddings).</p></li>
<li><p><strong>disable_per_token_loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to disable any (strong or weak) supervision on cells.</p></li>
<li><p><strong>aggregation_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">label]</span></code>, <cite>optional</cite>) ‚Äì The aggregation labels used to aggregate the results. For example, the WTQ models have the following
aggregation labels: <code class="xref py py-obj docutils literal notranslate"><span class="pre">{0:</span> <span class="pre">"NONE",</span> <span class="pre">1:</span> <span class="pre">"SUM",</span> <span class="pre">2:</span> <span class="pre">"AVERAGE",</span> <span class="pre">3:</span> <span class="pre">"COUNT"}</span></code></p></li>
<li><p><strong>no_aggregation_label_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If the aggregation labels are defined and one of these labels represents ‚ÄúNo aggregation‚Äù, this should be
set to its index. For example, the WTQ models have the ‚ÄúNONE‚Äù aggregation label at index 0, so that value
should be set to 0 for these models.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasModel</span><span class="p">,</span> <span class="n">TapasConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing a default (SQA) Tapas configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">TapasConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing a model from the configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Accessing the model configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</pre></div>
</div>
</dd></dl>
</div>
<div class="section" id="tapastokenizer">
<h2>TapasTokenizer<a class="headerlink" href="#tapastokenizer" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.TapasTokenizer"><a name="//apple_ref/cpp/Class/transformers.TapasTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TapasTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">do_basic_tokenize</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">never_split</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">empty_token</span><span class="o">=</span><span class="default_value">'[EMPTY]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cell_trim_length</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">max_column_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_row_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">strip_column_names</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">update_answer_coordinates</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">model_max_length</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">512</span></em>, <em class="sig-param"><span class="n">additional_special_tokens</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/tokenization_tapas.html#TapasTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasTokenizer" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Construct a TAPAS tokenizer. Based on WordPiece. Flattens a table and one or more related sentences to be used by
TAPAS models.</p>
<p>This tokenizer inherits from <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> creates several token type ids to encode tabular structure. To be more
precise, it adds 7 token type ids, in the following order: <code class="xref py py-obj docutils literal notranslate"><span class="pre">segment_ids</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">column_ids</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">row_ids</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">prev_labels</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">column_ranks</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">inv_column_ranks</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">numeric_relations</span></code>:</p>
<ul class="simple">
<li><p>segment_ids: indicate whether a token belongs to the question (0) or the table (1). 0 for special tokens and
padding.</p></li>
<li><p>column_ids: indicate to which column of the table a token belongs (starting from 1). Is 0 for all question
tokens, special tokens and padding.</p></li>
<li><p>row_ids: indicate to which row of the table a token belongs (starting from 1). Is 0 for all question tokens,
special tokens and padding. Tokens of column headers are also 0.</p></li>
<li><p>prev_labels: indicate whether a token was (part of) an answer to the previous question (1) or not (0). Useful in
a conversational setup (such as SQA).</p></li>
<li><p>column_ranks: indicate the rank of a table token relative to a column, if applicable. For example, if you have a
column ‚Äúnumber of movies‚Äù with values 87, 53 and 69, then the column ranks of these tokens are 3, 1 and 2
respectively. 0 for all question tokens, special tokens and padding.</p></li>
<li><p>inv_column_ranks: indicate the inverse rank of a table token relative to a column, if applicable. For example, if
you have a column ‚Äúnumber of movies‚Äù with values 87, 53 and 69, then the inverse column ranks of these tokens are
1, 3 and 2 respectively. 0 for all question tokens, special tokens and padding.</p></li>
<li><p>numeric_relations: indicate numeric relations between the question and the tokens of the table. 0 for all
question tokens, special tokens and padding.</p></li>
</ul>
<p><a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> runs end-to-end tokenization on a table and associated sentences: punctuation
splitting and wordpiece.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì File containing the vocabulary.</p></li>
<li><p><strong>do_lower_case</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to lowercase the input when tokenizing.</p></li>
<li><p><strong>do_basic_tokenize</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to do basic tokenization before WordPiece.</p></li>
<li><p><strong>never_split</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable</span></code>, <cite>optional</cite>) ‚Äì Collection of tokens which will never be split during tokenization. Only has an effect when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">do_basic_tokenize=True</span></code></p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"[UNK]"</span></code>) ‚Äì The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p></li>
<li><p><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"[SEP]"</span></code>) ‚Äì The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.</p></li>
<li><p><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"[PAD]"</span></code>) ‚Äì The token used for padding, for example when batching sequences of different lengths.</p></li>
<li><p><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"[CLS]"</span></code>) ‚Äì The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.</p></li>
<li><p><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"[MASK]"</span></code>) ‚Äì The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.</p></li>
<li><p><strong>empty_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"[EMPTY]"</span></code>) ‚Äì The token used for empty cell values in a table. Empty cell values include ‚Äú‚Äù, ‚Äún/a‚Äù, ‚Äúnan‚Äù and ‚Äú?‚Äù.</p></li>
<li><p><strong>tokenize_chinese_chars</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see this
<a class="reference external" href="https://github.com/huggingface/transformers/issues/328">issue</a>).</p></li>
<li><p><strong>strip_accents</strong> ‚Äì (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code class="xref py py-obj docutils literal notranslate"><span class="pre">lowercase</span></code> (as in the original BERT).</p></li>
<li><p><strong>cell_trim_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) ‚Äì If &gt; 0: Trim cells so that the length is &lt;= this value. Also disables further cell trimming, should thus be
used with <code class="xref py py-obj docutils literal notranslate"><span class="pre">truncation</span></code> set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>max_column_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì Max column id to extract.</p></li>
<li><p><strong>max_row_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì Max row id to extract.</p></li>
<li><p><strong>strip_column_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to add empty strings instead of column names.</p></li>
<li><p><strong>update_answer_coordinates</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether to recompute the answer coordinates from the answer text.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TapasTokenizer.__call__"><a name="//apple_ref/cpp/Method/transformers.TapasTokenizer.__call__"></a>
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">table</span><span class="p">:</span> <span class="n">pd.DataFrame</span></em>, <em class="sig-param"><span class="n">queries</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">answer_coordinates</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>List<span class="p">[</span>Tuple<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>Tuple<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">answer_text</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span>transformers.models.tapas.tokenization_tapas.TapasTruncationStrategy<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/models/tapas/tokenization_tapas.html#TapasTokenizer.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasTokenizer.__call__" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Main method to tokenize and prepare for the model one or several sequence(s) related to a table.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>table</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">pd.DataFrame</span></code>) ‚Äì Table containing tabular data. Note that all cell values must be text. Use <cite>.astype(str)</cite> on a Pandas
dataframe to convert it to string.</p></li>
<li><p><strong>queries</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì Question or batch of questions related to a table to be encoded. Note that in case of a batch, all
questions must refer to the <strong>same</strong> table.</p></li>
<li><p><strong>answer_coordinates</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[Tuple]]</span></code>, <cite>optional</cite>) ‚Äì Answer coordinates of each table-question pair in the batch. In case only a single table-question pair
is provided, then the answer_coordinates must be a single list of one or more tuples. Each tuple must
be a (row_index, column_index) pair. The first data row (not the column header row) has index 0. The
first column has index 0. In case a batch of table-question pairs is provided, then the
answer_coordinates must be a list of lists of tuples (each list corresponding to a single
table-question pair).</p></li>
<li><p><strong>answer_text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>, <cite>optional</cite>) ‚Äì Answer text of each table-question pair in the batch. In case only a single table-question pair is
provided, then the answer_text must be a single list of one or more strings. Each string must be the
answer text of a corresponding answer coordinate. In case a batch of table-question pairs is provided,
then the answer_coordinates must be a list of lists of strings (each list corresponding to a single
table-question pair).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'drop_rows_to_fit'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate row by row, removing rows from the table.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TapasTokenizer.convert_logits_to_predictions"><a name="//apple_ref/cpp/Method/transformers.TapasTokenizer.convert_logits_to_predictions"></a>
<code class="sig-name descname">convert_logits_to_predictions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em>, <em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="n">logits_agg</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cell_classification_threshold</span><span class="o">=</span><span class="default_value">0.5</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/tokenization_tapas.html#TapasTokenizer.convert_logits_to_predictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasTokenizer.convert_logits_to_predictions" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts logits of <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> to actual predicted answer coordinates and
optional aggregation indices.</p>
<p>The original implementation, on which this function is based, can be found <a class="reference external" href="https://github.com/google-research/tapas/blob/4908213eb4df7aa988573350278b44c4dbe3f71b/tapas/experiments/prediction_utils.py#L288">here</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>) ‚Äì Dictionary mapping features to actual values. Should be created using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì Tensor containing the logits at the token level.</p></li>
<li><p><strong>logits_agg</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_aggregation_labels)</span></code>, <cite>optional</cite>) ‚Äì Tensor containing the aggregation logits.</p></li>
<li><p><strong>cell_classification_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.5) ‚Äì Threshold to be used for cell selection. All table cells for which their probability is larger than
this threshold will be selected.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>predicted_answer_coordinates (<code class="docutils literal notranslate"><span class="pre">List[List[[tuple]]</span></code> of length <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>): Predicted answer
coordinates as a list of lists of tuples. Each element in the list contains the predicted answer
coordinates of a single example in the batch, as a list of tuples. Each tuple is a cell, i.e. (row index,
column index).</p></li>
<li><p>predicted_aggregation_indices (<code class="docutils literal notranslate"><span class="pre">List[int]``of</span> <span class="pre">length</span> <span class="pre">``batch_size</span></code>, <cite>optional</cite>, returned when
<code class="docutils literal notranslate"><span class="pre">logits_aggregation</span></code> is provided): Predicted aggregation operator indices of the aggregation head.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> comprising various elements depending on the inputs</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TapasTokenizer.save_vocabulary"><a name="//apple_ref/cpp/Method/transformers.TapasTokenizer.save_vocabulary"></a>
<code class="sig-name descname">save_vocabulary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí Tuple<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/tapas/tokenization_tapas.html#TapasTokenizer.save_vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasTokenizer.save_vocabulary" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won‚Äôt save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The directory in which to save the vocabulary.</p></li>
<li><p><strong>filename_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) ‚Äì An optional prefix to add to the named of the saved files.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Paths to the files saved.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tapasmodel">
<h2>TapasModel<a class="headerlink" href="#tapasmodel" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.TapasModel"><a name="//apple_ref/cpp/Class/transformers.TapasModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TapasModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">add_pooling_layer</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasModel" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The bare Tapas Model transformer outputting raw hidden-states without any specific head on top.
This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its models (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) ‚Äì Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<p>This class is a small change compared to <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>, taking into account the additional token
type ids.</p>
<p>The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
cross-attention is added between the self-attention layers, following the architecture described in <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is
all you need</a> by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</p>
<dl class="py method">
<dt id="transformers.TapasModel.forward"><a name="//apple_ref/cpp/Method/transformers.TapasModel.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasModel.forward" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TapasModel" title="transformers.TapasModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì <p>Indices of input sequence tokens in the vocabulary. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">7)</span></code>, <cite>optional</cite>) ‚Äì <p>Token indices that encode tabular structure. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See this class for more info.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Indices of positions of each input sequence tokens in the position embeddings. If
<code class="docutils literal notranslate"><span class="pre">reset_position_index_per_cell</span></code> of <a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, relative
position embeddings will be used. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) ‚Äì Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: - 1
indicates the head is <strong>not masked</strong>, - 0 indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) ‚Äì Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model‚Äôs internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/output.html#transformers.modeling_outputs.BaseModelOutputWithPooling" title="transformers.modeling_outputs.BaseModelOutputWithPooling"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModelOutputWithPooling</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>last_hidden_state</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>) ‚Äì Sequence of hidden-states at the output of the last layer of the model.</p></li>
<li><p><strong>pooler_output</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">hidden_size)</span></code>) ‚Äì Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasTokenizer</span><span class="p">,</span> <span class="n">TapasModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TapasTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Actors'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Brad Pitt"</span><span class="p">,</span> <span class="s2">"Leonardo Di Caprio"</span><span class="p">,</span> <span class="s2">"George Clooney"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Age'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"56"</span><span class="p">,</span> <span class="s2">"45"</span><span class="p">,</span> <span class="s2">"59"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Number of movies'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"87"</span><span class="p">,</span> <span class="s2">"53"</span><span class="p">,</span> <span class="s2">"69"</span><span class="p">]</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"How many movies has George Clooney played in?"</span><span class="p">,</span> <span class="s2">"How old is Brad Pitt?"</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/output.html#transformers.modeling_outputs.BaseModelOutputWithPooling" title="transformers.modeling_outputs.BaseModelOutputWithPooling"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModelOutputWithPooling</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tapasformaskedlm">
<h2>TapasForMaskedLM<a class="headerlink" href="#tapasformaskedlm" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.TapasForMaskedLM"><a name="//apple_ref/cpp/Class/transformers.TapasForMaskedLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TapasForMaskedLM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasForMaskedLM" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Tapas Model with a <cite>language modeling</cite> head on top.
This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its models (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) ‚Äì Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TapasForMaskedLM.forward"><a name="//apple_ref/cpp/Method/transformers.TapasForMaskedLM.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasForMaskedLM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasForMaskedLM.forward" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TapasForMaskedLM" title="transformers.TapasForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForMaskedLM</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì <p>Indices of input sequence tokens in the vocabulary. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">7)</span></code>, <cite>optional</cite>) ‚Äì <p>Token indices that encode tabular structure. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See this class for more info.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Indices of positions of each input sequence tokens in the position embeddings. If
<code class="docutils literal notranslate"><span class="pre">reset_position_index_per_cell</span></code> of <a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, relative
position embeddings will be used. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) ‚Äì Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: - 1
indicates the head is <strong>not masked</strong>, - 0 indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) ‚Äì Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model‚Äôs internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì Labels for computing the masked language modeling loss. Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span>
<span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring) Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored
(masked), the loss is only computed for the tokens with labels in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/output.html#transformers.modeling_outputs.MaskedLMOutput" title="transformers.modeling_outputs.MaskedLMOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedLMOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided) ‚Äì Masked language modeling (MLM) loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>) ‚Äì Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasTokenizer</span><span class="p">,</span> <span class="n">TapasForMaskedLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TapasTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Actors'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Brad Pitt"</span><span class="p">,</span> <span class="s2">"Leonardo Di Caprio"</span><span class="p">,</span> <span class="s2">"George Clooney"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Age'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"56"</span><span class="p">,</span> <span class="s2">"45"</span><span class="p">,</span> <span class="s2">"59"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Number of movies'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"87"</span><span class="p">,</span> <span class="s2">"53"</span><span class="p">,</span> <span class="s2">"69"</span><span class="p">]</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="s2">"How many [MASK] has George [MASK] played in?"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="s2">"How many movies has George Clooney played in?"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)[</span><span class="s2">"input_ids"</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/output.html#transformers.modeling_outputs.MaskedLMOutput" title="transformers.modeling_outputs.MaskedLMOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedLMOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tapasforsequenceclassification">
<h2>TapasForSequenceClassification<a class="headerlink" href="#tapasforsequenceclassification" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.TapasForSequenceClassification"><a name="//apple_ref/cpp/Class/transformers.TapasForSequenceClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TapasForSequenceClassification</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasForSequenceClassification" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Tapas Model with a sequence classification head on top (a linear layer on top of the pooled output), e.g. for table
entailment tasks, such as TabFact (Chen et al., 2020).</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its models (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) ‚Äì Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TapasForSequenceClassification.forward"><a name="//apple_ref/cpp/Method/transformers.TapasForSequenceClassification.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasForSequenceClassification.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasForSequenceClassification.forward" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TapasForSequenceClassification" title="transformers.TapasForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForSequenceClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì <p>Indices of input sequence tokens in the vocabulary. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">7)</span></code>, <cite>optional</cite>) ‚Äì <p>Token indices that encode tabular structure. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See this class for more info.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Indices of positions of each input sequence tokens in the position embeddings. If
<code class="docutils literal notranslate"><span class="pre">reset_position_index_per_cell</span></code> of <a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, relative
position embeddings will be used. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) ‚Äì Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: - 1
indicates the head is <strong>not masked</strong>, - 0 indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) ‚Äì Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model‚Äôs internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>) ‚Äì Labels for computing the sequence classification/regression loss. Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span>
<span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy). Note: this is called
‚Äúclassification_class_index‚Äù in the original implementation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput" title="transformers.modeling_outputs.SequenceClassifierOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SequenceClassifierOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided) ‚Äì Classification (or regression if config.num_labels==1) loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.num_labels)</span></code>) ‚Äì Classification (or regression if config.num_labels==1) scores (before SoftMax).</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasTokenizer</span><span class="p">,</span> <span class="n">TapasForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TapasTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-tabfact'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-tabfact'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Actors'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Brad Pitt"</span><span class="p">,</span> <span class="s2">"Leonardo Di Caprio"</span><span class="p">,</span> <span class="s2">"George Clooney"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Age'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"56"</span><span class="p">,</span> <span class="s2">"45"</span><span class="p">,</span> <span class="s2">"59"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Number of movies'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"87"</span><span class="p">,</span> <span class="s2">"53"</span><span class="p">,</span> <span class="s2">"69"</span><span class="p">]</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"There is only one actor who is 45 years old"</span><span class="p">,</span> <span class="s2">"There are 3 actors which played in more than 60 movies"</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 means entailed, 0 means refuted</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput" title="transformers.modeling_outputs.SequenceClassifierOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SequenceClassifierOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tapasforquestionanswering">
<h2>TapasForQuestionAnswering<a class="headerlink" href="#tapasforquestionanswering" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.TapasForQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.TapasForQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TapasForQuestionAnswering</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.models.tapas.configuration_tapas.TapasConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasForQuestionAnswering" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Tapas Model with a cell selection head and optional aggregation head on top for question-answering tasks on tables
(linear layers on top of the hidden-states output to compute <cite>logits</cite> and optional <cite>logits_aggregation</cite>), e.g. for
SQA, WTQ or WikiSQL-supervised tasks.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its models (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) ‚Äì Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TapasForQuestionAnswering.forward"><a name="//apple_ref/cpp/Method/transformers.TapasForQuestionAnswering.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">table_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aggregation_labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">float_answer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">numeric_values</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">numeric_values_scale</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/tapas/modeling_tapas.html#TapasForQuestionAnswering.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TapasForQuestionAnswering.forward" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì <p>Indices of input sequence tokens in the vocabulary. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">7)</span></code>, <cite>optional</cite>) ‚Äì <p>Token indices that encode tabular structure. Indices can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. See this class for more info.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Indices of positions of each input sequence tokens in the position embeddings. If
<code class="docutils literal notranslate"><span class="pre">reset_position_index_per_cell</span></code> of <a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, relative
position embeddings will be used. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) ‚Äì Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: - 1
indicates the head is <strong>not masked</strong>, - 0 indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) ‚Äì Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model‚Äôs internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>table_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_length)</span></code>, <cite>optional</cite>) ‚Äì Mask for the table. Indicates which tokens belong to the table (1). Question tokens, table headers and
padding are 0.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_length)</span></code>, <cite>optional</cite>) ‚Äì <p>Labels per token for computing the hierarchical cell selection loss. This encodes the positions of the
answer appearing in the table. Can be obtained using <a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>.</p>
<ul>
<li><p>1 for tokens that are <strong>part of the answer</strong>,</p></li>
<li><p>0 for tokens that are <strong>not part of the answer</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>aggregation_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">)</span></code>, <cite>optional</cite>) ‚Äì Aggregation function index for every example in the batch for computing the aggregation loss. Indices
should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_aggregation_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>. Only required in case of strong
supervision for aggregation (WikiSQL-supervised).</p></li>
<li><p><strong>float_answer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">)</span></code>, <cite>optional</cite>) ‚Äì Float answer for every example in the batch. Set to <cite>float(‚Äònan‚Äô)</cite> for cell selection questions. Only
required in case of weak supervision (WTQ) to calculate the aggregate mask and regression loss.</p></li>
<li><p><strong>numeric_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_length)</span></code>, <cite>optional</cite>) ‚Äì Numeric values of every token, NaN for tokens which are not numeric values. Can be obtained using
<a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>. Only required in case of weak supervision for aggregation (WTQ) to
calculate the regression loss.</p></li>
<li><p><strong>numeric_values_scale</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_length)</span></code>, <cite>optional</cite>) ‚Äì Scale of the numeric values of every token. Can be obtained using <a class="reference internal" href="#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a>.
Only required in case of weak supervision for aggregation (WTQ) to calculate the regression loss.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput" title="transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">TableQuestionAnsweringOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a>) and inputs.</p>
<ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> (and possibly <code class="xref py py-obj docutils literal notranslate"><span class="pre">answer</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">aggregation_labels</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">numeric_values</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">numeric_values_scale</span></code> are provided)) ‚Äì Total loss as the sum of the hierarchical cell selection log-likelihood loss and (optionally) the
semi-supervised regression loss and (optionally) supervised loss for aggregations.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) ‚Äì Prediction scores of the cell selection head, for every token.</p></li>
<li><p><strong>logits_aggregation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>, <cite>optional</cite>, of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_aggregation_labels)</span></code>) ‚Äì Prediction scores of the aggregation head, for every aggregation operator.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</p></li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) ‚Äì Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</p></li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TapasTokenizer</span><span class="p">,</span> <span class="n">TapasForQuestionAnswering</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TapasTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-wtq'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TapasForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-wtq'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Actors'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Brad Pitt"</span><span class="p">,</span> <span class="s2">"Leonardo Di Caprio"</span><span class="p">,</span> <span class="s2">"George Clooney"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Age'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"56"</span><span class="p">,</span> <span class="s2">"45"</span><span class="p">,</span> <span class="s2">"59"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="s1">'Number of movies'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"87"</span><span class="p">,</span> <span class="s2">"53"</span><span class="p">,</span> <span class="s2">"69"</span><span class="p">]</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"How many movies has George Clooney played in?"</span><span class="p">,</span> <span class="s2">"How old is Brad Pitt?"</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_aggregation</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits_aggregation</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput" title="transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">TableQuestionAnsweringOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="transformerxl.html" rel="next" title="Transformer XL">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="t5.html" rel="prev" title="T5"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        ¬© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>