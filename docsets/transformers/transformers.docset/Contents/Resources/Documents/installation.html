

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Installation &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Philosophy" href="philosophy.html" />
    <link rel="prev" title="Quick tour" href="quicktour.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation-with-pip">Installation with pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="#installing-from-source">Installing from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="#with-conda">With conda</a></li>
<li class="toctree-l2"><a class="reference internal" href="#caching-models">Caching models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#note-on-model-downloads-continuous-integration-or-large-scale-deployments">Note on model downloads (Continuous Integration or large-scale deployments)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device">Do you want to run a Transformer model on a mobile device?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Installation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/installation.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">Â¶</a></h1>
<p>ðŸ¤— Transformers is tested on Python 3.6+, and PyTorch 1.1.0+ or TensorFlow 2.0+.</p>
<p>You should install ðŸ¤— Transformers in a <a class="reference external" href="https://docs.python.org/3/library/venv.html">virtual environment</a>. If youâ€™re
unfamiliar with Python virtual environments, check out the <a class="reference external" href="https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/">user guide</a>. Create a virtual environment with the version of Python youâ€™re going
to use and activate it.</p>
<p>Now, if you want to use ðŸ¤— Transformers, you can install it with pip. If youâ€™d like to play with the examples, you
must install it from source.</p>
<div class="section" id="installation-with-pip">
<h2>Installation with pip<a class="headerlink" href="#installation-with-pip" title="Permalink to this headline">Â¶</a></h2>
<p>First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to <a class="reference external" href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available">TensorFlow installation page</a>,
<a class="reference external" href="https://pytorch.org/get-started/locally/#start-locally">PyTorch installation page</a> and/or
<a class="reference external" href="https://github.com/google/flax#quick-install">Flax installation page</a>
regarding the specific install command for your platform.</p>
<p>When TensorFlow 2.0 and/or PyTorch has been installed, ðŸ¤— Transformers can be installed using pip as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers
</pre></div>
</div>
<p>Alternatively, for CPU-support only, you can install ðŸ¤— Transformers and PyTorch in one line with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers<span class="o">[</span>torch<span class="o">]</span>
</pre></div>
</div>
<p>or ðŸ¤— Transformers and TensorFlow 2.0 in one line with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers<span class="o">[</span>tf-cpu<span class="o">]</span>
</pre></div>
</div>
<p>or ðŸ¤— Transformers and Flax in one line with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers<span class="o">[</span>flax<span class="o">]</span>
</pre></div>
</div>
<p>To check ðŸ¤— Transformers is properly installed, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -c <span class="s2">&quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;we love you&#39;))&quot;</span>
</pre></div>
</div>
<p>It should download a pretrained model then print something like</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[{</span><span class="s1">&#39;label&#39;</span>: <span class="s1">&#39;POSITIVE&#39;</span>, <span class="s1">&#39;score&#39;</span>: <span class="m">0</span>.9998704791069031<span class="o">}]</span>
</pre></div>
</div>
<p>(Note that TensorFlow will print additional stuff before that last statement.)</p>
</div>
<div class="section" id="installing-from-source">
<h2>Installing from source<a class="headerlink" href="#installing-from-source" title="Permalink to this headline">Â¶</a></h2>
<p>To install from source, clone the repository and install with the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/huggingface/transformers.git
<span class="nb">cd</span> transformers
pip install -e .
</pre></div>
</div>
<p>Again, you can run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -c <span class="s2">&quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I hate you&#39;))&quot;</span>
</pre></div>
</div>
<p>to check ðŸ¤— Transformers is properly installed.</p>
</div>
<div class="section" id="with-conda">
<h2>With conda<a class="headerlink" href="#with-conda" title="Permalink to this headline">Â¶</a></h2>
<p>Since Transformers version v4.0.0, we now have a conda channel: <code class="docutils literal notranslate"><span class="pre">huggingface</span></code>.</p>
<p>ðŸ¤— Transformers can be installed using conda as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">huggingface</span> <span class="n">transformers</span>
</pre></div>
</div>
<p>Follow the installation pages of TensorFlow, PyTorch or Flax to see how to install them with conda.</p>
</div>
<div class="section" id="caching-models">
<h2>Caching models<a class="headerlink" href="#caching-models" title="Permalink to this headline">Â¶</a></h2>
<p>This library provides pretrained models that will be downloaded and cached locally. Unless you specify a location with
<code class="docutils literal notranslate"><span class="pre">cache_dir=...</span></code> when you use methods like <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code>, these models will automatically be downloaded in the
folder given by the shell environment variable <code class="docutils literal notranslate"><span class="pre">TRANSFORMERS_CACHE</span></code>. The default value for it will be the Hugging
Face cache home followed by <code class="docutils literal notranslate"><span class="pre">/transformers/</span></code>. This is (by order of priority):</p>
<ul class="simple">
<li><p>shell environment variable <code class="docutils literal notranslate"><span class="pre">HF_HOME</span></code></p></li>
<li><p>shell environment variable <code class="docutils literal notranslate"><span class="pre">XDG_CACHE_HOME</span></code> + <code class="docutils literal notranslate"><span class="pre">/huggingface/</span></code></p></li>
<li><p>default: <code class="docutils literal notranslate"><span class="pre">~/.cache/huggingface/</span></code></p></li>
</ul>
<p>So if you donâ€™t have any specific environment variable set, the cache directory will be at
<code class="docutils literal notranslate"><span class="pre">~/.cache/huggingface/transformers/</span></code>.</p>
<p><strong>Note:</strong> If you have set a shell environment variable for one of the predecessors of this library
(<code class="docutils literal notranslate"><span class="pre">PYTORCH_TRANSFORMERS_CACHE</span></code> or <code class="docutils literal notranslate"><span class="pre">PYTORCH_PRETRAINED_BERT_CACHE</span></code>), those will be used if there is no shell
environment variable for <code class="docutils literal notranslate"><span class="pre">TRANSFORMERS_CACHE</span></code>.</p>
<div class="section" id="note-on-model-downloads-continuous-integration-or-large-scale-deployments">
<h3>Note on model downloads (Continuous Integration or large-scale deployments)<a class="headerlink" href="#note-on-model-downloads-continuous-integration-or-large-scale-deployments" title="Permalink to this headline">Â¶</a></h3>
<p>If you expect to be downloading large volumes of models (more than 1,000) from our hosted bucket (for instance through
your CI setup, or a large-scale production deployment), please cache the model files on your end. It will be way
faster, and cheaper. Feel free to contact us privately if you need any help.</p>
</div>
</div>
<div class="section" id="do-you-want-to-run-a-transformer-model-on-a-mobile-device">
<h2>Do you want to run a Transformer model on a mobile device?<a class="headerlink" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device" title="Permalink to this headline">Â¶</a></h2>
<p>You should check out our <a class="reference external" href="https://github.com/huggingface/swift-coreml-transformers">swift-coreml-transformers</a> repo.</p>
<p>It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains <code class="docutils literal notranslate"><span class="pre">GPT-2</span></code>,
<code class="docutils literal notranslate"><span class="pre">DistilGPT-2</span></code>, <code class="docutils literal notranslate"><span class="pre">BERT</span></code>, and <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code>) to CoreML models that run on iOS devices.</p>
<p>At some point in the future, youâ€™ll be able to seamlessly move from pretraining or fine-tuning models in PyTorch or
TensorFlow 2.0 to productizing them in CoreML, or prototype a model or an app in CoreML then research its
hyperparameters or architecture from PyTorch or TensorFlow 2.0. Super exciting!</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="philosophy.html" class="btn btn-neutral float-right" title="Philosophy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="quicktour.html" class="btn btn-neutral float-left" title="Quick tour" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>