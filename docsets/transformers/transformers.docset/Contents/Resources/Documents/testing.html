

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Testing &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exporting transformers models" href="serialization.html" />
    <link rel="prev" title="How to contribute to transformers?" href="contributing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#how-transformers-are-tested">How transformers are tested</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-tests">Running tests</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#choosing-which-tests-to-run">Choosing which tests to run</a></li>
<li class="toctree-l3"><a class="reference internal" href="#getting-the-list-of-all-tests">Getting the list of all tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-a-specific-test-module">Run a specific test module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-specific-tests">Run specific tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-only-modified-tests">Run only modified tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="#automatically-rerun-failed-tests-on-source-modification">Automatically rerun failed tests on source modification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#skip-a-test-module">Skip a test module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#clearing-state">Clearing state</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-tests-in-parallel">Running tests in parallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="#test-order-and-repetition">Test order and repetition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#repeat-tests">Repeat tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-tests-in-a-random-order">Run tests in a random order</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#look-and-feel-variations">Look and feel variations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pytest-sugar">pytest-sugar</a></li>
<li class="toctree-l4"><a class="reference internal" href="#report-each-sub-test-name-and-its-progress">Report each sub-test name and its progress</a></li>
<li class="toctree-l4"><a class="reference internal" href="#instantly-shows-failed-tests">Instantly shows failed tests</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#to-gpu-or-not-to-gpu">To GPU or not to GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-training">Distributed training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#output-capture">Output capture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#color-control">Color control</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sending-test-report-to-online-pastebin-service">Sending test report to online pastebin service</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#writing-tests">Writing tests</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#parametrization">Parametrization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#files-and-directories">Files and directories</a></li>
<li class="toctree-l3"><a class="reference internal" href="#temporary-files-and-directories">Temporary files and directories</a></li>
<li class="toctree-l3"><a class="reference internal" href="#skipping-tests">Skipping tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#slow-tests">Slow tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-the-stdout-stderr-output">Testing the stdout/stderr output</a></li>
<li class="toctree-l3"><a class="reference internal" href="#capturing-logger-stream">Capturing logger stream</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-with-environment-variables">Testing with environment variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#getting-reproducible-results">Getting reproducible results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#debugging-tests">Debugging tests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#testing-experimental-ci-features">Testing Experimental CI Features</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Testing</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/testing.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="testing">
<h1>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">Â¶</a></h1>
<p>Letâ€™s take a look at how ðŸ¤— Transformer models are tested and how you can write new tests and improve the existing ones.</p>
<p>There are 2 test suites in the repository:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tests</span></code> â€“ tests for the general API</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">examples</span></code> â€“ tests primarily for various applications that arenâ€™t part of the API</p></li>
</ol>
<div class="section" id="how-transformers-are-tested">
<h2>How transformers are tested<a class="headerlink" href="#how-transformers-are-tested" title="Permalink to this headline">Â¶</a></h2>
<ol class="arabic">
<li><p>Once a PR is submitted it gets tested with 9 CircleCi jobs. Every new commit to that PR gets retested. These jobs
are defined in this <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/.circleci/config.yml">config file</a>, so that if needed you can reproduce the same
environment on your machine.</p>
<p>These CI jobs donâ€™t run <code class="docutils literal notranslate"><span class="pre">&#64;slow</span></code> tests.</p>
</li>
<li><p>There are 3 jobs run by <a class="reference external" href="https://github.com/huggingface/transformers/actions">github actions</a>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/master/.github/workflows/github-torch-hub.yml">torch hub integration</a>: checks whether torch hub
integration works.</p></li>
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/master/.github/workflows/self-push.yml">self-hosted (push)</a>: runs fast tests on GPU only on commits on
<code class="docutils literal notranslate"><span class="pre">master</span></code>. It only runs if a commit on <code class="docutils literal notranslate"><span class="pre">master</span></code> has updated the code in one of the following folders: <code class="docutils literal notranslate"><span class="pre">src</span></code>,
<code class="docutils literal notranslate"><span class="pre">tests</span></code>, <code class="docutils literal notranslate"><span class="pre">.github</span></code> (to prevent running on added model cards, notebooks, etc.)</p></li>
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/master/.github/workflows/self-scheduled.yml">self-hosted runner</a>: runs normal and slow tests on GPU in
<code class="docutils literal notranslate"><span class="pre">tests</span></code> and <code class="docutils literal notranslate"><span class="pre">examples</span></code>:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">RUN_SLOW</span><span class="o">=</span><span class="m">1</span> pytest tests/
<span class="nv">RUN_SLOW</span><span class="o">=</span><span class="m">1</span> pytest examples/
</pre></div>
</div>
<p>The results can be observed <a class="reference external" href="https://github.com/huggingface/transformers/actions">here</a>.</p>
</li>
</ol>
</div>
<div class="section" id="running-tests">
<h2>Running tests<a class="headerlink" href="#running-tests" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="choosing-which-tests-to-run">
<h3>Choosing which tests to run<a class="headerlink" href="#choosing-which-tests-to-run" title="Permalink to this headline">Â¶</a></h3>
<p>This document goes into many details of how tests can be run. If after reading everything, you need even more details
you will find them <a class="reference external" href="https://docs.pytest.org/en/latest/usage.html">here</a>.</p>
<p>Here are some most useful ways of running tests.</p>
<p>Run all:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pytest</span>
</pre></div>
</div>
<p>or:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make <span class="nb">test</span>
</pre></div>
</div>
<p>Note that the latter is defined as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m pytest -n auto --dist<span class="o">=</span>loadfile -s -v ./tests/
</pre></div>
</div>
<p>which tells pytest to:</p>
<ul class="simple">
<li><p>run as many test processes as they are CPU cores (which could be too many if you donâ€™t have a ton of RAM!)</p></li>
<li><p>ensure that all tests from the same file will be run by the same test process</p></li>
<li><p>do not capture output</p></li>
<li><p>run in verbose mode</p></li>
</ul>
</div>
<div class="section" id="getting-the-list-of-all-tests">
<h3>Getting the list of all tests<a class="headerlink" href="#getting-the-list-of-all-tests" title="Permalink to this headline">Â¶</a></h3>
<p>All tests of the test suite:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --collect-only -q
</pre></div>
</div>
<p>All tests of a given test file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests/test_optimization.py --collect-only -q
</pre></div>
</div>
</div>
<div class="section" id="run-a-specific-test-module">
<h3>Run a specific test module<a class="headerlink" href="#run-a-specific-test-module" title="Permalink to this headline">Â¶</a></h3>
<p>To run an individual test module:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests/test_logging.py
</pre></div>
</div>
</div>
<div class="section" id="run-specific-tests">
<h3>Run specific tests<a class="headerlink" href="#run-specific-tests" title="Permalink to this headline">Â¶</a></h3>
<p>Since unittest is used inside most of the tests, to run specific subtests you need to know the name of the unittest
class containing those tests. For example, it could be:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests/test_optimization.py::OptimizationTest::test_adam_w
</pre></div>
</div>
<p>Here:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tests/test_optimization.py</span></code> - the file with tests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OptimizationTest</span></code> - the name of the class</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_adam_w</span></code> - the name of the specific test function</p></li>
</ul>
<p>If the file contains multiple classes, you can choose to run only tests of a given class. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests/test_optimization.py::OptimizationTest
</pre></div>
</div>
<p>will run all the tests inside that class.</p>
<p>As mentioned earlier you can see what tests are contained inside the <code class="docutils literal notranslate"><span class="pre">OptimizationTest</span></code> class by running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests/test_optimization.py::OptimizationTest --collect-only -q
</pre></div>
</div>
<p>You can run tests by keyword expressions.</p>
<p>To run only tests whose name contains <code class="docutils literal notranslate"><span class="pre">adam</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -k adam tests/test_optimization.py
</pre></div>
</div>
<p>To run all tests except those whose name contains <code class="docutils literal notranslate"><span class="pre">adam</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -k <span class="s2">&quot;not adam&quot;</span> tests/test_optimization.py
</pre></div>
</div>
<p>And you can combine the two patterns in one:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -k <span class="s2">&quot;ada and not adam&quot;</span> tests/test_optimization.py
</pre></div>
</div>
</div>
<div class="section" id="run-only-modified-tests">
<h3>Run only modified tests<a class="headerlink" href="#run-only-modified-tests" title="Permalink to this headline">Â¶</a></h3>
<p>You can run the tests related to the unstaged files or the current branch (according to Git) by using <a class="reference external" href="https://github.com/anapaulagomes/pytest-picked">pytest-picked</a>. This is a great way of quickly testing your changes didnâ€™t break
anything, since it wonâ€™t run the tests related to files you didnâ€™t touch.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-picked
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --picked
</pre></div>
</div>
<p>All tests will be run from files and folders which are modified, but not yet committed.</p>
</div>
<div class="section" id="automatically-rerun-failed-tests-on-source-modification">
<h3>Automatically rerun failed tests on source modification<a class="headerlink" href="#automatically-rerun-failed-tests-on-source-modification" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a> provides a very useful feature of detecting all failed
tests, and then waiting for you to modify files and continuously re-rerun those failing tests until they pass while you
fix them. So that you donâ€™t need to re start pytest after you made the fix. This is repeated until all tests pass after
which again a full run is performed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-xdist
</pre></div>
</div>
<p>To enter the mode: <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">-f</span></code> or <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">--looponfail</span></code></p>
<p>File changes are detected by looking at <code class="docutils literal notranslate"><span class="pre">looponfailroots</span></code> root directories and all of their contents (recursively).
If the default for this value does not work for you, you can change it in your project by setting a configuration
option in <code class="docutils literal notranslate"><span class="pre">setup.cfg</span></code>:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[tool:pytest]</span>
<span class="na">looponfailroots</span> <span class="o">=</span> <span class="s">transformers tests</span>
</pre></div>
</div>
<p>or <code class="docutils literal notranslate"><span class="pre">pytest.ini</span></code>/<code class="docutils literal notranslate"><span class="pre">tox.ini</span></code> files:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[pytest]</span>
<span class="na">looponfailroots</span> <span class="o">=</span> <span class="s">transformers tests</span>
</pre></div>
</div>
<p>This would lead to only looking for file changes in the respective directories, specified relatively to the ini-fileâ€™s
directory.</p>
<p><a class="reference external" href="https://github.com/joeyespo/pytest-watch">pytest-watch</a> is an alternative implementation of this functionality.</p>
</div>
<div class="section" id="skip-a-test-module">
<h3>Skip a test module<a class="headerlink" href="#skip-a-test-module" title="Permalink to this headline">Â¶</a></h3>
<p>If you want to run all test modules, except a few you can exclude them by giving an explicit list of tests to run. For
example, to run all except <code class="docutils literal notranslate"><span class="pre">test_modeling_*.py</span></code> tests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest <span class="sb">`</span>ls -1 tests/*py <span class="p">|</span> grep -v test_modeling<span class="sb">`</span>
</pre></div>
</div>
</div>
<div class="section" id="clearing-state">
<h3>Clearing state<a class="headerlink" href="#clearing-state" title="Permalink to this headline">Â¶</a></h3>
<p>CI builds and when isolation is important (against speed), cache should be cleared:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --cache-clear tests
</pre></div>
</div>
</div>
<div class="section" id="running-tests-in-parallel">
<h3>Running tests in parallel<a class="headerlink" href="#running-tests-in-parallel" title="Permalink to this headline">Â¶</a></h3>
<p>As mentioned earlier <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">test</span></code> runs tests in parallel via <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code> plugin (<code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">X</span></code> argument, e.g. <code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">2</span></code>
to run 2 parallel jobs).</p>
<p><code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code>â€™s <code class="docutils literal notranslate"><span class="pre">--dist=</span></code> option allows one to control how the tests are grouped. <code class="docutils literal notranslate"><span class="pre">--dist=loadfile</span></code> puts the
tests located in one file onto the same process.</p>
<p>Since the order of executed tests is different and unpredictable, if running the test suite with <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code>
produces failures (meaning we have some undetected coupled tests), use <a class="reference external" href="https://github.com/ESSS/pytest-replay">pytest-replay</a> to replay the tests in the same order, which should help with then somehow
reducing that failing sequence to a minimum.</p>
</div>
<div class="section" id="test-order-and-repetition">
<h3>Test order and repetition<a class="headerlink" href="#test-order-and-repetition" title="Permalink to this headline">Â¶</a></h3>
<p>Itâ€™s good to repeat the tests several times, in sequence, randomly, or in sets, to detect any potential
inter-dependency and state-related bugs (tear down). And the straightforward multiple repetition is just good to detect
some problems that get uncovered by randomness of DL.</p>
<div class="section" id="repeat-tests">
<h4>Repeat tests<a class="headerlink" href="#repeat-tests" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/dropbox/pytest-flakefinder">pytest-flakefinder</a>:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-flakefinder
</pre></div>
</div>
<p>And then run every test multiple times (50 by default):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --flake-finder --flake-runs<span class="o">=</span><span class="m">5</span> tests/test_failing_test.py
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This plugin doesnâ€™t work with <code class="docutils literal notranslate"><span class="pre">-n</span></code> flag from <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is another plugin <code class="docutils literal notranslate"><span class="pre">pytest-repeat</span></code>, but it doesnâ€™t work with <code class="docutils literal notranslate"><span class="pre">unittest</span></code>.</p>
</div>
</div>
<div class="section" id="run-tests-in-a-random-order">
<h4>Run tests in a random order<a class="headerlink" href="#run-tests-in-a-random-order" title="Permalink to this headline">Â¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-random-order
</pre></div>
</div>
<p>Important: the presence of <code class="docutils literal notranslate"><span class="pre">pytest-random-order</span></code> will automatically randomize tests, no configuration change or
command line options is required.</p>
<p>As explained earlier this allows detection of coupled tests - where one testâ€™s state affects the state of another. When
<code class="docutils literal notranslate"><span class="pre">pytest-random-order</span></code> is installed it will print the random seed it used for that session, e.g:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests
<span class="o">[</span>...<span class="o">]</span>
Using --random-order-bucket<span class="o">=</span>module
Using --random-order-seed<span class="o">=</span><span class="m">573663</span>
</pre></div>
</div>
<p>So that if the given particular sequence fails, you can reproduce it by adding that exact seed, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --random-order-seed<span class="o">=</span><span class="m">573663</span>
<span class="o">[</span>...<span class="o">]</span>
Using --random-order-bucket<span class="o">=</span>module
Using --random-order-seed<span class="o">=</span><span class="m">573663</span>
</pre></div>
</div>
<p>It will only reproduce the exact order if you use the exact same list of tests (or no list at all). Once you start to
manually narrowing down the list you can no longer rely on the seed, but have to list them manually in the exact order
they failed and tell pytest to not randomize them instead using <code class="docutils literal notranslate"><span class="pre">--random-order-bucket=none</span></code>, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --random-order-bucket<span class="o">=</span>none tests/test_a.py tests/test_c.py tests/test_b.py
</pre></div>
</div>
<p>To disable the shuffling for all tests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --random-order-bucket<span class="o">=</span>none
</pre></div>
</div>
<p>By default <code class="docutils literal notranslate"><span class="pre">--random-order-bucket=module</span></code> is implied, which will shuffle the files on the module levels. It can also
shuffle on <code class="docutils literal notranslate"><span class="pre">class</span></code>, <code class="docutils literal notranslate"><span class="pre">package</span></code>, <code class="docutils literal notranslate"><span class="pre">global</span></code> and <code class="docutils literal notranslate"><span class="pre">none</span></code> levels. For the complete details please see its
<a class="reference external" href="https://github.com/jbasko/pytest-random-order">documentation</a>.</p>
<p>Another randomization alternative is: <code class="docutils literal notranslate"><span class="pre">pytest-randomly</span></code> &lt;<a class="reference external" href="https://github.com/pytest-dev/pytest-randomly">https://github.com/pytest-dev/pytest-randomly</a>&gt;`__. This
module has a very similar functionality/interface, but it doesnâ€™t have the bucket modes available in
<code class="docutils literal notranslate"><span class="pre">pytest-random-order</span></code>. It has the same problem of imposing itself once installed.</p>
</div>
</div>
<div class="section" id="look-and-feel-variations">
<h3>Look and feel variations<a class="headerlink" href="#look-and-feel-variations" title="Permalink to this headline">Â¶</a></h3>
<div class="section" id="pytest-sugar">
<h4>pytest-sugar<a class="headerlink" href="#pytest-sugar" title="Permalink to this headline">Â¶</a></h4>
<p><a class="reference external" href="https://github.com/Frozenball/pytest-sugar">pytest-sugar</a> is a plugin that improves the look-n-feel, adds a
progressbar, and show tests that fail and the assert instantly. It gets activated automatically upon installation.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-sugar
</pre></div>
</div>
<p>To run tests without it, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -p no:sugar
</pre></div>
</div>
<p>or uninstall it.</p>
</div>
<div class="section" id="report-each-sub-test-name-and-its-progress">
<h4>Report each sub-test name and its progress<a class="headerlink" href="#report-each-sub-test-name-and-its-progress" title="Permalink to this headline">Â¶</a></h4>
<p>For a single or a group of tests via <code class="docutils literal notranslate"><span class="pre">pytest</span></code> (after <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pytest-pspec</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --pspec tests/test_optimization.py
</pre></div>
</div>
</div>
<div class="section" id="instantly-shows-failed-tests">
<h4>Instantly shows failed tests<a class="headerlink" href="#instantly-shows-failed-tests" title="Permalink to this headline">Â¶</a></h4>
<p><a class="reference external" href="https://github.com/pytest-dev/pytest-instafail">pytest-instafail</a> shows failures and errors instantly instead of
waiting until the end of test session.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-instafail
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --instafail
</pre></div>
</div>
</div>
</div>
<div class="section" id="to-gpu-or-not-to-gpu">
<h3>To GPU or not to GPU<a class="headerlink" href="#to-gpu-or-not-to-gpu" title="Permalink to this headline">Â¶</a></h3>
<p>On a GPU-enabled setup, to test in CPU-only mode add <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES=&quot;&quot;</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">&quot;&quot;</span> pytest tests/test_logging.py
</pre></div>
</div>
<p>or if you have multiple gpus, you can specify which one is to be used by <code class="docutils literal notranslate"><span class="pre">pytest</span></code>. For example, to use only the
second gpu if you have gpus <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, you can run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">&quot;1&quot;</span> pytest tests/test_logging.py
</pre></div>
</div>
<p>This is handy when you want to run different tasks on different GPUs.</p>
<p>Some tests must be run on CPU-only, others on either CPU or GPU or TPU, yet others on multiple-GPUs. The following skip
decorators are used to set the requirements of tests CPU/GPU/TPU-wise:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">require_torch</span></code> - this test will run only under torch</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">require_torch_gpu</span></code> - as <code class="docutils literal notranslate"><span class="pre">require_torch</span></code> plus requires at least 1 GPU</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">require_torch_multi_gpu</span></code> - as <code class="docutils literal notranslate"><span class="pre">require_torch</span></code> plus requires at least 2 GPUs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">require_torch_non_multi_gpu</span></code> - as <code class="docutils literal notranslate"><span class="pre">require_torch</span></code> plus requires 0 or 1 GPUs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">require_torch_tpu</span></code> - as <code class="docutils literal notranslate"><span class="pre">require_torch</span></code> plus requires at least 1 TPU</p></li>
</ul>
<p>Letâ€™s depict the GPU requirements in the following table:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 23%" />
<col style="width: 77%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>n gpus</p></th>
<th class="head"><p>decorator</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">&gt;=</span> <span class="pre">0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&#64;require_torch</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">&gt;=</span> <span class="pre">1</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&#64;require_torch_gpu</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">&gt;=</span> <span class="pre">2</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&#64;require_torch_multi_gpu</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">&lt;</span> <span class="pre">2</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&#64;require_torch_non_multi_gpu</span></code></p></td>
</tr>
</tbody>
</table>
<p>For example, here is a test that must be run only when there are 2 or more GPUs available and pytorch is installed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@require_torch_multi_gpu</span>
<span class="k">def</span> <span class="nf">test_example_with_multi_gpu</span><span class="p">():</span>
</pre></div>
</div>
<p>If a test requires <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> use the <code class="docutils literal notranslate"><span class="pre">require_tf</span></code> decorator. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@require_tf</span>
<span class="k">def</span> <span class="nf">test_tf_thing_with_tensorflow</span><span class="p">():</span>
</pre></div>
</div>
<p>These decorators can be stacked. For example, if a test is slow and requires at least one GPU under pytorch, here is
how to set it up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@require_torch_gpu</span>
<span class="nd">@slow</span>
<span class="k">def</span> <span class="nf">test_example_slow_on_gpu</span><span class="p">():</span>
</pre></div>
</div>
<p>Some decorators like <code class="docutils literal notranslate"><span class="pre">&#64;parametrized</span></code> rewrite test names, therefore <code class="docutils literal notranslate"><span class="pre">&#64;require_*</span></code> skip decorators have to be listed
last for them to work correctly. Here is an example of the correct usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@parameterized</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="nd">@require_torch_multi_gpu</span>
<span class="k">def</span> <span class="nf">test_integration_foo</span><span class="p">():</span>
</pre></div>
</div>
<p>This order problem doesnâ€™t exist with <code class="docutils literal notranslate"><span class="pre">&#64;pytest.mark.parametrize</span></code>, you can put it first or last and it will still
work. But it only works with non-unittests.</p>
<p>Inside tests:</p>
<ul class="simple">
<li><p>How many GPUs are available:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>from transformers.testing_utils import get_gpu_count
<span class="nv">n_gpu</span> <span class="o">=</span> get_gpu_count<span class="o">()</span> <span class="c1"># works with torch and tf</span>
</pre></div>
</div>
</div>
<div class="section" id="distributed-training">
<h3>Distributed training<a class="headerlink" href="#distributed-training" title="Permalink to this headline">Â¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">pytest</span></code> canâ€™t deal with distributed training directly. If this is attempted - the sub-processes donâ€™t do the right
thing and end up thinking they are <code class="docutils literal notranslate"><span class="pre">pytest</span></code> and start running the test suite in loops. It works, however, if one
spawns a normal process that then spawns off multiple workers and manages the IO pipes.</p>
<p>This is still under development but you can study 2 different tests that perform this successfully:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/seq2seq/test_seq2seq_examples_multi_gpu.py">test_seq2seq_examples_multi_gpu.py</a> - a
<code class="docutils literal notranslate"><span class="pre">pytorch-lightning</span></code>-running test (had to use PLâ€™s <code class="docutils literal notranslate"><span class="pre">ddp</span></code> spawning method which is the default)</p></li>
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/seq2seq/test_finetune_trainer.py">test_finetune_trainer.py</a> - a normal (non-PL) test</p></li>
</ul>
<p>To jump right into the execution point, search for the <code class="docutils literal notranslate"><span class="pre">execute_subprocess_async</span></code> function in those tests.</p>
<p>You will need at least 2 GPUs to see these tests in action:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">&quot;0,1&quot;</span> <span class="nv">RUN_SLOW</span><span class="o">=</span><span class="m">1</span> pytest -sv examples/seq2seq/test_finetune_trainer.py <span class="se">\</span>
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
</pre></div>
</div>
</div>
<div class="section" id="output-capture">
<h3>Output capture<a class="headerlink" href="#output-capture" title="Permalink to this headline">Â¶</a></h3>
<p>During test execution any output sent to <code class="docutils literal notranslate"><span class="pre">stdout</span></code> and <code class="docutils literal notranslate"><span class="pre">stderr</span></code> is captured. If a test or a setup method fails, its
according captured output will usually be shown along with the failure traceback.</p>
<p>To disable output capturing and to get the <code class="docutils literal notranslate"><span class="pre">stdout</span></code> and <code class="docutils literal notranslate"><span class="pre">stderr</span></code> normally, use <code class="docutils literal notranslate"><span class="pre">-s</span></code> or <code class="docutils literal notranslate"><span class="pre">--capture=no</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -s tests/test_logging.py
</pre></div>
</div>
<p>To send test results to JUnit format output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>py.test tests --junitxml<span class="o">=</span>result.xml
</pre></div>
</div>
</div>
<div class="section" id="color-control">
<h3>Color control<a class="headerlink" href="#color-control" title="Permalink to this headline">Â¶</a></h3>
<p>To have no color (e.g., yellow on white background is not readable):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --color<span class="o">=</span>no tests/test_logging.py
</pre></div>
</div>
</div>
<div class="section" id="sending-test-report-to-online-pastebin-service">
<h3>Sending test report to online pastebin service<a class="headerlink" href="#sending-test-report-to-online-pastebin-service" title="Permalink to this headline">Â¶</a></h3>
<p>Creating a URL for each test failure:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --pastebin<span class="o">=</span>failed tests/test_logging.py
</pre></div>
</div>
<p>This will submit test run information to a remote Paste service and provide a URL for each failure. You may select
tests as usual or add for example -x if you only want to send one particular failure.</p>
<p>Creating a URL for a whole test session log:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --pastebin<span class="o">=</span>all tests/test_logging.py
</pre></div>
</div>
</div>
</div>
<div class="section" id="writing-tests">
<h2>Writing tests<a class="headerlink" href="#writing-tests" title="Permalink to this headline">Â¶</a></h2>
<p>ðŸ¤— transformers tests are based on <code class="docutils literal notranslate"><span class="pre">unittest</span></code>, but run by <code class="docutils literal notranslate"><span class="pre">pytest</span></code>, so most of the time features from both systems
can be used.</p>
<p>You can read <a class="reference external" href="https://docs.pytest.org/en/stable/unittest.html">here</a> which features are supported, but the important
thing to remember is that most <code class="docutils literal notranslate"><span class="pre">pytest</span></code> fixtures donâ€™t work. Neither parametrization, but we use the module
<code class="docutils literal notranslate"><span class="pre">parameterized</span></code> that works in a similar way.</p>
<div class="section" id="parametrization">
<h3>Parametrization<a class="headerlink" href="#parametrization" title="Permalink to this headline">Â¶</a></h3>
<p>Often, there is a need to run the same test multiple times, but with different arguments. It could be done from within
the test, but then there is no way of running that test for just one set of arguments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># test_this1.py</span>
<span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">from</span> <span class="nn">parameterized</span> <span class="kn">import</span> <span class="n">parameterized</span>
<span class="k">class</span> <span class="nc">TestMathUnitTest</span><span class="p">(</span><span class="n">unittest</span><span class="o">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="nd">@parameterized</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span>
        <span class="p">(</span><span class="s2">&quot;negative&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;integer&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;large fraction&quot;</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">])</span>
    <span class="k">def</span> <span class="nf">test_floor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
        <span class="n">assert_equal</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, by default this test will be run 3 times, each time with the last 3 arguments of <code class="docutils literal notranslate"><span class="pre">test_floor</span></code> being assigned the
corresponding arguments in the parameter list.</p>
<p>and you could run just the <code class="docutils literal notranslate"><span class="pre">negative</span></code> and <code class="docutils literal notranslate"><span class="pre">integer</span></code> sets of params with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -k <span class="s2">&quot;negative and integer&quot;</span> tests/test_mytest.py
</pre></div>
</div>
<p>or all but <code class="docutils literal notranslate"><span class="pre">negative</span></code> sub-tests, with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest -k <span class="s2">&quot;not negative&quot;</span> tests/test_mytest.py
</pre></div>
</div>
<p>Besides using the <code class="docutils literal notranslate"><span class="pre">-k</span></code> filter that was just mentioned, you can find out the exact name of each sub-test and run any
or all of them using their exact names.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest test_this1.py --collect-only -q
</pre></div>
</div>
<p>and it will list:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>test_this1.py::TestMathUnitTest::test_floor_0_negative
test_this1.py::TestMathUnitTest::test_floor_1_integer
test_this1.py::TestMathUnitTest::test_floor_2_large_fraction
</pre></div>
</div>
<p>So now you can run just 2 specific sub-tests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest test_this1.py::TestMathUnitTest::test_floor_0_negative  test_this1.py::TestMathUnitTest::test_floor_1_integer
</pre></div>
</div>
<p>The module <a class="reference external" href="https://pypi.org/project/parameterized/">parameterized</a> which is already in the developer dependencies
of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> works for both: <code class="docutils literal notranslate"><span class="pre">unittests</span></code> and <code class="docutils literal notranslate"><span class="pre">pytest</span></code> tests.</p>
<p>If, however, the test is not a <code class="docutils literal notranslate"><span class="pre">unittest</span></code>, you may use <code class="docutils literal notranslate"><span class="pre">pytest.mark.parametrize</span></code> (or you may see it being used in
some existing tests, mostly under <code class="docutils literal notranslate"><span class="pre">examples</span></code>).</p>
<p>Here is the same example, this time using <code class="docutils literal notranslate"><span class="pre">pytest</span></code>â€™s <code class="docutils literal notranslate"><span class="pre">parametrize</span></code> marker:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># test_this2.py</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span>
    <span class="s2">&quot;name, input, expected&quot;</span><span class="p">,</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;negative&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;integer&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;large fraction&quot;</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">test_floor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
    <span class="n">assert_equal</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
</div>
<p>Same as with <code class="docutils literal notranslate"><span class="pre">parameterized</span></code>, with <code class="docutils literal notranslate"><span class="pre">pytest.mark.parametrize</span></code> you can have a fine control over which sub-tests are
run, if the <code class="docutils literal notranslate"><span class="pre">-k</span></code> filter doesnâ€™t do the job. Except, this parametrization function creates a slightly different set of
names for the sub-tests. Here is what they look like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest test_this2.py --collect-only -q
</pre></div>
</div>
<p>and it will list:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>test_this2.py::test_floor<span class="o">[</span>integer-1-1.0<span class="o">]</span>
test_this2.py::test_floor<span class="o">[</span>negative--1.5--2.0<span class="o">]</span>
test_this2.py::test_floor<span class="o">[</span>large fraction-1.6-1<span class="o">]</span>
</pre></div>
</div>
<p>So now you can run just the specific test:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest test_this2.py::test_floor<span class="o">[</span>negative--1.5--2.0<span class="o">]</span> test_this2.py::test_floor<span class="o">[</span>integer-1-1.0<span class="o">]</span>
</pre></div>
</div>
<p>as in the previous example.</p>
</div>
<div class="section" id="files-and-directories">
<h3>Files and directories<a class="headerlink" href="#files-and-directories" title="Permalink to this headline">Â¶</a></h3>
<p>In tests often we need to know where things are relative to the current test file, and itâ€™s not trivial since the test
could be invoked from more than one directory or could reside in sub-directories with different depths. A helper class
<code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers.test_utils.TestCasePlus</span></code> solves this problem by sorting out all the basic paths and provides easy
accessors to them:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">pathlib</span></code> objects (all fully resolved):</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_file_path</span></code> - the current test file path, i.e. <code class="docutils literal notranslate"><span class="pre">__file__</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_file_dir</span></code> - the directory containing the current test file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tests_dir</span></code> - the directory of the <code class="docutils literal notranslate"><span class="pre">tests</span></code> test suite</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">examples_dir</span></code> - the directory of the <code class="docutils literal notranslate"><span class="pre">examples</span></code> test suite</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">repo_root_dir</span></code> - the directory of the repository</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">src_dir</span></code> - the directory of <code class="docutils literal notranslate"><span class="pre">src</span></code> (i.e. where the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> sub-dir resides)</p></li>
</ul>
</div></blockquote>
</li>
<li><p>stringified pathsâ€”same as above but these return paths as strings, rather than <code class="docutils literal notranslate"><span class="pre">pathlib</span></code> objects:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_file_path_str</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_file_dir_str</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tests_dir_str</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">examples_dir_str</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">repo_root_dir_str</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">src_dir_str</span></code></p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>To start using those all you need is to make sure that the test resides in a subclass of
<code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers.test_utils.TestCasePlus</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">TestCasePlus</span>
<span class="k">class</span> <span class="nc">PathExampleTest</span><span class="p">(</span><span class="n">TestCasePlus</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_something_involving_local_locations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">examples_dir</span> <span class="o">/</span> <span class="s2">&quot;seq2seq/test_data/wmt_en_ro&quot;</span>
</pre></div>
</div>
<p>If you donâ€™t need to manipulated paths via <code class="docutils literal notranslate"><span class="pre">pathlib</span></code> or you just need a path as a string, you can always invoked
<code class="docutils literal notranslate"><span class="pre">str()</span></code> on the <code class="docutils literal notranslate"><span class="pre">pathlib</span></code> oboject or use the accessors ending with <code class="docutils literal notranslate"><span class="pre">_str</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">TestCasePlus</span>
<span class="k">class</span> <span class="nc">PathExampleTest</span><span class="p">(</span><span class="n">TestCasePlus</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_something_involving_stringified_locations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">examples_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">examples_dir_str</span>
</pre></div>
</div>
</div>
<div class="section" id="temporary-files-and-directories">
<h3>Temporary files and directories<a class="headerlink" href="#temporary-files-and-directories" title="Permalink to this headline">Â¶</a></h3>
<p>Using unique temporary files and directories are essential for parallel test running, so that the tests wonâ€™t overwrite
each otherâ€™s data. Also we want to get the temporary files and directories removed at the end of each test that created
them. Therefore, using packages like <code class="docutils literal notranslate"><span class="pre">tempfile</span></code>, which address these needs is essential.</p>
<p>However, when debugging tests, you need to be able to see what goes into the temporary file or directory and you want
to know itâ€™s exact path and not having it randomized on every test re-run.</p>
<p>A helper class <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers.test_utils.TestCasePlus</span></code> is best used for such purposes. Itâ€™s a sub-class of
<code class="xref py py-obj docutils literal notranslate"><span class="pre">unittest.TestCase</span></code>, so we can easily inherit from it in the test modules.</p>
<p>Here is an example of its usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">TestCasePlus</span>
<span class="k">class</span> <span class="nc">ExamplesTests</span><span class="p">(</span><span class="n">TestCasePlus</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_whatever</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tmp_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_auto_remove_tmp_dir</span><span class="p">()</span>
</pre></div>
</div>
<p>This code creates a unique temporary directory, and sets <code class="xref py py-obj docutils literal notranslate"><span class="pre">tmp_dir</span></code> to its location.</p>
<ul class="simple">
<li><p>Create a unique temporary dir:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_whatever</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">tmp_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_auto_remove_tmp_dir</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tmp_dir</span></code> will contain the path to the created temporary dir. It will be automatically removed at the end of the
test.</p>
<ul class="simple">
<li><p>Create a temporary dir of my choice, ensure itâ€™s empty before the test starts and donâ€™t empty it after the test.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_whatever</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">tmp_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_auto_remove_tmp_dir</span><span class="p">(</span><span class="s2">&quot;./xxx&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This is useful for debug when you want to monitor a specific directory and want to make sure the previous tests didnâ€™t
leave any data in there.</p>
<ul>
<li><p>You can override the default behavior by directly overriding the <code class="docutils literal notranslate"><span class="pre">before</span></code> and <code class="docutils literal notranslate"><span class="pre">after</span></code> args, leading to one of the
following behaviors:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">before=True</span></code>: the temporary dir will always be cleared at the beginning of the test.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">before=False</span></code>: if the temporary dir already existed, any existing files will remain there.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">after=True</span></code>: the temporary dir will always be deleted at the end of the test.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">after=False</span></code>: the temporary dir will always be left intact at the end of the test.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to run the equivalent of <code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">-r</span></code> safely, only subdirs of the project repository checkout are allowed if
an explicit obj:<cite>tmp_dir</cite> is used, so that by mistake no <code class="docutils literal notranslate"><span class="pre">/tmp</span></code> or similar important part of the filesystem will
get nuked. i.e. please always pass paths that start with <code class="docutils literal notranslate"><span class="pre">./</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each test can register multiple temporary directories and they all will get auto-removed, unless requested
otherwise.</p>
</div>
</div>
<div class="section" id="skipping-tests">
<h3>Skipping tests<a class="headerlink" href="#skipping-tests" title="Permalink to this headline">Â¶</a></h3>
<p>This is useful when a bug is found and a new test is written, yet the bug is not fixed yet. In order to be able to
commit it to the main repository we need make sure itâ€™s skipped during <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">test</span></code>.</p>
<p>Methods:</p>
<ul class="simple">
<li><p>A <strong>skip</strong> means that you expect your test to pass only if some conditions are met, otherwise pytest should skip
running the test altogether. Common examples are skipping windows-only tests on non-windows platforms, or skipping
tests that depend on an external resource which is not available at the moment (for example a database).</p></li>
<li><p>A <strong>xfail</strong> means that you expect a test to fail for some reason. A common example is a test for a feature not yet
implemented, or a bug not yet fixed. When a test passes despite being expected to fail (marked with
pytest.mark.xfail), itâ€™s an xpass and will be reported in the test summary.</p></li>
</ul>
<p>One of the important differences between the two is that <code class="docutils literal notranslate"><span class="pre">skip</span></code> doesnâ€™t run the test, and <code class="docutils literal notranslate"><span class="pre">xfail</span></code> does. So if the
code thatâ€™s buggy causes some bad state that will affect other tests, do not use <code class="docutils literal notranslate"><span class="pre">xfail</span></code>.</p>
<div class="section" id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><p>Here is how to skip whole test unconditionally:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@unittest</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="s2">&quot;this bug needs to be fixed&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">():</span>
</pre></div>
</div>
<p>or via pytest:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">reason</span><span class="o">=</span><span class="s2">&quot;this bug needs to be fixed&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>or the <code class="docutils literal notranslate"><span class="pre">xfail</span></code> way:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">xfail</span>
<span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">():</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Here is how to skip a test based on some internal check inside the test:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_something</span><span class="p">():</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="s2">&quot;unsupported configuration&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>or the whole module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">pytest</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--custom-flag&quot;</span><span class="p">):</span>
    <span class="n">pytest</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="s2">&quot;--custom-flag is missing, skipping tests&quot;</span><span class="p">,</span> <span class="n">allow_module_level</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>or the <code class="docutils literal notranslate"><span class="pre">xfail</span></code> way:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">():</span>
    <span class="n">pytest</span><span class="o">.</span><span class="n">xfail</span><span class="p">(</span><span class="s2">&quot;expected to fail until bug XYZ is fixed&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Here is how to skip all tests in a module if some import is missing:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">docutils</span> <span class="o">=</span> <span class="n">pytest</span><span class="o">.</span><span class="n">importorskip</span><span class="p">(</span><span class="s2">&quot;docutils&quot;</span><span class="p">,</span> <span class="n">minversion</span><span class="o">=</span><span class="s2">&quot;0.3&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Skip a test based on a condition:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">skipif</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">reason</span><span class="o">=</span><span class="s2">&quot;requires python3.6 or higher&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">():</span>
</pre></div>
</div>
<p>or:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@unittest</span><span class="o">.</span><span class="n">skipIf</span><span class="p">(</span><span class="n">torch_device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;Can&#39;t do half precision&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">():</span>
</pre></div>
</div>
<p>or skip the whole module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">skipif</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span><span class="p">,</span> <span class="n">reason</span><span class="o">=</span><span class="s2">&quot;does not run on windows&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TestClass</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">test_feature_x</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</pre></div>
</div>
<p>More details, example and ways are <a class="reference external" href="https://docs.pytest.org/en/latest/skipping.html">here</a>.</p>
</div>
</div>
<div class="section" id="slow-tests">
<h3>Slow tests<a class="headerlink" href="#slow-tests" title="Permalink to this headline">Â¶</a></h3>
<p>The library of tests is ever-growing, and some of the tests take minutes to run, therefore we canâ€™t afford waiting for
an hour for the test suite to complete on CI. Therefore, with some exceptions for essential tests, slow tests should be
marked as in the example below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">slow</span>
<span class="nd">@slow</span>
<span class="k">def</span> <span class="nf">test_integration_foo</span><span class="p">():</span>
</pre></div>
</div>
<p>Once a test is marked as <code class="docutils literal notranslate"><span class="pre">&#64;slow</span></code>, to run such tests set <code class="docutils literal notranslate"><span class="pre">RUN_SLOW=1</span></code> env var, e.g.:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">RUN_SLOW</span><span class="o">=</span><span class="m">1</span> pytest tests
</pre></div>
</div>
<p>Some decorators like <code class="docutils literal notranslate"><span class="pre">&#64;parameterized</span></code> rewrite test names, therefore <code class="docutils literal notranslate"><span class="pre">&#64;slow</span></code> and the rest of the skip decorators
<code class="docutils literal notranslate"><span class="pre">&#64;require_*</span></code> have to be listed last for them to work correctly. Here is an example of the correct usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@parameterized</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="nd">@slow</span>
<span class="k">def</span> <span class="nf">test_integration_foo</span><span class="p">():</span>
</pre></div>
</div>
<p>As explained at the beginning of this document, slow tests get to run on a scheduled basis, rather than in PRs CI
checks. So itâ€™s possible that some problems will be missed during a PR submission and get merged. Such problems will
get caught during the next scheduled CI job. But it also means that itâ€™s important to run the slow tests on your
machine before submitting the PR.</p>
<p>Here is a rough decision making mechanism for choosing which tests should be marked as slow:</p>
<p>If the test is focused on one of the libraryâ€™s internal components (e.g., modeling files, tokenization files,
pipelines), then we should run that test in the non-slow test suite. If itâ€™s focused on an other aspect of the library,
such as the documentation or the examples, then we should run these tests in the slow test suite. And then, to refine
this approach we should have exceptions:</p>
<ul class="simple">
<li><p>All tests that need to download a heavy set of weights or a dataset that is larger than ~50MB (e.g., model or
tokenizer integration tests, pipeline integration tests) should be set to slow. If youâ€™re adding a new model, you
should create and upload to the hub a tiny version of it (with random weights) for integration tests. This is
discussed in the following paragraphs.</p></li>
<li><p>All tests that need to do a training not specifically optimized to be fast should be set to slow.</p></li>
<li><p>We can introduce exceptions if some of these should-be-non-slow tests are excruciatingly slow, and set them to
<code class="docutils literal notranslate"><span class="pre">&#64;slow</span></code>. Auto-modeling tests, which save and load large files to disk, are a good example of tests that are marked
as <code class="docutils literal notranslate"><span class="pre">&#64;slow</span></code>.</p></li>
<li><p>If a test completes under 1 second on CI (including downloads if any) then it should be a normal test regardless.</p></li>
</ul>
<p>Collectively, all the non-slow tests need to cover entirely the different internals, while remaining fast. For example,
a significant coverage can be achieved by testing with specially created tiny models with random weights. Such models
have the very minimal number of layers (e.g., 2), vocab size (e.g., 1000), etc. Then the <code class="docutils literal notranslate"><span class="pre">&#64;slow</span></code> tests can use large
slow models to do qualitative testing. To see the use of these simply look for <em>tiny</em> models with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>grep tiny tests examples
</pre></div>
</div>
<p>Here is a an example of a <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/scripts/fsmt/fsmt-make-tiny-model.py">script</a> that created the tiny model
<a class="reference external" href="https://huggingface.co/stas/tiny-wmt19-en-de">stas/tiny-wmt19-en-de</a>. You can easily adjust it to your specific
modelâ€™s architecture.</p>
<p>Itâ€™s easy to measure the run-time incorrectly if for example there is an overheard of downloading a huge model, but if
you test it locally the downloaded files would be cached and thus the download time not measured. Hence check the
execution speed report in CI logs instead (the output of <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">--durations=0</span> <span class="pre">tests</span></code>).</p>
<p>That report is also useful to find slow outliers that arenâ€™t marked as such, or which need to be re-written to be fast.
If you notice that the test suite starts getting slow on CI, the top listing of this report will show the slowest
tests.</p>
</div>
<div class="section" id="testing-the-stdout-stderr-output">
<h3>Testing the stdout/stderr output<a class="headerlink" href="#testing-the-stdout-stderr-output" title="Permalink to this headline">Â¶</a></h3>
<p>In order to test functions that write to <code class="docutils literal notranslate"><span class="pre">stdout</span></code> and/or <code class="docutils literal notranslate"><span class="pre">stderr</span></code>, the test can access those streams using the
<code class="docutils literal notranslate"><span class="pre">pytest</span></code>â€™s <a class="reference external" href="https://docs.pytest.org/en/latest/capture.html">capsys system</a>. Here is how this is accomplished:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="k">def</span> <span class="nf">print_to_stdout</span><span class="p">(</span><span class="n">s</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">print_to_stderr</span><span class="p">(</span><span class="n">s</span><span class="p">):</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_result_and_stdout</span><span class="p">(</span><span class="n">capsys</span><span class="p">):</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>
    <span class="n">print_to_stdout</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">print_to_stderr</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">err</span> <span class="o">=</span> <span class="n">capsys</span><span class="o">.</span><span class="n">readouterr</span><span class="p">()</span> <span class="c1"># consume the captured output streams</span>
    <span class="c1"># optional: if you want to replay the consumed streams:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="c1"># test:</span>
    <span class="k">assert</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">out</span>
    <span class="k">assert</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">err</span>
</pre></div>
</div>
<p>And, of course, most of the time, <code class="docutils literal notranslate"><span class="pre">stderr</span></code> will come as a part of an exception, so try/except has to be used in such
a case:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">raise_exception</span><span class="p">(</span><span class="n">msg</span><span class="p">):</span> <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_something_exception</span><span class="p">():</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Not a good value&quot;</span>
    <span class="n">error</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">raise_exception</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">error</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">error</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2"> is in the exception:</span><span class="se">\n</span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Another approach to capturing stdout is via <code class="docutils literal notranslate"><span class="pre">contextlib.redirect_stdout</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">redirect_stdout</span>
<span class="k">def</span> <span class="nf">print_to_stdout</span><span class="p">(</span><span class="n">s</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_result_and_stdout</span><span class="p">():</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">redirect_stdout</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
        <span class="n">print_to_stdout</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">getvalue</span><span class="p">()</span>
    <span class="c1"># optional: if you want to replay the consumed streams:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="c1"># test:</span>
    <span class="k">assert</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">out</span>
</pre></div>
</div>
<p>An important potential issue with capturing stdout is that it may contain <code class="docutils literal notranslate"><span class="pre">\r</span></code> characters that in normal <code class="docutils literal notranslate"><span class="pre">print</span></code>
reset everything that has been printed so far. There is no problem with <code class="docutils literal notranslate"><span class="pre">pytest</span></code>, but with <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">-s</span></code> these
characters get included in the buffer, so to be able to have the test run with and without <code class="docutils literal notranslate"><span class="pre">-s</span></code>, you have to make an
extra cleanup to the captured output, using <code class="docutils literal notranslate"><span class="pre">re.sub(r'~.*\r',</span> <span class="pre">'',</span> <span class="pre">buf,</span> <span class="pre">0,</span> <span class="pre">re.M)</span></code>.</p>
<p>But, then we have a helper context manager wrapper to automatically take care of it all, regardless of whether it has
some <code class="docutils literal notranslate"><span class="pre">\r</span></code>â€™s in it or not, so itâ€™s a simple:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">CaptureStdout</span>
<span class="k">with</span> <span class="n">CaptureStdout</span><span class="p">()</span> <span class="k">as</span> <span class="n">cs</span><span class="p">:</span>
    <span class="n">function_that_writes_to_stdout</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cs</span><span class="o">.</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is a full test example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">CaptureStdout</span>
<span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Secret message</span><span class="se">\r</span><span class="s2">&quot;</span>
<span class="n">final</span> <span class="o">=</span> <span class="s2">&quot;Hello World&quot;</span>
<span class="k">with</span> <span class="n">CaptureStdout</span><span class="p">()</span> <span class="k">as</span> <span class="n">cs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">msg</span> <span class="o">+</span> <span class="n">final</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">cs</span><span class="o">.</span><span class="n">out</span> <span class="o">==</span> <span class="n">final</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;captured: </span><span class="si">{</span><span class="n">cs</span><span class="o">.</span><span class="n">out</span><span class="si">}</span><span class="s2">, expecting </span><span class="si">{</span><span class="n">final</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>If youâ€™d like to capture <code class="docutils literal notranslate"><span class="pre">stderr</span></code> use the <code class="xref py py-obj docutils literal notranslate"><span class="pre">CaptureStderr</span></code> class instead:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">CaptureStderr</span>
<span class="k">with</span> <span class="n">CaptureStderr</span><span class="p">()</span> <span class="k">as</span> <span class="n">cs</span><span class="p">:</span>
    <span class="n">function_that_writes_to_stderr</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cs</span><span class="o">.</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</div>
<p>If you need to capture both streams at once, use the parent <code class="xref py py-obj docutils literal notranslate"><span class="pre">CaptureStd</span></code> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">CaptureStd</span>
<span class="k">with</span> <span class="n">CaptureStd</span><span class="p">()</span> <span class="k">as</span> <span class="n">cs</span><span class="p">:</span>
    <span class="n">function_that_writes_to_stdout_and_stderr</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cs</span><span class="o">.</span><span class="n">err</span><span class="p">,</span> <span class="n">cs</span><span class="o">.</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="capturing-logger-stream">
<h3>Capturing logger stream<a class="headerlink" href="#capturing-logger-stream" title="Permalink to this headline">Â¶</a></h3>
<p>If you need to validate the output of a logger, you can use <code class="xref py py-obj docutils literal notranslate"><span class="pre">CaptureLogger</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">CaptureLogger</span>

<span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Testing 1, 2, 3&quot;</span>
<span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_info</span><span class="p">()</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="s2">&quot;transformers.models.bart.tokenization_bart&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">CaptureLogger</span><span class="p">(</span><span class="n">logger</span><span class="p">)</span> <span class="k">as</span> <span class="n">cl</span><span class="p">:</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">cl</span><span class="o">.</span><span class="n">out</span><span class="p">,</span> <span class="n">msg</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="testing-with-environment-variables">
<h3>Testing with environment variables<a class="headerlink" href="#testing-with-environment-variables" title="Permalink to this headline">Â¶</a></h3>
<p>If you want to test the impact of environment variables for a specific test you can use a helper decorator
<code class="docutils literal notranslate"><span class="pre">transformers.testing_utils.mockenv</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">mockenv</span>
<span class="k">class</span> <span class="nc">HfArgumentParserTest</span><span class="p">(</span><span class="n">unittest</span><span class="o">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="nd">@mockenv</span><span class="p">(</span><span class="n">TRANSFORMERS_VERBOSITY</span><span class="o">=</span><span class="s2">&quot;error&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">test_env_override</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">env_level_str</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TRANSFORMERS_VERBOSITY&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>At times an external program needs to be called, which requires setting <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code> in <code class="docutils literal notranslate"><span class="pre">os.environ</span></code> to include
multiple local paths. A helper class <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers.test_utils.TestCasePlus</span></code> comes to help:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.testing_utils</span> <span class="kn">import</span> <span class="n">TestCasePlus</span>
<span class="k">class</span> <span class="nc">EnvExampleTest</span><span class="p">(</span><span class="n">TestCasePlus</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_external_prog</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_env</span><span class="p">()</span>
        <span class="c1"># now call the external program, passing ``env`` to it</span>
</pre></div>
</div>
<p>Depending on whether the test file was under the <code class="docutils literal notranslate"><span class="pre">tests</span></code> test suite or <code class="docutils literal notranslate"><span class="pre">examples</span></code> itâ€™ll correctly set up
<code class="docutils literal notranslate"><span class="pre">env[PYTHONPATH]</span></code> to include one of these two directories, and also the <code class="docutils literal notranslate"><span class="pre">src</span></code> directory to ensure the testing is
done against the current repo, and finally with whatever <code class="docutils literal notranslate"><span class="pre">env[PYTHONPATH]</span></code> was already set to before the test was
called if anything.</p>
<p>This helper method creates a copy of the <code class="docutils literal notranslate"><span class="pre">os.environ</span></code> object, so the original remains intact.</p>
</div>
<div class="section" id="getting-reproducible-results">
<h3>Getting reproducible results<a class="headerlink" href="#getting-reproducible-results" title="Permalink to this headline">Â¶</a></h3>
<p>In some situations you may want to remove randomness for your tests. To get identical reproducable results set, you
will need to fix the seed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>

<span class="c1"># python RNG</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># pytorch RNGs</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># numpy RNG</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># tf RNG</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="debugging-tests">
<h3>Debugging tests<a class="headerlink" href="#debugging-tests" title="Permalink to this headline">Â¶</a></h3>
<p>To start a debugger at the point of the warning, do this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest tests/test_logging.py -W error::UserWarning --pdb
</pre></div>
</div>
</div>
</div>
<div class="section" id="testing-experimental-ci-features">
<h2>Testing Experimental CI Features<a class="headerlink" href="#testing-experimental-ci-features" title="Permalink to this headline">Â¶</a></h2>
<p>Testing CI features can be potentially problematic as it can interfere with the normal CI functioning. Therefore if a
new CI feature is to be added, it should be done as following.</p>
<ol class="arabic simple">
<li><p>Create a new dedicated job that tests what needs to be tested</p></li>
<li><p>The new job must always succeed so that it gives us a green âœ“ (details below).</p></li>
<li><p>Let it run for some days to see that a variety of different PR types get to run on it (user fork branches,
non-forked branches, branches originating from github.com UI direct file edit, various forced pushes, etc. - there
are so many) while monitoring the experimental jobâ€™s logs (not the overall job green as itâ€™s purposefully always
green)</p></li>
<li><p>When itâ€™s clear that everything is solid, then merge the new changes into existing jobs.</p></li>
</ol>
<p>That way experiments on CI functionality itself wonâ€™t interfere with the normal workflow.</p>
<p>Now how can we make the job always succeed while the new CI feature is being developed?</p>
<p>Some CIs, like TravisCI support ignore-step-failure and will report the overall job as successful, but CircleCI and
Github Actions as of this writing donâ€™t support that.</p>
<p>So the following workaround can be used:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">set</span> <span class="pre">+euo</span> <span class="pre">pipefail</span></code> at the beginning of the run command to suppress most potential failures in the bash script.</p></li>
<li><p>the last command must be a success: <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">&quot;done&quot;</span></code> or just <code class="docutils literal notranslate"><span class="pre">true</span></code> will do</p></li>
</ol>
<p>Here is an example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">-</span> <span class="nt">run</span><span class="p">:</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">run CI experiment</span>
    <span class="nt">command</span><span class="p">:</span> <span class="p p-Indicator">|</span>
        <span class="no">set +euo pipefail</span>
        <span class="no">echo &quot;setting run-all-despite-any-errors-mode&quot;</span>
        <span class="no">this_command_will_fail</span>
        <span class="no">echo &quot;but bash continues to run&quot;</span>
        <span class="no"># emulate another failure</span>
        <span class="no">false</span>
        <span class="no"># but the last command must be a success</span>
        <span class="no">echo &quot;during experiment do not remove: reporting success to CI, even if there were failures&quot;</span>
</pre></div>
</div>
<p>For simple commands you could also do:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmd_that_may_fail <span class="o">||</span> <span class="nb">true</span>
</pre></div>
</div>
<p>Of course, once satisfied with the results, integrate the experimental step or job with the rest of the normal jobs,
while removing <code class="docutils literal notranslate"><span class="pre">set</span> <span class="pre">+euo</span> <span class="pre">pipefail</span></code> or any other things you may have added to ensure that the experimental job doesnâ€™t
interfere with the normal CI functioning.</p>
<p>This whole process would have been much easier if we only could set something like <code class="docutils literal notranslate"><span class="pre">allow-failure</span></code> for the
experimental step, and let it fail without impacting the overall status of PRs. But as mentioned earlier CircleCI and
Github Actions donâ€™t support it at the moment.</p>
<p>You can vote for this feature and see where it is at at these CI-specific threads:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/actions/toolkit/issues/399">Github Actions:</a></p></li>
<li><p><a class="reference external" href="https://ideas.circleci.com/ideas/CCI-I-344">CircleCI:</a></p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="serialization.html" class="btn btn-neutral float-right" title="Exporting transformers models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="contributing.html" class="btn btn-neutral float-left" title="How to contribute to transformers?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>