

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Summary of the tokenizers &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi-lingual models" href="multilingual.html" />
    <link rel="prev" title="Model sharing and uploading" href="model_sharing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using 🤗 Transformers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Summary of the tokenizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#subword-tokenization">Subword tokenization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#byte-pair-encoding-bpe">Byte-Pair Encoding (BPE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#byte-level-bpe">Byte-level BPE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#wordpiece">WordPiece</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unigram">Unigram</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sentencepiece">SentencePiece</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">🤗 Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Summary of the tokenizers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/tokenizer_summary.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="summary-of-the-tokenizers">
<h1>Summary of the tokenizers<a class="headerlink" href="#summary-of-the-tokenizers" title="Permalink to this headline">¶</a></h1>
<p>On this page, we will have a closer look at tokenization. As we saw in <a class="reference internal" href="preprocessing.html"><span class="doc">the preprocessing tutorial</span></a>, tokenizing a text is splitting it into words or subwords, which then are converted to ids through a
look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a
text into words or subwords (i.e. tokenizing a text). More specifically, we will look at the three main types of
tokenizers used in 🤗 Transformers: <a class="reference internal" href="#byte-pair-encoding"><span class="std std-ref">Byte-Pair Encoding (BPE)</span></a>, <a class="reference internal" href="#wordpiece"><span class="std std-ref">WordPiece</span></a>,
and <a class="reference internal" href="#sentencepiece"><span class="std std-ref">SentencePiece</span></a>, and show examples of which tokenizer type is used by which model.</p>
<p>Note that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer
type was used by the pretrained model. For instance, if we look at <a class="reference internal" href="model_doc/bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a>, we can see
that the model uses <a class="reference internal" href="#wordpiece"><span class="std std-ref">WordPiece</span></a>.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Splitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so.
For instance, let’s look at the sentence <code class="docutils literal notranslate"><span class="pre">&quot;Don't</span> <span class="pre">you</span> <span class="pre">love</span> <span class="pre">🤗</span> <span class="pre">Transformers?</span> <span class="pre">We</span> <span class="pre">sure</span> <span class="pre">do.&quot;</span></code> A simple way of tokenizing
this text is to split it by spaces, which would give:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;Don&#39;t&quot;</span><span class="p">,</span> <span class="s2">&quot;you&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;🤗&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformers?&quot;</span><span class="p">,</span> <span class="s2">&quot;We&quot;</span><span class="p">,</span> <span class="s2">&quot;sure&quot;</span><span class="p">,</span> <span class="s2">&quot;do.&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>This is a sensible first step, but if we look at the tokens <code class="docutils literal notranslate"><span class="pre">&quot;Transformers?&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;do.&quot;</span></code>, we notice that the
punctuation is attached to the words <code class="docutils literal notranslate"><span class="pre">&quot;Transformer&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;do&quot;</span></code>, which is suboptimal. We should take the
punctuation into account so that a model does not have to learn a different representation of a word and every possible
punctuation symbol that could follow it, which would explode the number of representations the model has to learn.
Taking punctuation into account, tokenizing our exemplary text would give:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;Don&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;t&quot;</span><span class="p">,</span> <span class="s2">&quot;you&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;🤗&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformers&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">,</span> <span class="s2">&quot;We&quot;</span><span class="p">,</span> <span class="s2">&quot;sure&quot;</span><span class="p">,</span> <span class="s2">&quot;do&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Better. However, it is disadvantageous, how the tokenization dealt with the word <code class="docutils literal notranslate"><span class="pre">&quot;Don't&quot;</span></code>. <code class="docutils literal notranslate"><span class="pre">&quot;Don't&quot;</span></code> stands for
<code class="docutils literal notranslate"><span class="pre">&quot;do</span> <span class="pre">not&quot;</span></code>, so it would be better tokenized as <code class="docutils literal notranslate"><span class="pre">[&quot;Do&quot;,</span> <span class="pre">&quot;n't&quot;]</span></code>. This is where things start getting complicated, and
part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a
different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an
input that was tokenized with the same rules that were used to tokenize its training data.</p>
<p><a class="reference external" href="https://spacy.io/">spaCy</a> and <a class="reference external" href="http://www.statmt.org/moses/?n=Development.GetStarted">Moses</a> are two popular
rule-based tokenizers. Applying them on our example, <em>spaCy</em> and <em>Moses</em> would output something like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;Do&quot;</span><span class="p">,</span> <span class="s2">&quot;n&#39;t&quot;</span><span class="p">,</span> <span class="s2">&quot;you&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;🤗&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformers&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">,</span> <span class="s2">&quot;We&quot;</span><span class="p">,</span> <span class="s2">&quot;sure&quot;</span><span class="p">,</span> <span class="s2">&quot;do&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and
punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined
as splitting sentences into words. While it’s the most intuitive way to split texts into smaller chunks, this
tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization
usually generates a very big vocabulary (the set of all unique words and tokens used). <em>E.g.</em>, <a class="reference internal" href="model_doc/transformerxl.html"><span class="doc">Transformer XL</span></a> uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!</p>
<p>Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which
causes both an increased memory and time complexity. In general, transformers models rarely have a vocabulary size
greater than 50,000, especially if they are pretrained only on a single language.</p>
<p>So if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters? While
character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for
the model to learn meaningful input representations. <em>E.g.</em> learning a meaningful context-independent representation
for the letter <code class="docutils literal notranslate"><span class="pre">&quot;t&quot;</span></code> is much harder than learning a context-independent representation for the word <code class="docutils literal notranslate"><span class="pre">&quot;today&quot;</span></code>.
Therefore, character tokenization is often accompanied by a loss of performance. So to get the best of both worlds,
transformers models use a hybrid between word-level and character-level tokenization called <strong>subword</strong> tokenization.</p>
<div class="section" id="subword-tokenization">
<h3>Subword tokenization<a class="headerlink" href="#subword-tokenization" title="Permalink to this headline">¶</a></h3>
<p>Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller
subwords, but rare words should be decomposed into meaningful subwords. For instance <code class="docutils literal notranslate"><span class="pre">&quot;annoyingly&quot;</span></code> might be
considered a rare word and could be decomposed into <code class="docutils literal notranslate"><span class="pre">&quot;annoying&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;ly&quot;</span></code>. Both <code class="docutils literal notranslate"><span class="pre">&quot;annoying&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;ly&quot;</span></code> as
stand-alone subwords would appear more frequently while at the same time the meaning of <code class="docutils literal notranslate"><span class="pre">&quot;annoyingly&quot;</span></code> is kept by the
composite meaning of <code class="docutils literal notranslate"><span class="pre">&quot;annoying&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;ly&quot;</span></code>. This is especially useful in agglutinative languages such as Turkish,
where you can form (almost) arbitrarily long complex words by stringing together subwords.</p>
<p>Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful
context-independent representations. In addition, subword tokenization enables the model to process words it has never
seen before, by decomposing them into known subwords. For instance, the <a class="reference internal" href="model_doc/bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> tokenizes
<code class="docutils literal notranslate"><span class="pre">&quot;I</span> <span class="pre">have</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">GPU!&quot;</span></code> as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;I have a new GPU!&quot;</span><span class="p">)</span>
<span class="go">[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;, &quot;gp&quot;, &quot;##u&quot;, &quot;!&quot;]</span>
</pre></div>
</div>
<p>Because we are considering the uncased model, the sentence was lowercased first. We can see that the words <code class="docutils literal notranslate"><span class="pre">[&quot;i&quot;,</span>
<span class="pre">&quot;have&quot;,</span> <span class="pre">&quot;a&quot;,</span> <span class="pre">&quot;new&quot;]</span></code> are present in the tokenizer’s vocabulary, but the word <code class="docutils literal notranslate"><span class="pre">&quot;gpu&quot;</span></code> is not. Consequently, the
tokenizer splits <code class="docutils literal notranslate"><span class="pre">&quot;gpu&quot;</span></code> into known subwords: <code class="docutils literal notranslate"><span class="pre">[&quot;gp&quot;</span> <span class="pre">and</span> <span class="pre">&quot;##u&quot;]</span></code>. <code class="docutils literal notranslate"><span class="pre">&quot;##&quot;</span></code> means that the rest of the token should
be attached to the previous one, without space (for decoding or reversal of the tokenization).</p>
<p>As another example, <a class="reference internal" href="model_doc/xlnet.html#transformers.XLNetTokenizer" title="transformers.XLNetTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetTokenizer</span></code></a> tokenizes our previously exemplary text as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">XLNetTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">XLNetTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;xlnet-base-cased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Don&#39;t you love 🤗 Transformers? We sure do.&quot;</span><span class="p">)</span>
<span class="go">[&quot;▁Don&quot;, &quot;&#39;&quot;, &quot;t&quot;, &quot;▁you&quot;, &quot;▁love&quot;, &quot;▁&quot;, &quot;🤗&quot;, &quot;▁&quot;, &quot;Transform&quot;, &quot;ers&quot;, &quot;?&quot;, &quot;▁We&quot;, &quot;▁sure&quot;, &quot;▁do&quot;, &quot;.&quot;]</span>
</pre></div>
</div>
<p>We’ll get back to the meaning of those <code class="docutils literal notranslate"><span class="pre">&quot;▁&quot;</span></code> when we look at <a class="reference internal" href="#sentencepiece"><span class="std std-ref">SentencePiece</span></a>. As one can see,
the rare word <code class="docutils literal notranslate"><span class="pre">&quot;Transformers&quot;</span></code> has been split into the more frequent subwords <code class="docutils literal notranslate"><span class="pre">&quot;Transform&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;ers&quot;</span></code>.</p>
<p>Let’s now look at how the different subword tokenization algorithms work. Note that all of those tokenization
algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained
on.</p>
</div>
</div>
<div class="section" id="byte-pair-encoding-bpe">
<span id="byte-pair-encoding"></span><h2>Byte-Pair Encoding (BPE)<a class="headerlink" href="#byte-pair-encoding-bpe" title="Permalink to this headline">¶</a></h2>
<p>Byte-Pair Encoding (BPE) was introduced in <a class="reference external" href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units (Sennrich et
al., 2015)</a>. BPE relies on a pre-tokenizer that splits the training data into
words. Pretokenization can be as simple as space tokenization, e.g. <a class="reference internal" href="model_doc/gpt2.html"><span class="doc">GPT-2</span></a>, <a class="reference internal" href="model_doc/roberta.html"><span class="doc">Roberta</span></a>. More advanced pre-tokenization include rule-based tokenization, e.g. <a class="reference internal" href="model_doc/xlm.html"><span class="doc">XLM</span></a>,
<a class="reference internal" href="model_doc/flaubert.html"><span class="doc">FlauBERT</span></a> which uses Moses for most languages, or <a class="reference internal" href="model_doc/gpt.html"><span class="doc">GPT</span></a> which uses
Spacy and ftfy, to count the frequency of each word in the training corpus.</p>
<p>After pre-tokenization, a set of unique words has been created and the frequency of each word it occurred in the
training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set
of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until
the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to
define before training the tokenizer.</p>
<p>As an example, let’s assume that after pre-tokenization, the following set of words including their frequency has been
determined:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s2">&quot;hug&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;pug&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;pun&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;bun&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;hugs&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Consequently, the base vocabulary is <code class="docutils literal notranslate"><span class="pre">[&quot;b&quot;,</span> <span class="pre">&quot;g&quot;,</span> <span class="pre">&quot;h&quot;,</span> <span class="pre">&quot;n&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;s&quot;,</span> <span class="pre">&quot;u&quot;]</span></code>. Splitting all words into symbols of the
base vocabulary, we obtain:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s2">&quot;h&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;p&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;p&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;h&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;g&quot;</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In
the example above <code class="docutils literal notranslate"><span class="pre">&quot;h&quot;</span></code> followed by <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code> is present <cite>10 + 5 = 15</cite> times (10 times in the 10 occurrences of
<code class="docutils literal notranslate"><span class="pre">&quot;hug&quot;</span></code>, 5 times in the 5 occurrences of “hugs”). However, the most frequent symbol pair is <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code> followed by “g”,
occurring <cite>10 + 5 + 5 = 20</cite> times in total. Thus, the first merge rule the tokenizer learns is to group all <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code>
symbols followed by a <code class="docutils literal notranslate"><span class="pre">&quot;g&quot;</span></code> symbol together. Next, “ug” is added to the vocabulary. The set of words then becomes</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s2">&quot;h&quot;</span> <span class="s2">&quot;ug&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;p&quot;</span> <span class="s2">&quot;ug&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;p&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span> <span class="s2">&quot;u&quot;</span> <span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;h&quot;</span> <span class="s2">&quot;ug&quot;</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>BPE then identifies the next most common symbol pair. It’s <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code> followed by <code class="docutils literal notranslate"><span class="pre">&quot;n&quot;</span></code>, which occurs 16 times. <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code>,
<code class="docutils literal notranslate"><span class="pre">&quot;n&quot;</span></code> is merged to <code class="docutils literal notranslate"><span class="pre">&quot;un&quot;</span></code> and added to the vocabulary. The next most frequent symbol pair is <code class="docutils literal notranslate"><span class="pre">&quot;h&quot;</span></code> followed by
<code class="docutils literal notranslate"><span class="pre">&quot;ug&quot;</span></code>, occurring 15 times. Again the pair is merged and <code class="docutils literal notranslate"><span class="pre">&quot;hug&quot;</span></code> can be added to the vocabulary.</p>
<p>At this stage, the vocabulary is <code class="docutils literal notranslate"><span class="pre">[&quot;b&quot;,</span> <span class="pre">&quot;g&quot;,</span> <span class="pre">&quot;h&quot;,</span> <span class="pre">&quot;n&quot;,</span> <span class="pre">&quot;p&quot;,</span> <span class="pre">&quot;s&quot;,</span> <span class="pre">&quot;u&quot;,</span> <span class="pre">&quot;ug&quot;,</span> <span class="pre">&quot;un&quot;,</span> <span class="pre">&quot;hug&quot;]</span></code> and our set of unique words
is represented as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s2">&quot;hug&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;p&quot;</span> <span class="s2">&quot;ug&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;p&quot;</span> <span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span> <span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;hug&quot;</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Assuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied
to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance,
the word <code class="docutils literal notranslate"><span class="pre">&quot;bug&quot;</span></code> would be tokenized to <code class="docutils literal notranslate"><span class="pre">[&quot;b&quot;,</span> <span class="pre">&quot;ug&quot;]</span></code> but <code class="docutils literal notranslate"><span class="pre">&quot;mug&quot;</span></code> would be tokenized as <code class="docutils literal notranslate"><span class="pre">[&quot;&lt;unk&gt;&quot;,</span> <span class="pre">&quot;ug&quot;]</span></code> since
the symbol <code class="docutils literal notranslate"><span class="pre">&quot;m&quot;</span></code> is not in the base vocabulary. In general, single letters such as <code class="docutils literal notranslate"><span class="pre">&quot;m&quot;</span></code> are not replaced by the
<code class="docutils literal notranslate"><span class="pre">&quot;&lt;unk&gt;&quot;</span></code> symbol because the training data usually includes at least one occurrence of each letter, but it is likely
to happen for very special characters like emojis.</p>
<p>As mentioned earlier, the vocabulary size, <em>i.e.</em> the base vocabulary size + the number of merges, is a hyperparameter
to choose. For instance <a class="reference internal" href="model_doc/gpt.html"><span class="doc">GPT</span></a> has a vocabulary size of 40,478 since they have 478 base characters
and chose to stop training after 40,000 merges.</p>
<div class="section" id="byte-level-bpe">
<h3>Byte-level BPE<a class="headerlink" href="#byte-level-bpe" title="Permalink to this headline">¶</a></h3>
<p>A base vocabulary that includes all possible base characters can be quite large if <em>e.g.</em> all unicode characters are
considered as base characters. To have a better base vocabulary, <a class="reference external" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a> uses bytes
as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that
every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2’s
tokenizer can tokenize every text without the need for the &lt;unk&gt; symbol. <a class="reference internal" href="model_doc/gpt.html"><span class="doc">GPT-2</span></a> has a vocabulary
size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned
with 50,000 merges.</p>
<div class="section" id="wordpiece">
<span id="id1"></span><h4>WordPiece<a class="headerlink" href="#wordpiece" title="Permalink to this headline">¶</a></h4>
<p>WordPiece is the subword tokenization algorithm used for <a class="reference internal" href="model_doc/bert.html"><span class="doc">BERT</span></a>, <a class="reference internal" href="model_doc/distilbert.html"><span class="doc">DistilBERT</span></a>, and <a class="reference internal" href="model_doc/electra.html"><span class="doc">Electra</span></a>. The algorithm was outlined in <a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">Japanese and Korean
Voice Search (Schuster et al., 2012)</a> and is very similar to
BPE. WordPiece first initializes the vocabulary to include every character present in the training data and
progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent
symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.</p>
<p>So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is
equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by
its second symbol is the greatest among all symbol pairs. <em>E.g.</em> <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code>, followed by <code class="docutils literal notranslate"><span class="pre">&quot;g&quot;</span></code> would have only been
merged if the probability of <code class="docutils literal notranslate"><span class="pre">&quot;ug&quot;</span></code> divided by <code class="docutils literal notranslate"><span class="pre">&quot;u&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;g&quot;</span></code> would have been greater than for any other symbol
pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it <cite>loses</cite> by merging two symbols
to make ensure it’s <cite>worth it</cite>.</p>
</div>
<div class="section" id="unigram">
<span id="id2"></span><h4>Unigram<a class="headerlink" href="#unigram" title="Permalink to this headline">¶</a></h4>
<p>Unigram is a subword tokenization algorithm introduced in <a class="reference external" href="https://arxiv.org/pdf/1804.10959.pdf">Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)</a>. In contrast to BPE or
WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each
symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and
the most common substrings. Unigram is not used directly for any of the models in the transformers, but it’s used in
conjunction with <a class="reference internal" href="#sentencepiece"><span class="std std-ref">SentencePiece</span></a>.</p>
<p>At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training
data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm
computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then
removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, <em>i.e.</em> those
symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has
reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.</p>
<p>Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of
tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;u&quot;</span><span class="p">,</span> <span class="s2">&quot;ug&quot;</span><span class="p">,</span> <span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;hug&quot;</span><span class="p">],</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&quot;hugs&quot;</span></code> could be tokenized both as <code class="docutils literal notranslate"><span class="pre">[&quot;hug&quot;,</span> <span class="pre">&quot;s&quot;]</span></code>, <code class="docutils literal notranslate"><span class="pre">[&quot;h&quot;,</span> <span class="pre">&quot;ug&quot;,</span> <span class="pre">&quot;s&quot;]</span></code> or <code class="docutils literal notranslate"><span class="pre">[&quot;h&quot;,</span> <span class="pre">&quot;u&quot;,</span> <span class="pre">&quot;g&quot;,</span> <span class="pre">&quot;s&quot;]</span></code>. So which one
to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that
the probability of each possible tokenization can be computed after training. The algorithm simply picks the most
likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their
probabilities.</p>
<p>Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of
the words <span class="math notranslate nohighlight">\(x_{1}, \dots, x_{N}\)</span> and that the set of all possible tokenizations for a word <span class="math notranslate nohighlight">\(x_{i}\)</span> is
defined as <span class="math notranslate nohighlight">\(S(x_{i})\)</span>, then the overall loss is defined as</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = -\sum_{i=1}^{N} \log \left ( \sum_{x \in S(x_{i})} p(x) \right )\]</div>
</div>
<div class="section" id="sentencepiece">
<span id="id3"></span><h4>SentencePiece<a class="headerlink" href="#sentencepiece" title="Permalink to this headline">¶</a></h4>
<p>All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to
separate words. However, not all languages use spaces to separate words. One possible solution is to use language
specific pre-tokenizers, <em>e.g.</em> <a class="reference internal" href="model_doc/xlm.html"><span class="doc">XLM</span></a> uses a specific Chinese, Japanese, and Thai pre-tokenizer).
To solve this problem more generally, <a class="reference external" href="https://arxiv.org/pdf/1808.06226.pdf">SentencePiece: A simple and language independent subword tokenizer and
detokenizer for Neural Text Processing (Kudo et al., 2018)</a> treats the input
as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram
algorithm to construct the appropriate vocabulary.</p>
<p>The <a class="reference internal" href="model_doc/xlnet.html#transformers.XLNetTokenizer" title="transformers.XLNetTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetTokenizer</span></code></a> uses SentencePiece for example, which is also why in the example earlier the
<code class="docutils literal notranslate"><span class="pre">&quot;▁&quot;</span></code> character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be
concatenated and <code class="docutils literal notranslate"><span class="pre">&quot;▁&quot;</span></code> is replaced by a space.</p>
<p>All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models
using SentencePiece are <a class="reference internal" href="model_doc/albert.html"><span class="doc">ALBERT</span></a>, <a class="reference internal" href="model_doc/xlnet.html"><span class="doc">XLNet</span></a>, <a class="reference internal" href="model_doc/marian.html"><span class="doc">Marian</span></a>, and <a class="reference internal" href="model_doc/t5.html"><span class="doc">T5</span></a>.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="multilingual.html" class="btn btn-neutral float-right" title="Multi-lingual models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="model_sharing.html" class="btn btn-neutral float-left" title="Model sharing and uploading" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>