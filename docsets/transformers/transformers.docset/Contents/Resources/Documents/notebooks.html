

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>ðŸ¤— Transformers Notebooks &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Converting Tensorflow Checkpoints" href="converting_tensorflow_models.html" />
    <link rel="prev" title="Fine-tuning with custom datasets" href="custom_datasets.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ðŸ¤— Transformers Notebooks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hugging-face-s-notebooks">Hugging Faceâ€™s notebooks ðŸ¤—</a></li>
<li class="toctree-l2"><a class="reference internal" href="#community-notebooks">Community notebooks:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>ðŸ¤— Transformers Notebooks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/notebooks.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="transformers-notebooks">
<h1>ðŸ¤— Transformers Notebooks<a class="headerlink" href="#transformers-notebooks" title="Permalink to this headline">Â¶</a></h1>
<p>You can find here a list of the official notebooks provided by Hugging Face.</p>
<p>Also, we would like to list here interesting content created by the community.
If you wrote some notebook(s) leveraging ðŸ¤— Transformers and would like be listed here, please open a
Pull Request so it can be included under the Community notebooks.</p>
<div class="section" id="hugging-face-s-notebooks">
<h2>Hugging Faceâ€™s notebooks ðŸ¤—<a class="headerlink" href="#hugging-face-s-notebooks" title="Permalink to this headline">Â¶</a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Notebook</th>
<th align="left">Description</th>
<th align="right"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://github.com/huggingface/transformers/blob/master/notebooks/01-training-tokenizers.ipynb">Getting Started Tokenizers</a></td>
<td align="left">How to train and use your very own tokenizer</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/01-training-tokenizers.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb">Getting Started Transformers</a></td>
<td align="left">How to easily start using transformers</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/transformers/blob/master/notebooks/03-pipelines.ipynb">How to use Pipelines</a></td>
<td align="left">Simple and efficient way to use State-of-the-Art models on downstream tasks through transformers</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/03-pipelines.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb">How to fine-tune a model on text classification</a></td>
<td align="left">Show how to preprocess the data and fine-tune a pretrained model on any GLUE task.</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb">How to fine-tune a model on language modeling</a></td>
<td align="left">Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task.</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb">How to fine-tune a model on token classification</a></td>
<td align="left">Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS).</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb">How to fine-tune a model on question answering</a></td>
<td align="left">Show how to preprocess the data and fine-tune a pretrained model on SQUAD.</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">How to train a language model from scratch</a></td>
<td align="left">Highlight all the steps to effectively train Transformer model on custom data</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb">How to generate text</a></td>
<td align="left">How to use different decoding methods for language generation with transformers</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb">How to export model to ONNX</a></td>
<td align="left">Highlight how to export and run inference workloads through ONNX</td>
<td align="right"></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/transformers/blob/master/notebooks/05-benchmark.ipynb">How to use Benchmarks</a></td>
<td align="left">How to benchmark models with transformers</td>
<td align="right"><a href="https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/05-benchmark.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/huggingface/blog/blob/master/notebooks/03_reformer.ipynb">Reformer</a></td>
<td align="left">How Reformer pushes the limits of language modeling</td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/blog/blob/master/notebooks/03_reformer.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
</tbody>
</table></div>
<div class="section" id="community-notebooks">
<h2>Community notebooks:<a class="headerlink" href="#community-notebooks" title="Permalink to this headline">Â¶</a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Notebook</th>
<th align="left">Description</th>
<th align="left">Author</th>
<th align="right"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://github.com/snapthat/TF-T5-text-to-text">Train T5 in Tensorflow 2 </a></td>
<td align="left">How to train T5 for any task using Tensorflow 2. This notebook demonstrates a Question &amp; Answer task implemented in Tensorflow 2 using SQUAD</td>
<td align="left"><a href="https://github.com/HarrisDePerceptron">Muhammad Harris</a></td>
<td align="right"><a href="https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb">Train T5 on TPU</a></td>
<td align="left">How to train T5 on SQUAD with Transformers and Nlp</td>
<td align="left"><a href="https://github.com/patil-suraj">Suraj Patil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb">Fine-tune T5 for Classification and Multiple Choice</a></td>
<td align="left">How to fine-tune T5 for classification and multiple choice tasks using a text-to-text format with PyTorch Lightning</td>
<td align="left"><a href="https://github.com/patil-suraj">Suraj Patil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb">Fine-tune DialoGPT on New Datasets and Languages</a></td>
<td align="left">How to fine-tune the DialoGPT model on a new dataset for open-dialog conversational chatbots</td>
<td align="left"><a href="https://github.com/ncoop57">Nathan Cooper</a></td>
<td align="right"><a href="https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb">Long Sequence Modeling with Reformer</a></td>
<td align="left">How to train on sequences as long as 500,000 tokens with Reformer</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb">Fine-tune BART for Summarization</a></td>
<td align="left">How to fine-tune BART for summarization with fastai using blurr</td>
<td align="left"><a href="https://ohmeow.com/">Wayde Gilliam</a></td>
<td align="right"><a href="https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb">Fine-tune a pre-trained Transformer on anyone's tweets</a></td>
<td align="left">How to generate tweets in the style of your favorite Twitter account by fine-tune a GPT-2 model</td>
<td align="left"><a href="https://github.com/borisdayma">Boris Dayma</a></td>
<td align="right"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/drive/1NEiqNPhiouu2pPwDAVeFoN4-vTYMz9F8">A Step by Step Guide to Tracking Hugging Face Model Performance</a></td>
<td align="left">A quick tutorial for training NLP models with HuggingFace and &amp; visualizing their performance with Weights &amp; Biases</td>
<td align="left"><a href="https://github.com/jxmorris12">Jack Morris</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1NEiqNPhiouu2pPwDAVeFoN4-vTYMz9F8"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb">Pretrain Longformer</a></td>
<td align="left">How to build a "long" version of existing pretrained models</td>
<td align="left"><a href="https://beltagy.net">Iz Beltagy</a></td>
<td align="right"><a href="https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb">Fine-tune Longformer for QA</a></td>
<td align="left">How to fine-tune longformer model for QA task</td>
<td align="left"><a href="https://github.com/patil-suraj">Suraj Patil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb">Evaluate Model with ðŸ¤—nlp</a></td>
<td align="left">How to evaluate longformer on TriviaQA with <code>nlp</code></td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb">Fine-tune T5 for Sentiment Span Extraction</a></td>
<td align="left">How to fine-tune T5 for sentiment span extraction using a text-to-text format with PyTorch Lightning</td>
<td align="left"><a href="https://github.com/enzoampil">Lorenzo Ampil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb">Fine-tune DistilBert for Multiclass Classification</a></td>
<td align="left">How to fine-tune DistilBert for multiclass classification with PyTorch</td>
<td align="left"><a href="https://github.com/abhimishra91">Abhishek Kumar Mishra</a></td>
<td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb">Fine-tune BERT for Multi-label Classification</a></td>
<td align="left">How to fine-tune BERT for multi-label classification using PyTorch</td>
<td align="left"><a href="https://github.com/abhimishra91">Abhishek Kumar Mishra</a></td>
<td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb">Fine-tune T5 for Summarization</a></td>
<td align="left">How to fine-tune T5 for summarization in PyTorch and track experiments with WandB</td>
<td align="left"><a href="https://github.com/abhimishra91">Abhishek Kumar Mishra</a></td>
<td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb">Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing</a></td>
<td align="left">How to speed up fine-tuning by a factor of 2 using dynamic padding / bucketing</td>
<td align="left"><a href="https://github.com/pommedeterresautee">Michael Benesty</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb">Pretrain Reformer for Masked Language Modeling</a></td>
<td align="left">How to train a Reformer model with bi-directional self-attention layers</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb">Expand and Fine Tune Sci-BERT</a></td>
<td align="left">How to increase vocabulary of a pretrained SciBERT model from AllenAI on the CORD dataset and pipeline it.</td>
<td align="left"><a href="https://github.com/lordtt13">Tanmay Thakur</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb">Fine-tune Electra and interpret with Integrated Gradients</a></td>
<td align="left">How to fine-tune Electra for sentiment analysis and interpret predictions with Captum Integrated Gradients</td>
<td align="left"><a href="https://elsanns.github.io">Eliza Szczechla</a></td>
<td align="right"><a href="https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb">fine-tune a non-English GPT-2 Model with Trainer class</a></td>
<td align="left">How to fine-tune a non-English GPT-2 Model with Trainer class</td>
<td align="left"><a href="https://www.philschmid.de">Philipp Schmid</a></td>
<td align="right"><a href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb">Fine-tune a DistilBERT Model for Multi Label Classification task</a></td>
<td align="left">How to fine-tune a DistilBERT Model for Multi Label Classification task</td>
<td align="left"><a href="https://github.com/DhavalTaunk08">Dhaval Taunk</a></td>
<td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb">Fine-tune ALBERT for sentence-pair classification</a></td>
<td align="left">How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair classification task</td>
<td align="left"><a href="https://github.com/NadirEM">Nadir El Manouzi</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb">Fine-tune Roberta for sentiment analysis</a></td>
<td align="left">How to fine-tune an Roberta model for sentiment analysis</td>
<td align="left"><a href="https://github.com/DhavalTaunk08">Dhaval Taunk</a></td>
<td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/flexudy-pipe/qugeev">Evaluating Question Generation Models</a></td>
<td align="left">How accurate are the answers to questions generated by your seq2seq transformer model?</td>
<td align="left"><a href="https://github.com/zolekode">Pascal Zoleko</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb">Classify text with DistilBERT and Tensorflow</a></td>
<td align="left">How to fine-tune DistilBERT for text classification in TensorFlow</td>
<td align="left"><a href="https://github.com/peterbayerle">Peter Bayerle</a></td>
<td align="right"><a href="https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb">Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail</a></td>
<td align="left">How to warm-start a <em>EncoderDecoderModel</em> with a <em>bert-base-uncased</em> checkpoint for summarization on CNN/Dailymail</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb">Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum</a></td>
<td align="left">How to warm-start a shared <em>EncoderDecoderModel</em> with a <em>roberta-base</em> checkpoint for summarization on BBC/XSum</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb">Fine-tuning TAPAS on Sequential Question Answering (SQA)</a></td>
<td align="left">How to fine-tune <em>TapasForQuestionAnswering</em> with a <em>tapas-base</em> checkpoint on the Sequential Question Answering (SQA) dataset</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb">Evaluating TAPAS on Table Fact Checking (TabFact)</a></td>
<td align="left">How to evaluate a fine-tuned <em>TapasForSequenceClassification</em> with a <em>tapas-base-finetuned-tabfact</em> checkpoint using a combination of the ðŸ¤— datasets and ðŸ¤— transformers libraries</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb">Fine-tuning mBART for translation</a></td>
<td align="left">How to fine-tune mBART using Seq2SeqTrainer for Hindi to English translation</td>
<td align="left"><a href="https://github.com/vasudevgupta7">Vasudev Gupta</a></td>
<td align="right"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb">Fine-tuning LayoutLM on FUNSD (a form understanding dataset)</a></td>
<td align="left">How to fine-tune <em>LayoutLMForTokenClassification</em> on the FUNSD dataset</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb">Fine-Tune DistilGPT2 and Generate Text</a></td>
<td align="left">How to fine-tune DistilGPT2 and generate text</td>
<td align="left"><a href="https://github.com/tripathiaakash">Aakash Tripathi</a></td>
<td align="right"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb">Fine-Tune LED on up to 8K tokens</a></td>
<td align="left">How to fine-tune LED on pubmed for long-range summarization</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb">Evaluate LED on Arxiv</a></td>
<td align="left">How to effectively evaluate LED on long-range summarization</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
</tbody>
</table></div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="converting_tensorflow_models.html" class="btn btn-neutral float-right" title="Converting Tensorflow Checkpoints" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="custom_datasets.html" class="btn btn-neutral float-left" title="Fine-tuning with custom datasets" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>