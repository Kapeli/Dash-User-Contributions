

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>transformers.modeling_roberta &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.modeling_roberta</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.modeling_roberta</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;PyTorch RoBERTa model. &quot;&quot;&quot;</span>


<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">MSELoss</span>

<span class="kn">from</span> <span class="nn">.configuration_roberta</span> <span class="kn">import</span> <span class="n">RobertaConfig</span>
<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="n">add_start_docstrings</span><span class="p">,</span> <span class="n">add_start_docstrings_to_callable</span>
<span class="kn">from</span> <span class="nn">.modeling_bert</span> <span class="kn">import</span> <span class="n">BertEmbeddings</span><span class="p">,</span> <span class="n">BertLayerNorm</span><span class="p">,</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertPreTrainedModel</span><span class="p">,</span> <span class="n">gelu</span>
<span class="kn">from</span> <span class="nn">.modeling_utils</span> <span class="kn">import</span> <span class="n">create_position_ids_from_input_ids</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;roberta-base&quot;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roberta-large&quot;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roberta-large-mnli&quot;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-pytorch_model.bin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;distilroberta-base&quot;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-pytorch_model.bin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roberta-base-openai-detector&quot;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-openai-detector-pytorch_model.bin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roberta-large-openai-detector&quot;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-openai-detector-pytorch_model.bin&quot;</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">class</span> <span class="nc">RobertaEmbeddings</span><span class="p">(</span><span class="n">BertEmbeddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Create the position ids from the input token ids. Any padded tokens remain padded.</span>
                <span class="n">position_ids</span> <span class="o">=</span> <span class="n">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">position_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_position_ids_from_inputs_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; We are provided embeddings directly. We cannot infer which are padded so just generate</span>
<span class="sd">        sequential position ids.</span>

<span class="sd">        :param torch.Tensor inputs_embeds:</span>
<span class="sd">        :return torch.Tensor:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>


<span class="n">ROBERTA_START_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    This model is a PyTorch `torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;`_ sub-class.</span>
<span class="s2">    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general</span>
<span class="s2">    usage and behavior.</span>

<span class="s2">    Parameters:</span>
<span class="s2">        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the</span>
<span class="s2">            model. Initializing with a config file does not load the weights associated with the model, only the configuration.</span>
<span class="s2">            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">ROBERTA_INPUTS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Args:</span>
<span class="s2">        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):</span>
<span class="s2">            Indices of input sequence tokens in the vocabulary.</span>

<span class="s2">            Indices can be obtained using :class:`transformers.RobertaTokenizer`.</span>
<span class="s2">            See :func:`transformers.PreTrainedTokenizer.encode` and</span>
<span class="s2">            :func:`transformers.PreTrainedTokenizer.encode_plus` for details.</span>

<span class="s2">            `What are input IDs? &lt;../glossary.html#input-ids&gt;`__</span>
<span class="s2">        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):</span>
<span class="s2">            Mask to avoid performing attention on padding token indices.</span>
<span class="s2">            Mask values selected in ``[0, 1]``:</span>
<span class="s2">            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.</span>

<span class="s2">            `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="s2">        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):</span>
<span class="s2">            Segment token indices to indicate first and second portions of the inputs.</span>
<span class="s2">            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``</span>
<span class="s2">            corresponds to a `sentence B` token</span>

<span class="s2">            `What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`_</span>
<span class="s2">        position_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):</span>
<span class="s2">            Indices of positions of each input sequence tokens in the position embeddings.</span>
<span class="s2">            Selected in the range ``[0, config.max_position_embeddings - 1]``.</span>

<span class="s2">            `What are position IDs? &lt;../glossary.html#position-ids&gt;`_</span>
<span class="s2">        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`, defaults to :obj:`None`):</span>
<span class="s2">            Mask to nullify selected heads of the self-attention modules.</span>
<span class="s2">            Mask values selected in ``[0, 1]``:</span>
<span class="s2">            :obj:`1` indicates the head is **not masked**, :obj:`0` indicates the head is **masked**.</span>
<span class="s2">        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`, defaults to :obj:`None`):</span>
<span class="s2">            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.</span>
<span class="s2">            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="s2">            than the model&#39;s internal embedding lookup matrix.</span>
<span class="s2">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="RobertaModel"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaModel">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="s2">&quot;The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">RobertaModel</span><span class="p">(</span><span class="n">BertModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class overrides :class:`~transformers.BertModel`. Please check the</span>
<span class="sd">    superclass for the appropriate documentation alongside usage examples.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">RobertaEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

<div class="viewcode-block" id="RobertaModel.get_input_embeddings"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaModel.get_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span></div>

<div class="viewcode-block" id="RobertaModel.set_input_embeddings"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaModel.set_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">value</span></div></div>


<div class="viewcode-block" id="RobertaForMaskedLM"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForMaskedLM">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;RoBERTa Model with a `language modeling` head on top. &quot;&quot;&quot;</span><span class="p">,</span> <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">RobertaForMaskedLM</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">RobertaLMHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

<div class="viewcode-block" id="RobertaForMaskedLM.get_output_embeddings"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForMaskedLM.get_output_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span></div>

<div class="viewcode-block" id="RobertaForMaskedLM.forward"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForMaskedLM.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">masked_lm_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        masked_lm_labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">            Labels for computing the masked language modeling loss.</span>
<span class="sd">            Indices should be in ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)</span>
<span class="sd">            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens with labels</span>
<span class="sd">            in ``[0, ..., config.vocab_size]``</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:</span>
<span class="sd">        masked_lm_loss (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:</span>
<span class="sd">            Masked language modeling loss.</span>
<span class="sd">        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`)</span>
<span class="sd">            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape</span>
<span class="sd">            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        from transformers import RobertaTokenizer, RobertaForMaskedLM</span>
<span class="sd">        import torch</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = RobertaForMaskedLM.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids, masked_lm_labels=input_ids)</span>
<span class="sd">        loss, prediction_scores = outputs[:2]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prediction_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction_scores</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># Add hidden states and attention if they are here</span>

        <span class="k">if</span> <span class="n">masked_lm_labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">prediction_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">masked_lm_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># (masked_lm_loss), prediction_scores, (hidden_states), (attentions)</span></div></div>


<span class="k">class</span> <span class="nc">RobertaLMHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Roberta Head for masked language modeling.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">BertLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">))</span>

        <span class="c1"># Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># project back to size of vocabulary with bias</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="RobertaForSequenceClassification"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForSequenceClassification">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer</span>
<span class="sd">    on top of the pooled output) e.g. for GLUE tasks. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">RobertaForSequenceClassification</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">RobertaClassificationHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<div class="viewcode-block" id="RobertaForSequenceClassification.forward"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForSequenceClassification.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">            Labels for computing the sequence classification/regression loss.</span>
<span class="sd">            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.</span>
<span class="sd">            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),</span>
<span class="sd">            If :obj:`config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:</span>
<span class="sd">        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):</span>
<span class="sd">            Classification (or regression if config.num_labels==1) loss.</span>
<span class="sd">        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):</span>
<span class="sd">            Classification (or regression if config.num_labels==1) scores (before SoftMax).</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape</span>
<span class="sd">            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        from transformers import RobertaTokenizer, RobertaForSequenceClassification</span>
<span class="sd">        import torch</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = RobertaForSequenceClassification.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1</span>
<span class="sd">        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids, labels=labels)</span>
<span class="sd">        loss, logits = outputs[:2]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1">#  We are doing regression</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># (loss), logits, (hidden_states), (attentions)</span></div></div>


<span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;Roberta Model with a multiple choice classification head on top (a linear layer on top of</span>
<span class="sd">    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">RobertaForMultipleChoice</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">            Labels for computing the multiple choice classification loss.</span>
<span class="sd">            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension</span>
<span class="sd">            of the input tensors. (see `input_ids` above)</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:</span>
<span class="sd">        loss (:obj:`torch.FloatTensor`` of shape ``(1,)`, `optional`, returned when :obj:`labels` is provided):</span>
<span class="sd">            Classification loss.</span>
<span class="sd">        classification_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_choices)`):</span>
<span class="sd">            `num_choices` is the second dimension of the input tensors. (see `input_ids` above).</span>

<span class="sd">            Classification scores (before SoftMax).</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape</span>
<span class="sd">            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        from transformers import RobertaTokenizer, RobertaForMultipleChoice</span>
<span class="sd">        import torch</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = RobertaForMultipleChoice.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        choices = [&quot;Hello, my dog is cute&quot;, &quot;Hello, my cat is amazing&quot;]</span>
<span class="sd">        input_ids = torch.tensor([tokenizer.encode(s, add_special_tokens=True) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices</span>
<span class="sd">        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids, labels=labels)</span>
<span class="sd">        loss, classification_scores = outputs[:2]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_choices</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">flat_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">flat_position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">flat_token_type_ids</span> <span class="o">=</span> <span class="n">token_type_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="n">token_type_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">flat_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span>
            <span class="n">flat_input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">flat_position_ids</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">flat_token_type_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">flat_attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">pooled_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>
        <span class="n">reshaped_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_choices</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">reshaped_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># add hidden states and attention if they are here</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">reshaped_logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># (loss), reshaped_logits, (hidden_states), (attentions)</span>


<div class="viewcode-block" id="RobertaForTokenClassification"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForTokenClassification">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;Roberta Model with a token classification head on top (a linear layer on top of</span>
<span class="sd">    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">RobertaForTokenClassification</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

<div class="viewcode-block" id="RobertaForTokenClassification.forward"><a class="viewcode-back" href="../../model_doc/roberta.html#transformers.RobertaForTokenClassification.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">            Labels for computing the token classification loss.</span>
<span class="sd">            Indices should be in ``[0, ..., config.num_labels - 1]``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:</span>
<span class="sd">        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when ``labels`` is provided) :</span>
<span class="sd">            Classification loss.</span>
<span class="sd">        scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.num_labels)`)</span>
<span class="sd">            Classification scores (before SoftMax).</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape</span>
<span class="sd">            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        from transformers import RobertaTokenizer, RobertaForTokenClassification</span>
<span class="sd">        import torch</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = RobertaForTokenClassification.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = torch.tensor(tokenizer.encode(&quot;Hello, my dog is cute&quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1</span>
<span class="sd">        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids, labels=labels)</span>
<span class="sd">        loss, scores = outputs[:2]</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># add hidden states and attention if they are here</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="c1"># Only keep active parts of the loss</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">active_loss</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="n">active_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>
                <span class="n">active_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                    <span class="n">active_loss</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">loss_fct</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">active_logits</span><span class="p">,</span> <span class="n">active_labels</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># (loss), scores, (hidden_states), (attentions)</span></div></div>


<span class="k">class</span> <span class="nc">RobertaClassificationHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Head for sentence-level classification tasks.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># take &lt;s&gt; token (equiv. to [CLS])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of</span>
<span class="sd">    the hidden-states output to compute `span start logits` and `span end logits`). &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">RobertaForQuestionAnswering</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">start_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">end_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">            Labels for position (index) of the start of the labelled span for computing the token classification loss.</span>
<span class="sd">            Positions are clamped to the length of the sequence (`sequence_length`).</span>
<span class="sd">            Position outside of the sequence are not taken into account for computing the loss.</span>
<span class="sd">        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">            Labels for position (index) of the end of the labelled span for computing the token classification loss.</span>
<span class="sd">            Positions are clamped to the length of the sequence (`sequence_length`).</span>
<span class="sd">            Position outside of the sequence are not taken into account for computing the loss.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:</span>
<span class="sd">        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):</span>
<span class="sd">            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</span>
<span class="sd">        start_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length,)`):</span>
<span class="sd">            Span-start scores (before SoftMax).</span>
<span class="sd">        end_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length,)`):</span>
<span class="sd">            Span-end scores (before SoftMax).</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape</span>
<span class="sd">            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        # The checkpoint roberta-large is not fine-tuned for question answering. Please see the</span>
<span class="sd">        # examples/run_squad.py example to see how to fine-tune a model to a question answering task.</span>

<span class="sd">        from transformers import RobertaTokenizer, RobertaForQuestionAnswering</span>
<span class="sd">        import torch</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = RobertaForQuestionAnswering.from_pretrained(&#39;roberta-base&#39;)</span>

<span class="sd">        question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;</span>
<span class="sd">        input_ids = tokenizer.encode(question, text)</span>
<span class="sd">        start_scores, end_scores = model(torch.tensor([input_ids]))</span>

<span class="sd">        all_tokens = tokenizer.convert_ids_to_tokens(input_ids)</span>
<span class="sd">        answer = &#39; &#39;.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
        <span class="n">start_logits</span><span class="p">,</span> <span class="n">end_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">start_logits</span> <span class="o">=</span> <span class="n">start_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">end_logits</span> <span class="o">=</span> <span class="n">end_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">end_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">end_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we are on multi-GPU, split add a dimension</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_positions</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">end_positions</span> <span class="o">=</span> <span class="n">end_positions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sometimes the start/end positions are outside our model inputs, we ignore these terms</span>
            <span class="n">ignored_index</span> <span class="o">=</span> <span class="n">start_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">start_positions</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignored_index</span><span class="p">)</span>
            <span class="n">end_positions</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignored_index</span><span class="p">)</span>

            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">ignored_index</span><span class="p">)</span>
            <span class="n">start_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>
            <span class="n">end_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_loss</span> <span class="o">+</span> <span class="n">end_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># (loss), start_logits, end_logits, (hidden_states), (attentions)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>