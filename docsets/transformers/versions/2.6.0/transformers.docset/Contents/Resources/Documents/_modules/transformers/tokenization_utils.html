

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>transformers.tokenization_utils &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.tokenization_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.tokenization_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;Tokenization classes for OpenAI GPT.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">tokenizers.implementations</span> <span class="kn">import</span> <span class="n">BaseTokenizer</span>

<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="n">cached_path</span><span class="p">,</span> <span class="n">hf_bucket_url</span><span class="p">,</span> <span class="n">is_remote_url</span><span class="p">,</span> <span class="n">is_tf_available</span><span class="p">,</span> <span class="n">is_torch_available</span>


<span class="k">if</span> <span class="n">is_tf_available</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="k">if</span> <span class="n">is_torch_available</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">torch</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">SPECIAL_TOKENS_MAP_FILE</span> <span class="o">=</span> <span class="s2">&quot;special_tokens_map.json&quot;</span>
<span class="n">ADDED_TOKENS_FILE</span> <span class="o">=</span> <span class="s2">&quot;added_tokens.json&quot;</span>
<span class="n">TOKENIZER_CONFIG_FILE</span> <span class="o">=</span> <span class="s2">&quot;tokenizer_config.json&quot;</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">truncate_and_pad</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">pad_token_type_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This contextmanager is in charge of defining the truncation and the padding strategies and then</span>
<span class="sd">    restore the tokenizer settings afterwards.</span>

<span class="sd">    This contextmanager assumes the provider tokenizer has no padding / truncation strategy</span>
<span class="sd">    before the managed section. If your tokenizer set a padding / truncation strategy before,</span>
<span class="sd">    then it will be reset to no padding/truncation when exiting the managed section.</span>

<span class="sd">    :param tokenizer:</span>
<span class="sd">    :param max_length:</span>
<span class="sd">    :param stride:</span>
<span class="sd">    :param strategy:</span>
<span class="sd">    :param pad_to_max_length:</span>
<span class="sd">    :param padding_side:</span>
<span class="sd">    :param pad_token_id:</span>
<span class="sd">    :param pad_token_type_id:</span>
<span class="sd">    :param pad_token:</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Handle all the truncation and padding stuff</span>
    <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">enable_truncation</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token</span> <span class="ow">and</span> <span class="n">pad_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">enable_padding</span><span class="p">(</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">direction</span><span class="o">=</span><span class="n">padding_side</span><span class="p">,</span>
            <span class="n">pad_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">pad_type_id</span><span class="o">=</span><span class="n">pad_token_type_id</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">pad_to_max_length</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Disabled padding because no padding token set (pad_token: </span><span class="si">{}</span><span class="s2">, pad_token_id: </span><span class="si">{}</span><span class="s2">).</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;To remove this error, you can add a new pad token and then resize model embedding:</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">tokenizer.pad_token = &#39;&lt;PAD&gt;&#39;</span><span class="se">\n\t</span><span class="s2">model.resize_token_embeddings(len(tokenizer))&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">pad_token</span><span class="p">,</span> <span class="n">pad_token_id</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">yield</span>

    <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">no_truncation</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token</span> <span class="ow">and</span> <span class="n">pad_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">no_padding</span><span class="p">()</span>


<div class="viewcode-block" id="PreTrainedTokenizer"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer">[docs]</a><span class="k">class</span> <span class="nc">PreTrainedTokenizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Base class for all tokenizers.</span>
<span class="sd">    Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.</span>

<span class="sd">    This class also contain the added tokens in a unified way on top of all tokenizers so we don&#39;t have to handle the specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).</span>

<span class="sd">    Class attributes (overridden by derived classes):</span>

<span class="sd">        - ``vocab_files_names``: a python ``dict`` with, as keys, the ``__init__`` keyword name of each vocabulary file required by the model, and as associated values, the filename for saving the associated file (string).</span>
<span class="sd">        - ``pretrained_vocab_files_map``: a python ``dict of dict`` the high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the low-level being the `short-cut-names` (string) of the pretrained models with, as associated values, the `url` (string) to the associated pretrained vocabulary file.</span>
<span class="sd">        - ``max_model_input_sizes``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model, or None if the model has no maximum input size.</span>
<span class="sd">        - ``pretrained_init_configuration``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, a dictionnary of specific arguments to pass to the ``__init__``method of the tokenizer class for this pretrained model when loading the tokenizer with the ``from_pretrained()`` method.</span>

<span class="sd">    Parameters:</span>

<span class="sd">        - ``bos_token``: (`Optional`) string: a beginning of sentence token. Will be associated to ``self.bos_token`` and ``self.bos_token_id``</span>

<span class="sd">        - ``eos_token``: (`Optional`) string: an end of sentence token. Will be associated to ``self.eos_token`` and ``self.eos_token_id``</span>

<span class="sd">        - ``unk_token``: (`Optional`) string: an unknown token. Will be associated to ``self.unk_token`` and ``self.unk_token_id``</span>

<span class="sd">        - ``sep_token``: (`Optional`) string: a separation token (e.g. to separate context and query in an input sequence). Will be associated to ``self.sep_token`` and ``self.sep_token_id``</span>

<span class="sd">        - ``pad_token``: (`Optional`) string: a padding token. Will be associated to ``self.pad_token`` and ``self.pad_token_id``</span>

<span class="sd">        - ``cls_token``: (`Optional`) string: a classification token (e.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model). Will be associated to ``self.cls_token`` and ``self.cls_token_id``</span>

<span class="sd">        - ``mask_token``: (`Optional`) string: a masking token (e.g. when training a model with masked-language modeling). Will be associated to ``self.mask_token`` and ``self.mask_token_id``</span>

<span class="sd">        - ``additional_special_tokens``: (`Optional`) list: a list of additional special tokens. Adding all special tokens here ensure they won&#39;t be split by the tokenization process. Will be associated to ``self.additional_special_tokens`` and ``self.additional_special_tokens_ids``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vocab_files_names</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pretrained_vocab_files_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pretrained_init_configuration</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">max_model_input_sizes</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">model_input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>

    <span class="n">SPECIAL_TOKENS_ATTRIBUTES</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;bos_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eos_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;unk_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sep_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;pad_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;cls_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mask_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>

    <span class="n">NO_PAD_TOKEN_FOR_BATCH_MSG</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;No padding token is set for this model, therefore no batch can be made with uneven &quot;</span>
        <span class="s2">&quot;sequences. Set a padding token or adjust the lengths of the sequences building the &quot;</span>
        <span class="s2">&quot;batch so that every sequence is of the same length.&quot;</span>
    <span class="p">)</span>

    <span class="n">UNEVEN_SEQUENCES_FOR_BATCH_MSG</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;The sequences building the batch are not of the same size, no tensor &quot;</span>
        <span class="s2">&quot;can be built. Set `pad_to_max_length=True` to pad the smaller sequences&quot;</span>
        <span class="s2">&quot;up to the larger sequence&#39;s length.&quot;</span>
    <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Beginning of sentence token (string). Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using bos_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; End of sentence token (string). Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using eos_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Unknown token (string). Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using unk_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using sep_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Padding token (string). Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using pad_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using cls_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using mask_token, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using additional_special_tokens, but it is not set yet.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span>

    <span class="nd">@bos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@eos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@unk_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@sep_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@pad_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@cls_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@mask_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@additional_special_tokens</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the unknown token in the vocabulary. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the padding token in the vocabulary. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the padding token type in the vocabulary.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token_type_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mask_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">additional_special_tokens</span><span class="p">)</span>

<div class="viewcode-block" id="PreTrainedTokenizer.get_vocab"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.get_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Returns the vocabulary as a dict of {token: index} pairs. `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the vocab. &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token_type_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">=</span> <span class="n">max_len</span> <span class="k">if</span> <span class="n">max_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e12</span><span class="p">)</span>

        <span class="c1"># Padding side is right by default and over-riden in subclasses. If specified in the kwargs, it is changed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;padding_side&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model_input_names&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">)</span>

        <span class="c1"># Added tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span> <span class="o">=</span> <span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

<div class="viewcode-block" id="PreTrainedTokenizer.from_pretrained"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            pretrained_model_name_or_path: either:</span>

<span class="sd">                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.</span>
<span class="sd">                - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.</span>
<span class="sd">                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.</span>
<span class="sd">                - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.</span>

<span class="sd">            cache_dir: (`optional`) string:</span>
<span class="sd">                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.</span>

<span class="sd">            force_download: (`optional`) boolean, default False:</span>
<span class="sd">                Force to (re-)download the vocabulary files and override the cached versions if they exists.</span>

<span class="sd">            resume_download: (`optional`) boolean, default False:</span>
<span class="sd">                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</span>

<span class="sd">            proxies: (`optional`) dict, default None:</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {&#39;http&#39;: &#39;foo.bar:3128&#39;, &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}.</span>
<span class="sd">                The proxies are used on each request.</span>

<span class="sd">            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.</span>

<span class="sd">            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # We can&#39;t instantiate directly the base class `PreTrainedTokenizer` so let&#39;s show our examples on a derived class: BertTokenizer</span>

<span class="sd">            # Download vocabulary from S3 and cache.</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)</span>

<span class="sd">            # Download vocabulary from S3 (user-uploaded) and cache.</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;dbmdz/bert-base-german-cased&#39;)</span>

<span class="sd">            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`)</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;./test/saved_model/&#39;)</span>

<span class="sd">            # If the tokenizer uses a single vocabulary file, you can point directly to this file</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;./test/saved_model/my_vocab.txt&#39;)</span>

<span class="sd">            # You can link tokens to special vocabulary when instantiating</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;, unk_token=&#39;&lt;unk&gt;&#39;)</span>
<span class="sd">            # You should be sure &#39;&lt;unk&gt;&#39; is in the vocabulary when doing that.</span>
<span class="sd">            # Otherwise use tokenizer.add_special_tokens({&#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;}) instead)</span>
<span class="sd">            assert tokenizer.unk_token == &#39;&lt;unk&gt;&#39;</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">s3_models</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">vocab_files</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">init_configuration</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="n">s3_models</span><span class="p">:</span>
            <span class="c1"># Get the vocabulary from AWS S3 bucket</span>
            <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">map_list</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_vocab_files_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">map_list</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_init_configuration</span>
                <span class="ow">and</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_init_configuration</span>
            <span class="p">):</span>
                <span class="n">init_configuration</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_init_configuration</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Get the vocabulary from local files</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Model name &#39;</span><span class="si">{}</span><span class="s2">&#39; not found in model shortcut name list (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                <span class="s2">&quot;Assuming &#39;</span><span class="si">{}</span><span class="s2">&#39; is a path, a model identifier, or url to a directory containing tokenizer files.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s3_models</span><span class="p">),</span> <span class="n">pretrained_model_name_or_path</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Calling </span><span class="si">{}</span><span class="s2">.from_pretrained() with the path to a single file or url is not supported.&quot;</span>
                        <span class="s2">&quot;Use a model identifier or the path to a directory instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Calling </span><span class="si">{}</span><span class="s2">.from_pretrained() with the path to a single file or url is deprecated&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">file_id</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># At this point pretrained_model_name_or_path is either a directory or a model identifier name</span>
                <span class="n">additional_files_names</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;added_tokens_file&quot;</span><span class="p">:</span> <span class="n">ADDED_TOKENS_FILE</span><span class="p">,</span>
                    <span class="s2">&quot;special_tokens_map_file&quot;</span><span class="p">:</span> <span class="n">SPECIAL_TOKENS_MAP_FILE</span><span class="p">,</span>
                    <span class="s2">&quot;tokenizer_config_file&quot;</span><span class="p">:</span> <span class="n">TOKENIZER_CONFIG_FILE</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="c1"># Look for the tokenizer main vocabulary files + the additional tokens files</span>
                <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="p">{</span><span class="o">**</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">,</span> <span class="o">**</span><span class="n">additional_files_names</span><span class="p">}</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                        <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">full_file_name</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Didn&#39;t find file </span><span class="si">{}</span><span class="s2">. We won&#39;t load it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">full_file_name</span><span class="p">))</span>
                            <span class="n">full_file_name</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">hf_bucket_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">postfix</span><span class="o">=</span><span class="n">file_name</span><span class="p">)</span>

                    <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_file_name</span>

        <span class="c1"># Get files from url, cache, or disk depending on the case</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">resolved_vocab_files</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">file_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">cached_path</span><span class="p">(</span>
                        <span class="n">file_path</span><span class="p">,</span>
                        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                        <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="p">)</span>
        <span class="k">except</span> <span class="ne">EnvironmentError</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="n">s3_models</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Couldn&#39;t reach server at &#39;</span><span class="si">{}</span><span class="s2">&#39; to download vocabulary files.&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">&quot;Model name &#39;</span><span class="si">{}</span><span class="s2">&#39; was not found in tokenizers model name list (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                    <span class="s2">&quot;We assumed &#39;</span><span class="si">{}</span><span class="s2">&#39; was a path or url to a directory containing vocabulary files &quot;</span>
                    <span class="s2">&quot;named </span><span class="si">{}</span><span class="s2">, but couldn&#39;t find such vocabulary files at this path or url.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s3_models</span><span class="p">),</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">full_file_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">full_file_name</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                <span class="s2">&quot;Model name &#39;</span><span class="si">{}</span><span class="s2">&#39; was not found in tokenizers model name list (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                <span class="s2">&quot;We assumed &#39;</span><span class="si">{}</span><span class="s2">&#39; was a path, a model identifier, or url to a directory containing vocabulary files &quot;</span>
                <span class="s2">&quot;named </span><span class="si">{}</span><span class="s2"> but couldn&#39;t find such vocabulary files at this path or url.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s3_models</span><span class="p">),</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">file_path</span> <span class="o">==</span> <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading file </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading file </span><span class="si">{}</span><span class="s2"> from cache at </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]))</span>

        <span class="c1"># Prepare tokenizer initialization kwargs</span>
        <span class="c1"># Did we saved some inputs and kwargs to reload ?</span>
        <span class="n">tokenizer_config_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tokenizer_config_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tokenizer_config_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tokenizer_config_handle</span><span class="p">:</span>
                <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">tokenizer_config_handle</span><span class="p">)</span>
            <span class="n">saved_init_inputs</span> <span class="o">=</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_inputs&quot;</span><span class="p">,</span> <span class="p">())</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">init_inputs</span><span class="p">:</span>
                <span class="n">init_inputs</span> <span class="o">=</span> <span class="n">saved_init_inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">init_configuration</span>

        <span class="c1"># Update with newly provided kwargs</span>
        <span class="n">init_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Set max length if needed</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="p">:</span>
            <span class="c1"># if we&#39;re using a pretrained model, ensure the tokenizer</span>
            <span class="c1"># wont index sequences longer than the number of positional embeddings</span>
            <span class="n">max_len</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">max_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;max_len&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_len&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e12</span><span class="p">)),</span> <span class="n">max_len</span><span class="p">)</span>

        <span class="c1"># Merge resolved_vocab_files arguments in init_kwargs.</span>
        <span class="n">added_tokens_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;added_tokens_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">special_tokens_map_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;special_tokens_map_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">args_name</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">args_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">init_kwargs</span><span class="p">:</span>
                <span class="n">init_kwargs</span><span class="p">[</span><span class="n">args_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_path</span>
        <span class="k">if</span> <span class="n">special_tokens_map_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">special_tokens_map_handle</span><span class="p">:</span>
                <span class="n">special_tokens_map</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">special_tokens_map_handle</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">special_tokens_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">init_kwargs</span><span class="p">:</span>
                    <span class="n">init_kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># Instantiate tokenizer.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to load vocabulary from file. &quot;</span>
                <span class="s2">&quot;Please check that the provided vocabulary is accessible and not corrupted.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Save inputs and kwargs for saving and re-loading with ``save_pretrained``</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">init_inputs</span> <span class="o">=</span> <span class="n">init_inputs</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">init_kwargs</span>

        <span class="c1"># update unique_added_tokens_encoder with special tokens for correct tokenization</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">))</span>

        <span class="c1"># Add supplementary tokens.</span>
        <span class="k">if</span> <span class="n">added_tokens_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">added_tokens_handle</span><span class="p">:</span>
                <span class="n">added_tok_encoder</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">added_tokens_handle</span><span class="p">)</span>
            <span class="n">added_tok_decoder</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">added_tok_encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">added_tok_encoder</span><span class="p">)</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">added_tok_decoder</span><span class="p">)</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>

        <span class="k">return</span> <span class="n">tokenizer</span>

<div class="viewcode-block" id="PreTrainedTokenizer.save_pretrained"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.save_pretrained">[docs]</a>    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Save the tokenizer vocabulary files together with:</span>
<span class="sd">                - added tokens,</span>
<span class="sd">                - special-tokens-to-class-attributes-mapping,</span>
<span class="sd">                - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).</span>

<span class="sd">            This won&#39;t save modifications other than (added tokens and special token mapping) you may have</span>
<span class="sd">            applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).</span>

<span class="sd">            This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Saving directory (</span><span class="si">{}</span><span class="s2">) should be a directory&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">save_directory</span><span class="p">))</span>
            <span class="k">return</span>

        <span class="n">special_tokens_map_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">SPECIAL_TOKENS_MAP_FILE</span><span class="p">)</span>
        <span class="n">added_tokens_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">ADDED_TOKENS_FILE</span><span class="p">)</span>
        <span class="n">tokenizer_config_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">TOKENIZER_CONFIG_FILE</span><span class="p">)</span>

        <span class="n">tokenizer_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;init_inputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">file_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">tokenizer_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">file_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tokenizer_config</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">out_str</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out_str</span><span class="p">)</span>

        <span class="n">vocab_files</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">vocab_files</span> <span class="o">+</span> <span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="n">added_tokens_file</span><span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.save_vocabulary"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.save_vocabulary">[docs]</a>    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens</span>
<span class="sd">            and special token mappings.</span>

<span class="sd">            Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.vocab_size"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.vocab_size">[docs]</a>    <span class="k">def</span> <span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Size of the base vocabulary (without the added tokens) &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Size of the full vocabulary with the added tokens &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>

<div class="viewcode-block" id="PreTrainedTokenizer.add_tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.add_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">add_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_tokens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a list of new tokens to the tokenizer class. If the new tokens are not in the</span>
<span class="sd">        vocabulary, they are added to it with indices starting from length of the current vocabulary.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_tokens: string or list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of tokens added to the vocabulary.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # Let&#39;s see how to increase the vocabulary of Bert model and tokenizer</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)</span>

<span class="sd">            num_added_toks = tokenizer.add_tokens([&#39;new_tok1&#39;, &#39;my_new-tok2&#39;])</span>
<span class="sd">            print(&#39;We have added&#39;, num_added_toks, &#39;tokens&#39;)</span>
<span class="sd">            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">new_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_tokens</span><span class="p">]</span>

        <span class="n">to_add_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">new_tokens</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_lower_case&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">token</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">to_add_tokens</span>
            <span class="p">):</span>
                <span class="n">to_add_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Adding </span><span class="si">%s</span><span class="s2"> to the vocabulary&quot;</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span>

        <span class="n">added_tok_encoder</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">tok</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tok</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">to_add_tokens</span><span class="p">))</span>
        <span class="n">added_tok_decoder</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">added_tok_encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">added_tok_encoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">added_tok_decoder</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_add_tokens</span><span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.num_added_tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.num_added_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">num_added_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the number of added tokens when encoding a sequence with special tokens.</span>

<span class="sd">        Note:</span>
<span class="sd">            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this</span>
<span class="sd">            inside your training loop.</span>

<span class="sd">        Args:</span>
<span class="sd">            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the</span>
<span class="sd">                number of added tokens in the case of a single sequence if set to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of tokens added to sequences</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">token_ids_0</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">token_ids_1</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="kc">None</span><span class="p">))</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.add_special_tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.add_special_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">add_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">special_tokens_dict</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them</span>
<span class="sd">        to class attributes. If special tokens are NOT in the vocabulary, they are added</span>
<span class="sd">        to it (indexed starting from the last index of the current vocabulary).</span>

<span class="sd">        Using `add_special_tokens` will ensure your special tokens can be used in several ways:</span>

<span class="sd">        - special tokens are carefully handled by the tokenizer (they are never split)</span>
<span class="sd">        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.</span>

<span class="sd">        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be &#39;[CLS]&#39; and XLM&#39;s one is also registered to be &#39;&lt;/s&gt;&#39;)</span>

<span class="sd">        Args:</span>
<span class="sd">            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:</span>
<span class="sd">                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,</span>
<span class="sd">                ``additional_special_tokens``].</span>

<span class="sd">                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of tokens added to the vocabulary.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # Let&#39;s see how to add a new classification token to GPT-2</span>
<span class="sd">            tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)</span>
<span class="sd">            model = GPT2Model.from_pretrained(&#39;gpt2&#39;)</span>

<span class="sd">            special_tokens_dict = {&#39;cls_token&#39;: &#39;&lt;CLS&gt;&#39;}</span>

<span class="sd">            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)</span>
<span class="sd">            print(&#39;We have added&#39;, num_added_toks, &#39;tokens&#39;)</span>
<span class="sd">            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.</span>

<span class="sd">            assert tokenizer.cls_token == &#39;&lt;CLS&gt;&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">special_tokens_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

        <span class="n">added_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">special_tokens_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">value</span><span class="p">)</span>
                <span class="n">added_tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="n">added_tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">([</span><span class="n">value</span><span class="p">])</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Assigning </span><span class="si">%s</span><span class="s2"> to the </span><span class="si">%s</span><span class="s2"> key of the tokenizer&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">added_tokens</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.tokenize"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.tokenize">[docs]</a>    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a string in a sequence of tokens (string), using the tokenizer.</span>
<span class="sd">            Split in words for word-based vocabulary or sub-words for sub-word-based</span>
<span class="sd">            vocabularies (BPE/SentencePieces/WordPieces).</span>

<span class="sd">            Take care of added tokens.</span>

<span class="sd">            text: The sequence to be encoded.</span>
<span class="sd">            add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence</span>
<span class="sd">                begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.</span>
<span class="sd">            **kwargs: passed to the `prepare_for_tokenization` preprocessing method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_special_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span>
        <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_for_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">lowercase_text</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="c1"># convert non-special tokens to lowercase</span>
            <span class="n">escaped_special_toks</span> <span class="o">=</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">s_tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">s_tok</span> <span class="ow">in</span> <span class="n">all_special_tokens</span><span class="p">]</span>
            <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;(&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">escaped_special_toks</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;)|&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;(.+?)&quot;</span>
            <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="n">m</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;do_lower_case&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">lowercase_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">split_on_token</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">split_text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sub_text</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">split_text</span><span class="p">):</span>
                <span class="n">sub_text</span> <span class="o">=</span> <span class="n">sub_text</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">sub_text</span><span class="p">:</span>
                    <span class="n">result</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tok</span><span class="p">]</span>
                <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">split_text</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">sub_text</span><span class="p">:</span>
                        <span class="n">result</span> <span class="o">+=</span> <span class="p">[</span><span class="n">sub_text</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">sub_text</span><span class="p">:</span>
                        <span class="n">result</span> <span class="o">+=</span> <span class="p">[</span><span class="n">sub_text</span><span class="p">]</span>
                    <span class="n">result</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tok</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">result</span>

        <span class="k">def</span> <span class="nf">split_on_tokens</span><span class="p">(</span><span class="n">tok_list</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                <span class="k">return</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">tok_list</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

            <span class="n">tokenized_text</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">text_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tok_list</span><span class="p">:</span>
                <span class="n">tokenized_text</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">sub_text</span> <span class="ow">in</span> <span class="n">text_list</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">sub_text</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span><span class="p">:</span>
                        <span class="n">tokenized_text</span> <span class="o">+=</span> <span class="n">split_on_token</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="n">sub_text</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">tokenized_text</span> <span class="o">+=</span> <span class="p">[</span><span class="n">sub_text</span><span class="p">]</span>
                <span class="n">text_list</span> <span class="o">=</span> <span class="n">tokenized_text</span>

            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span>
                <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenize</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span> <span class="k">else</span> <span class="p">[</span><span class="n">token</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenized_text</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">added_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique_added_tokens_encoder</span>
        <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">split_on_tokens</span><span class="p">(</span><span class="n">added_tokens</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tokenized_text</span></div>

    <span class="k">def</span> <span class="nf">_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a string in a sequence of tokens (string), using the tokenizer.</span>
<span class="sd">            Split in words for word-based vocabulary or sub-words for sub-word-based</span>
<span class="sd">            vocabularies (BPE/SentencePieces/WordPieces).</span>

<span class="sd">            Do NOT take care of added tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="PreTrainedTokenizer.convert_tokens_to_ids"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.convert_tokens_to_ids">[docs]</a>    <span class="k">def</span> <span class="nf">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a single token, or a sequence of tokens, (str) in a single integer id</span>
<span class="sd">            (resp. a sequence of ids), using the vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_token_to_id_with_added_voc</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

        <span class="n">ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert_token_to_id_with_added_voc</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">ids</span></div>

    <span class="k">def</span> <span class="nf">_convert_token_to_id_with_added_voc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_token_to_id</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="PreTrainedTokenizer.encode"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode">[docs]</a>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.</span>

<span class="sd">        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str` or :obj:`List[str]`):</span>
<span class="sd">                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using</span>
<span class="sd">                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`</span>
<span class="sd">                method)</span>
<span class="sd">            text_pair (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized</span>
<span class="sd">                string using the `tokenize` method) or a list of integers (tokenized string ids using the</span>
<span class="sd">                `convert_tokens_to_ids` method)</span>
<span class="sd">            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                If set to ``True``, the sequences will be encoded with the special tokens relative</span>
<span class="sd">                to their model.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                If set to a number, will limit the total sequence returned so that it has a maximum length.</span>
<span class="sd">                If there are overflowing tokens, those will be added to the returned dictionary</span>
<span class="sd">            stride (:obj:`int`, `optional`, defaults to ``0``):</span>
<span class="sd">                If set to a number along with max_length, the overflowing tokens returned will contain some tokens</span>
<span class="sd">                from the main sequence returned. The value of this argument defines the number of additional tokens.</span>
<span class="sd">            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):</span>
<span class="sd">                String selected in the following options:</span>

<span class="sd">                - &#39;longest_first&#39; (default) Iteratively reduce the inputs sequence until the input is under max_length</span>
<span class="sd">                  starting from the longest one at each token (when there is a pair of input sequences)</span>
<span class="sd">                - &#39;only_first&#39;: Only truncate the first sequence</span>
<span class="sd">                - &#39;only_second&#39;: Only truncate the second sequence</span>
<span class="sd">                - &#39;do_not_truncate&#39;: Does not truncate (raise an error if the input sequence is longer than max_length)</span>
<span class="sd">            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                If set to True, the returned sequences will be padded according to the model&#39;s padding side and</span>
<span class="sd">                padding index, up to their max length. If no max length is specified, the padding is done up to the</span>
<span class="sd">                model&#39;s max length. The tokenizer padding sides are handled by the class attribute `padding_side`</span>
<span class="sd">                which can be set to the following strings:</span>

<span class="sd">                - &#39;left&#39;: pads on the left of the sequences</span>
<span class="sd">                - &#39;right&#39;: pads on the right of the sequences</span>
<span class="sd">                Defaults to False: no padding.</span>
<span class="sd">            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Can be set to &#39;tf&#39; or &#39;pt&#39; to return respectively TensorFlow :obj:`tf.constant`</span>
<span class="sd">                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.</span>
<span class="sd">            **kwargs: passed to the `self.tokenize()` method</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
            <span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">pad_to_max_length</span><span class="o">=</span><span class="n">pad_to_max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.encode_plus"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus">[docs]</a>    <span class="k">def</span> <span class="nf">encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dictionary containing the encoded sequence or sequence pair and additional information:</span>
<span class="sd">        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str` or :obj:`List[str]`):</span>
<span class="sd">                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using</span>
<span class="sd">                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`</span>
<span class="sd">                method)</span>
<span class="sd">            text_pair (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized</span>
<span class="sd">                string using the `tokenize` method) or a list of integers (tokenized string ids using the</span>
<span class="sd">                `convert_tokens_to_ids` method)</span>
<span class="sd">            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                If set to ``True``, the sequences will be encoded with the special tokens relative</span>
<span class="sd">                to their model.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                If set to a number, will limit the total sequence returned so that it has a maximum length.</span>
<span class="sd">                If there are overflowing tokens, those will be added to the returned dictionary</span>
<span class="sd">            stride (:obj:`int`, `optional`, defaults to ``0``):</span>
<span class="sd">                If set to a number along with max_length, the overflowing tokens returned will contain some tokens</span>
<span class="sd">                from the main sequence returned. The value of this argument defines the number of additional tokens.</span>
<span class="sd">            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):</span>
<span class="sd">                String selected in the following options:</span>

<span class="sd">                - &#39;longest_first&#39; (default) Iteratively reduce the inputs sequence until the input is under max_length</span>
<span class="sd">                  starting from the longest one at each token (when there is a pair of input sequences)</span>
<span class="sd">                - &#39;only_first&#39;: Only truncate the first sequence</span>
<span class="sd">                - &#39;only_second&#39;: Only truncate the second sequence</span>
<span class="sd">                - &#39;do_not_truncate&#39;: Does not truncate (raise an error if the input sequence is longer than max_length)</span>
<span class="sd">            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                If set to True, the returned sequences will be padded according to the model&#39;s padding side and</span>
<span class="sd">                padding index, up to their max length. If no max length is specified, the padding is done up to the</span>
<span class="sd">                model&#39;s max length. The tokenizer padding sides are handled by the class attribute `padding_side`</span>
<span class="sd">                which can be set to the following strings:</span>

<span class="sd">                - &#39;left&#39;: pads on the left of the sequences</span>
<span class="sd">                - &#39;right&#39;: pads on the right of the sequences</span>
<span class="sd">                Defaults to False: no padding.</span>
<span class="sd">            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Can be set to &#39;tf&#39; or &#39;pt&#39; to return respectively TensorFlow :obj:`tf.constant`</span>
<span class="sd">                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.</span>
<span class="sd">            return_token_type_ids (:obj:`bool`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Whether to return token type IDs. If left to the default, will return the token type IDs according</span>
<span class="sd">                to the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="sd">                `What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`_</span>
<span class="sd">            return_attention_mask (:obj:`bool`, `optional`, defaults to :obj:`none`):</span>
<span class="sd">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="sd">                to the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="sd">                `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="sd">            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Set to True to return overflowing token information (default False).</span>
<span class="sd">            return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Set to True to return special tokens mask information (default False).</span>
<span class="sd">            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Set to True to return (char_start, char_end) for each token (default False).</span>
<span class="sd">                If using Python&#39;s tokenizer, this method will raise NotImplementedError. This one is only available on</span>
<span class="sd">                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.</span>
<span class="sd">            **kwargs: passed to the `self.tokenize()` method</span>

<span class="sd">        Return:</span>
<span class="sd">            A Dictionary of shape::</span>

<span class="sd">                {</span>
<span class="sd">                    input_ids: list[int],</span>
<span class="sd">                    token_type_ids: list[int] if return_token_type_ids is True (default)</span>
<span class="sd">                    attention_mask: list[int] if return_attention_mask is True (default)</span>
<span class="sd">                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True</span>
<span class="sd">                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True</span>
<span class="sd">                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True</span>
<span class="sd">                }</span>

<span class="sd">            With the fields:</span>

<span class="sd">            - ``input_ids``: list of token ids to be fed to a model</span>
<span class="sd">            - ``token_type_ids``: list of token type ids to be fed to a model</span>
<span class="sd">            - ``attention_mask``: list of indices specifying which tokens should be attended to by the model</span>
<span class="sd">            - ``overflowing_tokens``: list of overflowing tokens if a max length is specified.</span>
<span class="sd">            - ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified</span>
<span class="sd">            - ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added</span>
<span class="sd">              tokens and 1 specifying sequence tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">get_input_ids</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">text</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_offsets_mapping</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;return_offset_mapping is not available when using Python tokenizers.&quot;</span>
                <span class="s2">&quot;To use this feature, change your tokenizer to one deriving from &quot;</span>
                <span class="s2">&quot;transformers.PreTrainedTokenizerFast.&quot;</span>
                <span class="s2">&quot;More information on available tokenizers at &quot;</span>
                <span class="s2">&quot;https://github.com/huggingface/transformers/pull/2674&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Throw an error if we can pad because there is no padding token</span>
        <span class="k">if</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy&quot;</span>
            <span class="p">)</span>

        <span class="n">first_ids</span> <span class="o">=</span> <span class="n">get_input_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">second_ids</span> <span class="o">=</span> <span class="n">get_input_ids</span><span class="p">(</span><span class="n">text_pair</span><span class="p">)</span> <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_for_model</span><span class="p">(</span>
            <span class="n">first_ids</span><span class="p">,</span>
            <span class="n">pair_ids</span><span class="o">=</span><span class="n">second_ids</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_max_length</span><span class="o">=</span><span class="n">pad_to_max_length</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.batch_encode_plus"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.batch_encode_plus">[docs]</a>    <span class="k">def</span> <span class="nf">batch_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_masks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_masks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_input_lengths</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dictionary containing the encoded sequence or sequence pair and additional information:</span>
<span class="sd">        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_text_or_text_pairs (:obj:`List[str]` or :obj:`List[List[str]]`):</span>
<span class="sd">                Batch of sequences or pair of sequences to be encoded.</span>
<span class="sd">                This can be a list of string/string-sequences/int-sequences or a list of pair of</span>
<span class="sd">                string/string-sequences/int-sequence (see details in encode_plus)</span>
<span class="sd">            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                If set to ``True``, the sequences will be encoded with the special tokens relative</span>
<span class="sd">                to their model.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                If set to a number, will limit the total sequence returned so that it has a maximum length.</span>
<span class="sd">                If there are overflowing tokens, those will be added to the returned dictionary</span>
<span class="sd">            stride (:obj:`int`, `optional`, defaults to ``0``):</span>
<span class="sd">                If set to a number along with max_length, the overflowing tokens returned will contain some tokens</span>
<span class="sd">                from the main sequence returned. The value of this argument defines the number of additional tokens.</span>
<span class="sd">            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):</span>
<span class="sd">                String selected in the following options:</span>

<span class="sd">                - &#39;longest_first&#39; (default) Iteratively reduce the inputs sequence until the input is under max_length</span>
<span class="sd">                  starting from the longest one at each token (when there is a pair of input sequences)</span>
<span class="sd">                - &#39;only_first&#39;: Only truncate the first sequence</span>
<span class="sd">                - &#39;only_second&#39;: Only truncate the second sequence</span>
<span class="sd">                - &#39;do_not_truncate&#39;: Does not truncate (raise an error if the input sequence is longer than max_length)</span>
<span class="sd">            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                If set to True, the returned sequences will be padded according to the model&#39;s padding side and</span>
<span class="sd">                padding index, up to their max length. If no max length is specified, the padding is done up to the</span>
<span class="sd">                model&#39;s max length. The tokenizer padding sides are handled by the class attribute `padding_side`</span>
<span class="sd">                which can be set to the following strings:</span>

<span class="sd">                - &#39;left&#39;: pads on the left of the sequences</span>
<span class="sd">                - &#39;right&#39;: pads on the right of the sequences</span>
<span class="sd">                Defaults to False: no padding.</span>
<span class="sd">            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Can be set to &#39;tf&#39; or &#39;pt&#39; to return respectively TensorFlow :obj:`tf.constant`</span>
<span class="sd">                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.</span>
<span class="sd">            return_token_type_ids (:obj:`bool`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Whether to return token type IDs. If left to the default, will return the token type IDs according</span>
<span class="sd">                to the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="sd">                `What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`_</span>
<span class="sd">            return_attention_masks (:obj:`bool`, `optional`, defaults to :obj:`none`):</span>
<span class="sd">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="sd">                to the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="sd">                `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="sd">            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Set to True to return overflowing token information (default False).</span>
<span class="sd">            return_special_tokens_masks (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Set to True to return special tokens mask information (default False).</span>
<span class="sd">            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Set to True to return (char_start, char_end) for each token (default False).</span>
<span class="sd">                If using Python&#39;s tokenizer, this method will raise NotImplementedError. This one is only available on</span>
<span class="sd">                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.</span>
<span class="sd">            return_input_lengths (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                If set the resulting dictionary will include the length of each sample</span>
<span class="sd">            **kwargs: passed to the `self.tokenize()` method</span>

<span class="sd">        Return:</span>
<span class="sd">            A Dictionary of shape::</span>

<span class="sd">                {</span>
<span class="sd">                    input_ids: list[List[int]],</span>
<span class="sd">                    token_type_ids: list[List[int]] if return_token_type_ids is True (default)</span>
<span class="sd">                    attention_mask: list[List[int]] if return_attention_mask is True (default)</span>
<span class="sd">                    overflowing_tokens: list[List[int]] if a ``max_length`` is specified and return_overflowing_tokens is True</span>
<span class="sd">                    num_truncated_tokens: List[int] if a ``max_length`` is specified and return_overflowing_tokens is True</span>
<span class="sd">                    special_tokens_mask: list[List[int]] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True</span>
<span class="sd">                }</span>

<span class="sd">            With the fields:</span>

<span class="sd">            - ``input_ids``: list of token ids to be fed to a model</span>
<span class="sd">            - ``token_type_ids``: list of token type ids to be fed to a model</span>
<span class="sd">            - ``attention_mask``: list of indices specifying which tokens should be attended to by the model</span>
<span class="sd">            - ``overflowing_tokens``: list of overflowing tokens if a max length is specified.</span>
<span class="sd">            - ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified</span>
<span class="sd">            - ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added</span>
<span class="sd">              tokens and 1 specifying sequence tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">get_input_ids</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">text</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Throw an error if we can pad because there is no padding token</span>
        <span class="k">if</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_offsets_mapping</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;return_offset_mapping is not available when using Python tokenizers.&quot;</span>
                <span class="s2">&quot;To use this feature, change your tokenizer to one deriving from &quot;</span>
                <span class="s2">&quot;transformers.PreTrainedTokenizerFast.&quot;</span>
                <span class="s2">&quot;More information on available tokenizers at &quot;</span>
                <span class="s2">&quot;https://github.com/huggingface/transformers/pull/2674&quot;</span>
            <span class="p">)</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ids_or_pair_ids</span> <span class="ow">in</span> <span class="n">batch_text_or_text_pairs</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids_or_pair_ids</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids_or_pair_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">ids_or_pair_ids</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">ids_or_pair_ids</span><span class="p">,</span> <span class="kc">None</span>

            <span class="n">first_ids</span> <span class="o">=</span> <span class="n">get_input_ids</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
            <span class="n">second_ids</span> <span class="o">=</span> <span class="n">get_input_ids</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">first_ids</span><span class="p">,</span> <span class="n">second_ids</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_to_max_length</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">total_sequence_length</span><span class="p">(</span><span class="n">input_pairs</span><span class="p">):</span>
                <span class="n">first_ids</span><span class="p">,</span> <span class="n">second_ids</span> <span class="o">=</span> <span class="n">input_pairs</span>
                <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_added_tokens</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">second_ids</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">second_ids</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_added_tokens</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="p">)</span>

            <span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">total_sequence_length</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">])</span>

        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">first_ids</span><span class="p">,</span> <span class="n">second_ids</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">:</span>
            <span class="c1"># Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by</span>
            <span class="c1"># the model. It adds special tokens, truncates sequences if overflowing while taking into account</span>
            <span class="c1"># the special tokens and manages a window stride for overflowing tokens</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_for_model</span><span class="p">(</span>
                <span class="n">first_ids</span><span class="p">,</span>
                <span class="n">pair_ids</span><span class="o">=</span><span class="n">second_ids</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">pad_to_max_length</span><span class="o">=</span><span class="n">pad_to_max_length</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_masks</span><span class="p">,</span>
                <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
                <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
                <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_masks</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Append the non-padded length to the output</span>
            <span class="k">if</span> <span class="n">return_input_lengths</span><span class="p">:</span>
                <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input_len&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="p">:</span>
                    <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

            <span class="c1"># Do the tensor conversion in batch</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span> <span class="ow">and</span> <span class="n">is_tf_available</span><span class="p">():</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                        <span class="k">if</span> <span class="kc">None</span> <span class="ow">in</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">value</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">]:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">NO_PAD_TOKEN_FOR_BATCH_MSG</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">UNEVEN_SEQUENCES_FOR_BATCH_MSG</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span> <span class="ow">and</span> <span class="n">is_torch_available</span><span class="p">():</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">UNEVEN_SEQUENCES_FOR_BATCH_MSG</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                        <span class="k">if</span> <span class="kc">None</span> <span class="ow">in</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">value</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">]:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">NO_PAD_TOKEN_FOR_BATCH_MSG</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span>
                <span class="k">elif</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Unable to convert output to tensors format </span><span class="si">{}</span><span class="s2">, PyTorch or TensorFlow is not available.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">return_tensors</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

        <span class="k">return</span> <span class="n">batch_outputs</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.prepare_for_model"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.prepare_for_model">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_for_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.</span>
<span class="sd">        It adds special tokens, truncates</span>
<span class="sd">        sequences if overflowing while taking into account the special tokens and manages a window stride for</span>
<span class="sd">        overflowing tokens</span>

<span class="sd">        Args:</span>
<span class="sd">            ids: list of tokenized input ids. Can be obtained from a string by chaining the</span>
<span class="sd">                `tokenize` and `convert_tokens_to_ids` methods.</span>
<span class="sd">            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the</span>
<span class="sd">                `tokenize` and `convert_tokens_to_ids` methods.</span>
<span class="sd">            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.</span>
<span class="sd">            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative</span>
<span class="sd">                to their model.</span>
<span class="sd">            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential</span>
<span class="sd">                list of inputs.</span>
<span class="sd">            truncation_strategy: string selected in the following options:</span>
<span class="sd">                - &#39;longest_first&#39; (default) Iteratively reduce the inputs sequence until the input is under max_length</span>
<span class="sd">                    starting from the longest one at each token (when there is a pair of input sequences)</span>
<span class="sd">                - &#39;only_first&#39;: Only truncate the first sequence</span>
<span class="sd">                - &#39;only_second&#39;: Only truncate the second sequence</span>
<span class="sd">                - &#39;do_not_truncate&#39;: Does not truncate (raise an error if the input sequence is longer than max_length)</span>
<span class="sd">            pad_to_max_length: if set to True, the returned sequences will be padded according to the model&#39;s padding side and</span>
<span class="sd">                padding index, up to their max length. If no max length is specified, the padding is done up to the model&#39;s max length.</span>
<span class="sd">                The tokenizer padding sides are handled by the following strings:</span>
<span class="sd">                - &#39;left&#39;: pads on the left of the sequences</span>
<span class="sd">                - &#39;right&#39;: pads on the right of the sequences</span>
<span class="sd">                Defaults to False: no padding.</span>
<span class="sd">            return_tensors: (optional) can be set to &#39;tf&#39; or &#39;pt&#39; to return respectively TensorFlow tf.constant</span>
<span class="sd">                or PyTorch torch.Tensor instead of a list of python integers.</span>
<span class="sd">            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).</span>
<span class="sd">            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)</span>
<span class="sd">            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).</span>
<span class="sd">            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).</span>

<span class="sd">        Return:</span>
<span class="sd">            A Dictionary of shape::</span>

<span class="sd">                {</span>
<span class="sd">                    input_ids: list[int],</span>
<span class="sd">                    token_type_ids: list[int] if return_token_type_ids is True (default)</span>
<span class="sd">                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True</span>
<span class="sd">                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True</span>
<span class="sd">                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True</span>
<span class="sd">                }</span>

<span class="sd">            With the fields:</span>
<span class="sd">                ``input_ids``: list of token ids to be fed to a model</span>
<span class="sd">                ``token_type_ids``: list of token type ids to be fed to a model</span>

<span class="sd">                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.</span>
<span class="sd">                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified</span>
<span class="sd">                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added</span>
<span class="sd">                tokens and 1 specifying sequence tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">len_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        <span class="n">len_pair_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_token_type_ids</span> <span class="o">=</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Handle max sequence length</span>
        <span class="n">total_len</span> <span class="o">=</span> <span class="n">len_ids</span> <span class="o">+</span> <span class="n">len_pair_ids</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_added_tokens</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="n">pair</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_special_tokens</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="n">total_len</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_sequences</span><span class="p">(</span>
                <span class="n">ids</span><span class="p">,</span>
                <span class="n">pair_ids</span><span class="o">=</span><span class="n">pair_ids</span><span class="p">,</span>
                <span class="n">num_tokens_to_remove</span><span class="o">=</span><span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span>
                <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">return_overflowing_tokens</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">overflowing_tokens</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;num_truncated_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span>

        <span class="c1"># Handle special_tokens</span>
        <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="n">ids</span> <span class="o">+</span> <span class="n">pair_ids</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="n">ids</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="p">[])</span>

        <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sequence</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_type_ids</span>

        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">][:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">][:</span><span class="n">max_length</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Token indices sequence length is longer than the specified maximum sequence length &quot;</span>
                <span class="s2">&quot;for this model (</span><span class="si">{}</span><span class="s2"> &gt; </span><span class="si">{}</span><span class="s2">). Running this sequence through the model will result in &quot;</span>
                <span class="s2">&quot;indexing errors&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="n">needs_to_be_padded</span> <span class="o">=</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">max_length</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">max_length</span>
            <span class="ow">or</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">&lt;=</span> <span class="mi">10000</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">&gt;</span> <span class="mi">10000</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Sequence can&#39;t be padded as no maximum length is specified and the model maximum length is too high.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">needs_to_be_padded</span><span class="p">:</span>
            <span class="n">difference</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span>
                        <span class="s2">&quot;token_type_ids&quot;</span>
                    <span class="p">]</span>
                <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid padding strategy:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">return_attention_mask</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

        <span class="c1"># Prepare inputs as tensors if asked</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span> <span class="ow">and</span> <span class="n">is_tf_available</span><span class="p">():</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]])</span>

            <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]])</span>

            <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]])</span>

        <span class="k">elif</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span> <span class="ow">and</span> <span class="n">is_torch_available</span><span class="p">():</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]])</span>

            <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]])</span>

            <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]])</span>
        <span class="k">elif</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Unable to convert output to tensors format </span><span class="si">{}</span><span class="s2">, PyTorch or TensorFlow is not available.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">return_tensors</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">encoded_inputs</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.prepare_for_tokenization"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.prepare_for_tokenization">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_for_tokenization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Performs any necessary transformations before tokenization &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">text</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.truncate_sequences"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.truncate_sequences">[docs]</a>    <span class="k">def</span> <span class="nf">truncate_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_tokens_to_remove</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="o">=</span><span class="s2">&quot;longest_first&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Truncates a sequence pair in place to the maximum length.</span>
<span class="sd">            truncation_strategy: string selected in the following options:</span>
<span class="sd">                - &#39;longest_first&#39; (default) Iteratively reduce the inputs sequence until the input is under max_length</span>
<span class="sd">                    starting from the longest one at each token (when there is a pair of input sequences).</span>
<span class="sd">                    Overflowing tokens only contains overflow from the first sequence.</span>
<span class="sd">                - &#39;only_first&#39;: Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.</span>
<span class="sd">                - &#39;only_second&#39;: Only truncate the second sequence</span>
<span class="sd">                - &#39;do_not_truncate&#39;: Does not truncate (raise an error if the input sequence is longer than max_length)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_tokens_to_remove</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">:</span>
            <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tokens_to_remove</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">):</span>
                    <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">overflowing_tokens</span>
                    <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="n">stride</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">window_len</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span> <span class="o">+</span> <span class="n">overflowing_tokens</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="s2">&quot;only_first&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_tokens_to_remove</span>
            <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">num_tokens_to_remove</span><span class="p">)</span>
            <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span>
            <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="o">-</span><span class="n">num_tokens_to_remove</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="s2">&quot;only_second&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_tokens_to_remove</span>
            <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">num_tokens_to_remove</span><span class="p">)</span>
            <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span>
            <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="o">-</span><span class="n">num_tokens_to_remove</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="s2">&quot;do_not_truncate&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input sequence are too long for max_length. Please select a truncation strategy.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Truncation_strategy should be selected in [&#39;longest_first&#39;, &#39;only_first&#39;, &#39;only_second&#39;, &#39;do_not_truncate&#39;]&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">)</span>

<div class="viewcode-block" id="PreTrainedTokenizer.build_inputs_with_special_tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.build_inputs_with_special_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks</span>
<span class="sd">        by concatenating and adding special tokens.</span>
<span class="sd">        A RoBERTa sequence has the following format:</span>
<span class="sd">            single sequence: &lt;s&gt; X &lt;/s&gt;</span>
<span class="sd">            pair of sequences: &lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">token_ids_0</span>
        <span class="k">return</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.get_special_tokens_mask"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.get_special_tokens_mask">[docs]</a>    <span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0: list of ids (must not contain special tokens)</span>
<span class="sd">            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids</span>
<span class="sd">                for sequence pairs</span>
<span class="sd">            already_has_special_tokens: (default False) Set to True if the token list is already formated with</span>
<span class="sd">                special tokens for the model</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">)</span> <span class="k">if</span> <span class="n">token_ids_1</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">))</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.convert_ids_to_tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.convert_ids_to_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">convert_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a single index or a sequence of indices (integers) in a token &quot;</span>
<span class="sd">            (resp.) a sequence of tokens (str), using the vocabulary and added tokens.</span>

<span class="sd">            Args:</span>
<span class="sd">                skip_special_tokens: Don&#39;t decode special tokens (self.all_special_tokens). Default: False</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">ids</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="p">[</span><span class="n">ids</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">skip_special_tokens</span> <span class="ow">and</span> <span class="n">index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_ids</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="p">:</span>
                <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert_id_to_token</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tokens</span></div>

    <span class="k">def</span> <span class="nf">_convert_id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="PreTrainedTokenizer.convert_tokens_to_string"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.convert_tokens_to_string">[docs]</a>    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a sequence of tokens (string) in a single string.</span>
<span class="sd">            The most simple way to do it is &#39; &#39;.join(self.convert_ids_to_tokens(token_ids))</span>
<span class="sd">            but we often want to remove sub-word tokenization artifacts at the same time.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span></div>

<div class="viewcode-block" id="PreTrainedTokenizer.decode"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.decode">[docs]</a>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary</span>
<span class="sd">        with options to remove special tokens and clean up tokenization spaces.</span>
<span class="sd">        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.</span>
<span class="sd">            skip_special_tokens: if set to True, will replace special tokens.</span>
<span class="sd">            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">filtered_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="n">skip_special_tokens</span><span class="p">)</span>

        <span class="c1"># To avoid mixing byte-level and unicode for byte-level BPT</span>
        <span class="c1"># we need to build string separatly for added tokens and byte-level tokens</span>
        <span class="c1"># cf. https://github.com/huggingface/transformers/issues/1133</span>
        <span class="n">sub_texts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_sub_text</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">filtered_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">skip_special_tokens</span> <span class="ow">and</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_ids</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">current_sub_text</span><span class="p">:</span>
                    <span class="n">sub_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="n">current_sub_text</span><span class="p">))</span>
                    <span class="n">current_sub_text</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">sub_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_sub_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">current_sub_text</span><span class="p">:</span>
            <span class="n">sub_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="n">current_sub_text</span><span class="p">))</span>
        <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sub_texts</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span>
            <span class="n">clean_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clean_up_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">clean_text</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">text</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">special_tokens_map</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; A dictionary mapping special token class attribute (cls_token, unk_token...) to their</span>
<span class="sd">            values (&#39;&lt;unk&gt;&#39;, &#39;&lt;cls&gt;&#39;...)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">attr_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">attr</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_value</span><span class="p">:</span>
                <span class="n">set_attr</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">attr_value</span>
        <span class="k">return</span> <span class="n">set_attr</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; List all the special tokens (&#39;&lt;unk&gt;&#39;, &#39;&lt;cls&gt;&#39;...) mapped to class attributes</span>
<span class="sd">            (cls_token, unk_token...).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map</span>
        <span class="k">for</span> <span class="n">attr_value</span> <span class="ow">in</span> <span class="n">set_attr</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">all_toks</span> <span class="o">=</span> <span class="n">all_toks</span> <span class="o">+</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attr_value</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="k">else</span> <span class="p">[</span><span class="n">attr_value</span><span class="p">])</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_toks</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">all_toks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; List the vocabulary indices of the special tokens (&#39;&lt;unk&gt;&#39;, &#39;&lt;cls&gt;&#39;...) mapped to</span>
<span class="sd">            class attributes (cls_token, unk_token...).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span>
        <span class="n">all_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">all_toks</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_ids</span>

<div class="viewcode-block" id="PreTrainedTokenizer.clean_up_tokenization"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.clean_up_tokenization">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">clean_up_tokenization</span><span class="p">(</span><span class="n">out_string</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">out_string</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">out_string</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; .&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; ?&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; !&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; ,&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39; &quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; n&#39;t&quot;</span><span class="p">,</span> <span class="s2">&quot;n&#39;t&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;m&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;m&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; do not&quot;</span><span class="p">,</span> <span class="s2">&quot; don&#39;t&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;s&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;ve&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;ve&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;re&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;re&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">out_string</span></div></div>


<span class="k">class</span> <span class="nc">PreTrainedTokenizerFast</span><span class="p">(</span><span class="n">PreTrainedTokenizer</span><span class="p">):</span>

    <span class="n">model_input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided tokenizer cannot be None&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_len_single_sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_added_tokens</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># take into account special tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_len_sentences_pair</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_added_tokens</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># take into account special tokens</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">decoder</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">(</span><span class="n">with_added_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">(</span><span class="n">with_added_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">eos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">unk_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">sep_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">cls_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">mask_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="nd">@PreTrainedTokenizer</span><span class="o">.</span><span class="n">additional_special_tokens</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_update_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_encoding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoding</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_token_type_ids</span> <span class="o">=</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="k">if</span> <span class="n">return_overflowing_tokens</span> <span class="ow">and</span> <span class="n">encoding</span><span class="o">.</span><span class="n">overflowing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encodings</span> <span class="o">=</span> <span class="p">[</span><span class="n">encoding</span><span class="p">]</span> <span class="o">+</span> <span class="n">encoding</span><span class="o">.</span><span class="n">overflowing</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encodings</span> <span class="o">=</span> <span class="p">[</span><span class="n">encoding</span><span class="p">]</span>

        <span class="n">encoding_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">encodings</span><span class="p">:</span>
            <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">type_ids</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">special_tokens_mask</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_offsets_mapping</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;offset_mapping&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">e</span><span class="o">.</span><span class="n">original_str</span><span class="o">.</span><span class="n">offsets</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">offsets</span><span class="p">])</span>

        <span class="c1"># Prepare inputs as tensors if asked</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span> <span class="ow">and</span> <span class="n">is_tf_available</span><span class="p">():</span>
            <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoding_dict</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">])</span>

            <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">encoding_dict</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>

        <span class="k">elif</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span> <span class="ow">and</span> <span class="n">is_torch_available</span><span class="p">():</span>
            <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoding_dict</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">])</span>

            <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">encoding_dict</span><span class="p">:</span>
                <span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoding_dict</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Unable to convert output to tensors format </span><span class="si">{}</span><span class="s2">, PyTorch or TensorFlow is not available.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">return_tensors</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">encoding_dict</span>

    <span class="k">def</span> <span class="nf">_convert_token_to_id_with_added_voc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
        <span class="nb">id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span>
        <span class="k">return</span> <span class="nb">id</span>

    <span class="k">def</span> <span class="nf">_convert_id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">id_to_token</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_tokens</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">special_tokens_dict</span><span class="p">):</span>
        <span class="n">added</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_special_tokens</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">added</span>

    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">token_ids_0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span>

    <span class="k">def</span> <span class="nf">num_added_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">batch_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Fast tokenizers add special tokens by default. To remove special tokens, please specify&quot;</span>
                <span class="s2">&quot;`add_special_tokens=False` during the initialisation rather than when calling `encode`,&quot;</span>
                <span class="s2">&quot;`encode_plus` or `batch_encode_plus`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Needed if we have to return a tensor</span>
        <span class="n">pad_to_max_length</span> <span class="o">=</span> <span class="n">pad_to_max_length</span> <span class="ow">or</span> <span class="p">(</span><span class="n">return_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Throw an error if we can pad because there is no padding token</span>
        <span class="k">if</span> <span class="n">pad_to_max_length</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unable to set proper padding strategy as the tokenizer does not have a padding token&quot;</span><span class="p">)</span>

        <span class="c1"># Set the truncation and padding strategy and restore the initial configuration</span>
        <span class="k">with</span> <span class="n">truncate_and_pad</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">pad_to_max_length</span><span class="o">=</span><span class="n">pad_to_max_length</span><span class="p">,</span>
            <span class="n">padding_side</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">pad_token_type_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span><span class="p">,</span>
        <span class="p">):</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_text_or_text_pairs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;batch_text_or_text_pairs has to be a list (got </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">batch_text_or_text_pairs</span><span class="p">))</span>
                <span class="p">)</span>

            <span class="c1"># Avoid thread overhead if only one example.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_text_or_text_pairs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_text_or_text_pairs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                    <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="o">*</span><span class="n">batch_text_or_text_pairs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch_text_or_text_pairs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokens</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">batch_text_or_text_pairs</span><span class="p">)</span>

        <span class="c1"># Convert encoding to dict</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_convert_encoding</span><span class="p">(</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
                <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
                <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
                <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">tokens</span>
        <span class="p">]</span>

        <span class="c1"># Sanitize the output to have dict[list] from list[dict]</span>
        <span class="n">sanitized</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">stack</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">item</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
                <span class="n">stack</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">stack</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span><span class="p">:</span>
                <span class="n">stack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">stack</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">return_tensors</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stack</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">stack</span> <span class="o">=</span> <span class="n">stack</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">sanitized</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">stack</span>

        <span class="c1"># If returning overflowing tokens, we need to return a mapping</span>
        <span class="c1"># from the batch idx to the original sample</span>
        <span class="k">if</span> <span class="n">return_overflowing_tokens</span><span class="p">:</span>
            <span class="n">overflow_to_sample_mapping</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">i</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">sanitized</span><span class="p">[</span><span class="s2">&quot;overflow_to_sample_mapping&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">overflow_to_sample_mapping</span>
        <span class="k">return</span> <span class="n">sanitized</span>

    <span class="k">def</span> <span class="nf">encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_to_max_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">batched_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">text</span><span class="p">,</span> <span class="n">text_pair</span><span class="p">)]</span> <span class="k">if</span> <span class="n">text_pair</span> <span class="k">else</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
        <span class="n">batched_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">(</span>
            <span class="n">batched_input</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
            <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="n">pad_to_max_length</span><span class="o">=</span><span class="n">pad_to_max_length</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Return tensor is None, then we can remove the leading batch axis</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_tensors</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">batched_output</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">batched_output</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span>
            <span class="n">clean_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clean_up_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">clean_text</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">text</span>

    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">files</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">folder</span><span class="p">,</span> <span class="n">file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">save_directory</span><span class="p">))</span>
            <span class="n">files</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">files</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">trim_batch</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Remove columns that are populated exclusively by pad_token_id&quot;&quot;&quot;</span>
    <span class="n">keep_column_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="n">keep_column_mask</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">keep_column_mask</span><span class="p">],</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="n">keep_column_mask</span><span class="p">])</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>